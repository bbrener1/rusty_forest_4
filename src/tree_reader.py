# import smooth_density_graph as sdg
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.linear_model import Ridge, Lasso
from scipy.spatial.distance import pdist, cdist
from pathlib import Path
import copy
from multiprocessing import Pool
from scipy.cluster import hierarchy as hrc
# from sklearn.metrics import jaccard_similarity_score
from umap import UMAP
from sklearn.decomposition import NMF
from sklearn.manifold import MDS
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from sklearn.manifold import TSNE
from sklearn.decomposition import KernelPCA
from sklearn.decomposition import PCA
import sklearn
from scipy.optimize import nnls
from scipy.spatial.distance import squareform
from scipy.spatial.distance import jaccard
from scipy.stats import linregress
import scipy.special
from functools import reduce
import pickle
import glob
import random
import os
import sys
import json
import re
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
# COLOR = 'white'
# BACKGROUND = 'black'
# mpl.rcParams['text.color'] = COLOR
# mpl.rcParams['axes.labelcolor'] = COLOR
# mpl.rcParams['xtick.color'] = COLOR
# mpl.rcParams['ytick.color'] = COLOR
# mpl.rc('axes',fc=BACKGROUND)

mpl.rcParams['figure.dpi'] = 300


# jaccard_index = jaccard_similarity_score


# from hdbscan import HDBSCAN


sdg_path_found = False

sdg_path = Path(__file__).parent.parent.absolute()
sdg_path = str(sdg_path) + "/smooth_density_graph/"
# print(f"Attempting to locate Smooth Density Graph in {sdg_path}")
sys.path.append(sdg_path)


class Node:

    def __init__(self, node_json, tree, forest, parent=None, cache=False, lr=None, level=0):

        # Here we initialize the node from a json object generated by random forest

        self.cache = cache

        self.tree = tree
        self.forest = forest
        self.parent = parent
        self.lr = lr
        self.level = level
        self.filter = Filter(node_json['filter'], self)
        # self.samples = np.array(node_json['samples'])
        self.local_samples = None
        if 'means' in node_json:
            self.mean_cache = np.array(node_json['means'])
        if 'medians' in node_json:
            self.median_cache = np.array(node_json['medians'])
        self.weights = np.ones(len(self.forest.output_features))
        self.children = []
        self.child_clusters = ([], [])

        # Note the unpacking is recursive to maintain the tree structure
        # There are probably other ways to encode this like network connectivity
        # matrices but I think this leads to slightly more elegant structure.
        # We got python, we might as well use it

        if len(node_json['children']) > 0:
            self.children.append(Node(
                node_json['children'][0], self.tree, self.forest, parent=self, lr=0, level=level + 1, cache=cache))
            self.children.append(Node(
                node_json['children'][1], self.tree, self.forest, parent=self, lr=1, level=level + 1, cache=cache))
        else:
            self.local_samples = node_json['samples']
            # pass

    # Two nodes used for testing, not relevant for normal operation

    # def null():
    #
    #     null_dictionary = {}
    #     null_dictionary['feature'] = None
    #     null_dictionary['split'] = None
    #     null_dictionary['features'] = []
    #     null_dictionary['samples'] = []
    #     null_dictionary['children'] = []
    #     return Node(null_dictionary,None,None)
    #
    # def test_node(feature,split,features,samples,medians,dispersions):
    #     test_node = Node.null()
    #     test_node.feature = feature
    #     test_node.split = split
    #     test_node.features.extend(features)
    #     test_node.samples.extend(samples)
    #     return test_node
    #
    def samples(self):
        if self.local_samples is None:
            leaves = self.leaves()
            samples = [s for l in leaves for s in l.local_samples]
            return samples
        else:
            return self.local_samples

    def pop(self):
        if hasattr(self, 'pop_cache'):
            return self.pop_cache
        else:
            pop = len(self.samples())
            self.pop_cache = pop
            return self.pop_cache

    def nodes(self):

        # Obtains all descendant nodes in deterministic order
        # Left to right

        nodes = []
        for child in self.children:
            nodes.extend(child.nodes())
        for child in self.children:
            nodes.append(child)
        return nodes

    def encoding(self):
        if self.cache:
            if hasattr(self, 'encoding_cache'):
                return self.encoding_cache
        encoding = np.zeros(len(self.forest.samples), dtype=bool)
        encoding[self.samples()] = True
        if self.cache:
            self.encoding_cache = encoding
        return encoding

    def sample_mask(self):
        # Alias of encoding
        return self.encoding()

    def node_counts(self):

        # Obtains the count matrix of samples belonging to this node
        # Samples are in order they are in the node

        copy = self.forest.output[self.samples()].copy()
        return copy

    def medians(self):

        # Medians of all features in this node. Relies on node_counts

        if self.cache:
            if hasattr(self, 'median_cache'):
                return self.median_cache
        matrix = self.forest.output[self.sample_mask()]
        medians = np.median(matrix, axis=0)
        if self.cache:
            self.median_cache = medians
        return medians

    def feature_median(self, feature):

        # Median of a specific feature within this node.
        # Much faster than getting the entire matrix, obviously, unless medians are cached

        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        # values = [self.forest.output[s, fi] for s in self.samples()]
        values = self.forest.output[self.sample_mask()].T[fi]
        return np.median(values)

    def means(self):

        # Means of all features within this node. Relies on node_counts

        if self.cache:
            if hasattr(self, 'mean_cache'):
                return self.mean_cache
        matrix = self.forest.output[self.sample_mask()]
        means = np.mean(matrix, axis=0)
        if self.cache:
            self.mean_cache = means
        return means

    def sample_cluster_means(self):

        labels = self.forest.sample_labels[self.samples]
        one_hot = np.array([labels == x.id for x in self.forest.sample_clusters])
        means = np.mean(one_hot,axis=0)
        return means

    def feature_mean(self, feature):

        # As above, mean of an individual feature within this node

        fi = self.forest.truth_dictionary.feature_dictionary[feature]

        if hasattr(self, 'mean_cache'):
            return self.mean_cache[fi]
        else:
            # values = [self.forest.output[s, fi] for s in self.samples()]
            values = self.forest.output[self.sample_mask()].T[fi]
            return np.mean(values)

    def mean_residuals(self):

        counts = self.node_counts()
        means = self.means()
        residuals = counts - means

        return residuals

    def median_residuals(self):

        counts = self.node_counts()
        medians = self.medians()
        residuals = counts - medians

        return residuals

    def squared_residual_sum(self):
        if hasattr(self, 'srs_cache'):
            return self.srs_cache
        else:
            squared_residuals = np.power(self.mean_residuals(), 2)
            srs = np.sum(squared_residuals, axis=0)
            if self.cache:
                self.srs_cache = srs
            return srs

    def dispersions(self, mode='mean'):

        # Dispersions of this node. Hardcoded for SSME at the moment.
        # RECOMPUTED FROM SCRATCH. This SHOULD be mathematically equivalent to what
        # rust computes, but this is NOT guaranteed.
        # To do eventually: altered dispersion modes?

        if self.cache:
            if hasattr(self, 'dispersion_cache'):
                return self.dispersion_cache

        if mode == 'mean':
            residuals = self.mean_residuals()
        elif mode == 'median':
            residuals = self.median_residuals()
        else:
            raise Exception(f"Mode not recognized:{mode}")
        dispersions = np.power(residuals, 2)
        if self.cache:
            self.dispersion_cache = dispersions
        return dispersions

    def mean_error_ratio(self):

        # Can't call to parent residuals because parent will have other samples not
        # present in this node

        counts = self.node_counts()
        self_predictions = self.means()
        if self.parent is not None:
            parent_predictions = self.parent.means()
        else:
            parent_predictions = self.means()
        self_error = np.sum(np.power(counts - self_predictions, 2), axis=0) + 1
        parent_error = np.sum(
            np.power(counts - parent_predictions, 2), axis=0) + 1

        return self_error / parent_error

    def absolute_gains(self):

        # Gains in dispersion relative to the root of the tree this node belongs to.
        # Only works if dispersions are of the summation type (eg sum of squared errors etc)

        if self.cache:
            if hasattr(self, 'absolute_gain_cache'):
                return self.absolute_gain_cache
        own_dispersions = self.dispersions()
        root_dispersions = self.root().dispersions()
        gains = root_dispersions - own_dispersions
        if self.cache:
            self.absolute_gain_cache = gains
        return gains

    def local_gains(self):

        # Gains in dispersion relative to the parent node.
        # As above, only works for summation-type errors like SSME

        if self.cache:
            if hasattr(self, 'local_gain_cache'):
                return self.local_gain_cache
        if self.parent is not None:
            parent_dispersions = self.parent.dispersions()
        else:
            parent_dispersions = self.dispersions()
        own_dispersions = self.dispersions()
        gains = parent_dispersions - own_dispersions
        if self.cache:
            self.local_gain_cache = gains
        return gains

    def additive_gains(self):

        # What is the change in absolute values of medians of all features between parent node
        # and this node?

        # If you have a sample, and you have a number of nodes
        # FROM THE SAME TREE that this sample belongs to, then summing the additive gains of
        # all nodes produces a prediction for that sample.

        if self.cache:
            if hasattr(self, 'additive_cache'):
                return self.additive_cache
        if self.parent is not None:
            parent_medians = self.parent.medians()
        else:
            parent_medians = np.zeros(len(self.forest.output_features))
        own_medians = self.medians()
        additive = own_medians - parent_medians
        if self.cache:
            self.additive_cache = additive
        return additive

    def additive_mean_gains(self):

        # What is the change in absolute values of medians of all features between parent node
        # and this node?

        # What the hell is this method? If you have a sample, and you have a number of nodes
        # FROM THE SAME TREE that this sample belongs to, then summing the additive gains of
        # all nodes produces a prediction for that sample.

        if self.cache:
            if hasattr(self, 'additive_mean_cache'):
                return self.additive_mean_cache
        if self.parent is not None:
            parent_means = self.parent.means()
        else:
            parent_means = np.zeros(len(self.forest.output_features))
        own_means = self.means()
        additive = own_means - parent_means
        if self.cache:
            self.additive_mean_cache = additive
        return additive

    def feature_additive(self, feature):

        # Additive gains for a single feature. See additive_gains for explanation of additive gains

        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        own_value = self.feature_median(feature)
        if self.parent is not None:
            parent_value = self.parent.feature_median(feature)
        else:
            parent_value = 0.
        return own_value - parent_value

    def feature_additive_mean(self, feature):

        # Additive gains using means instead of medians. Alternative prediction mode.
        # MAY produce superior results for features of intermediate sparsity.

        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        own_value = self.feature_mean(feature)
        if self.parent is not None:
            parent_value = self.parent.feature_mean(feature)
        else:
            parent_value = 0.
        return own_value - parent_value

    def leaves(self, depth=None):

        # Obtains all leaves belonging to this node
        if depth is None:
            depth_limit = self.level + 1
        else:
            depth_limit = depth

        leaves = []
        if len(self.children) < 1 or self.level >= depth_limit:
            leaves.append(self)
        else:
            for child in self.children:
                leaves.extend(child.leaves(depth=depth))

        return leaves

    def stems(self):

        # Obtains all stems belonging to this node
        # (Stems are not roots and not leaves)

        stems = []
        for child in self.children:
            stems.extend(child.stems())
        for child in self.children:
            if len(child.children) > 0:
                stems.append(child)
        return stems

    def sister(self):

        # Obtains the sister node, if any

        if self.parent is None:
            return None
        else:
            for child in self.parent.children:
                if child is not self:
                    return child
            return None

    def descend(self, n):

        # All nodes no more than n levels down

        nodes = []
        if n > 0:
            for child in self.children:
                nodes.extend(child.descend(n - 1))
            if len(nodes) < 1:
                nodes.append(self)
        else:
            nodes.append(self)
        return nodes

    def root(self):

        # Ascend to tree root

        if self.parent is not None:
            return self.parent.root()
        else:
            return self

    def ancestors(self):

        # All anscestors of this node

        ancestors = []
        if self.parent is not None:
            ancestors.append(self.parent)
            ancestors.extend(self.parent.ancestors())
        return ancestors

    def nodes_by_level(self):

        # Levelizes this tree

        # (Eg: a list of lists, list[x] corresponds to all descendent nodes from this one that are x levels down)

        levels = [[self]]
        for child in self.children:
            child_levels = child.nodes_by_level()
            for i, child_level in enumerate(child_levels):
                if len(levels) < i + 2:
                    levels.append([])
                levels[i + 1].extend(child_level)
        return levels

    def plotting_representation(self):

        # Used for plotting individual trees, returns a nested list of proportions
        # of samples present

        total_width = sum([x.pop() for x in self.children])
        child_proportions = []
        for child in self.children:
            child_proportions.append(
                [float(child.pop()) / float(total_width), ])
            child_proportions[-1].append(child.plotting_representation())
        # print(child_proportions)
        return child_proportions

    def sample_names(self):

        # Returns formal names (if any) for samples in this node

        return [self.forest.samples[i] for i in self.samples()]

    def prerequisite_levels(self):

        # May be defunct if prerequisites are going by the wayside in favor of braids

        prerequisite_levels = []
        for child in self.children:
            prerequisite_levels.extend(child.prerequisite_levels())
        prerequisite_levels.append((self.prerequisites[-1], self.level))
        return prerequisite_levels

    def braid_levels(self):

        # Provides the braids that appear in the children of this node, paired to the level they occur at

        braid_levels = []
        for child in self.children:
            braid_levels.extend(child.braid_levels())
        braid_levels.append((self.braids[-1], self.level))
        return braid_levels

    def feature(self):

        # Best guess at the "split feature" of this node, if any

        return self.filter.feature()

    def level(self, target):

        # Slices to a specific level of a given tree

        level_nodes = []
        if len(self.children) > 0:
            if self.level <= target:
                level_nodes.extend(self.children[0].level(target))
                level_nodes.extend(self.children[1].level(target))
        else:
            level_nodes.append(self)
        return level_nodes

    def depth(self, d=0):

        # How deep does this node go?

        for child in self.children:
            d = max(child.depth(d + 1), d)
        return d

    def trim(depth):

        # NOT YET FULLY IMPLEMENTED

        all_children = self.nodes()
        keep_children = set(self.descend(depth))
        for child in all_children:
            if child not in keep_children:
                del(child)

    def sorted_node_counts(self):

        # Creates a sorted table of the node values, primarily useful for plotting.

        node_counts = self.node_counts()
        try:
            sort_feature_index = self.forest.truth_dictionary.feature_dictionary[self.feature(
            )]
            sort_order = np.argsort(
                self.forest.counts[:, sort_feature_index][sample_mask])
        except:
            sort_order = np.arange(node_counts.shape[0])

        return node_counts[sort_order]

    def predict_sample_leaves(self, sample):

        # Predictive method
        # Finds the leaves that a given sample belongs to in this (sub) tree

        leaves = []

        for i, child in enumerate(self.children):
            if child.filter.filter(sample):
                leaves.extend(child.predict_sample_leaves(sample))
        if len(leaves) < 1:
            leaves.append(self)

        return leaves

    def predict_sample_nodes(self, sample):

        # Predictive method

        # Finds all nodes that a sample belongs to in this (sub) tree

        nodes = [self, ]

        for child in self.children:
            if child.filter.filter(sample):
                nodes.extend(child.predict_sample_nodes(sample))

        # print(nodes)

        return nodes

    def predict_matrix_encoding(self, matrix):
        encodings = []
        own_mask = self.filter.filter_matrix(matrix)
        if np.sum(own_mask) > 0:
            for child in self.children:
                child_encoding = child.predict_matrix_encoding(
                    matrix[own_mask])
                expanded_encoding = np.zeros(
                    (child_encoding.shape[0], matrix.shape[0]), dtype=bool)
                expanded_encoding.T[own_mask] = child_encoding.T
                encodings.append(expanded_encoding)
            for child in self.children:
                expanded_encoding = np.zeros((1, matrix.shape[0]), dtype=bool)
                expanded_encoding[0][own_mask] = child.filter.filter_matrix(
                    matrix[own_mask])
                encodings.append(expanded_encoding)
        else:
            encodings = [
                np.zeros((len(self.nodes()), matrix.shape[0]), dtype=bool), ]
        if len(encodings) > 0:
            combined_encoding = np.vstack(encodings)
        else:
            combined_encoding = np.zeros((0, matrix.shape[0]), dtype=bool)
        return combined_encoding

    def tree_path_vector(self):

        # Binary left-right encoding of the path to this node. Primarily for internal use
        # May be defunct with prerequisite phaseout

        return np.array([0 if prerequisite[2] == '<' else 1 for prerequisite in self.prerequisites])

    def leaf_distances(self):

        # Experimental, attempts to produce within-tree distances of one leaf to another

        leaves = self.leaves()
        distances = np.zeros((len(leaves), len(leaves)))
        for i in range(len(leaves)):
            l1v = leaves[i].tree_path_vector()
            for j in range(i, len(leaves)):
                l2v = leaves[j].tree_path_vector()
                distance = leaves[i].level + leaves[j].level
                for l1, l2 in zip(l1v, l2v):
                    if l1 == l2:
                        distance -= 2
                    else:
                        break
                distances[i, j] = distance
                distances[j, i] = distance
        return distances

    def add_child_cluster(self, cluster, lr):

        # Keeps track of clusters that occur among child nodes.
        # Useful for calculating cluster-cluster tree distances
        # Experimental

        self.child_clusters[lr].append(cluster)
        if self.parent is not None:
            self.parent.add_child_cluster(cluster, self.lr)

    def set_split_cluster(self, cluster):

        # Sets the node split cluster

        self.split_cluster = cluster
        if self.parent is not None:
            self.parent.add_child_cluster(cluster, self.lr)
        for descendant in self.nodes():
            if not hasattr(descendant, 'split_cluster'):
                descendant.split_cluster = cluster

    def add_child_cluster(self, cluster, lr):

        # Adds a cluster to the child cluster information of this node

        self.child_clusters[lr].append(cluster)
        if self.parent is not None:
            self.parent.add_child_cluster(cluster, self.lr)

    def sample_cluster_means(self):

        # (roughly) provides the proportion of the samples in this node that belong to
        # each sample cluster. Allows us to test if node clusters and sample clusters have a
        # correspondence

        if self.cache:
            if hasattr(self, 'sample_cluster_cache'):
                return self.sample_cluster_cache

        sample_clusters = self.forest.sample_cluster_encoding.T[self.samples(
        )].T
        sample_cluster_means = np.mean(sample_clusters, axis=1)

        if self.cache:
            self.sample_cluster_cache = sample_cluster_means

        return sample_cluster_means

    def reset_cache(self):

        # Resets the cache values for various things.
        # Save memory, avoid errors when you add/remove output features

        possible_caches = [
            "absolute_gain_cache",
            "local_gain_cache",
            "additive_cache",
            "additive_mean_cache",
            "median_cache",
            "dispersion_cache",
            "mean_cache",
            "encoding_cache",
            "weighted_prediction_cache"
        ]

        for cache in possible_caches:
            try:
                delattr(self, cache)
            except:
                continue

    # def lr_encoding_vectors(self):
    #     left = np.zeros(len(self.forest.samples),dtype=bool)
    #     right = np.zeros(len(self.forest.samples),dtype=bool)
    #     child_masks = [left,right]
    #     for i,child in enumerate(self.children):
    #         child_masks[i] = child.sample_mask()
    #     return child_masks


class Filter:

    def __init__(self, filter_json, node):
        try:
            self.node = node
            self.reduction = Reduction(filter_json['reduction'])
            self.split = filter_json['split']
            self.orientation = filter_json['orientation']
        except:
            print(filter_json)
            raise Exception

    def feature(self):
        if len(self.reduction.features) == 1:
            return self.reduction.features[0]
        else:
            raise Exception

    def filter(self, sample):
        sample_score = self.reduction.score_sample(sample)
        if self.orientation:
            return sample_score > self.split
        else:
            return sample_score <= self.split

    def filter_matrix(self, matrix):
        scores = self.reduction.score_matrix(matrix)
        if self.orientation:
            return scores > self.split
        else:
            return scores <= self.split


class Reduction:

    def __init__(self, reduction_json):
        self.features = reduction_json['features']
        self.scores = reduction_json['scores']
        self.means = reduction_json['means']

    def score_sample(self, sample):
        compound_score = 0
        for feature, feature_score, feature_mean in zip(self.features, self.scores, self.means):
            compound_score += (sample[feature] - feature_mean) * feature_score
        return compound_score

    def score_matrix(self, matrix):
        compound_scores = np.zeros(matrix.shape[0])
        if len(self.features) > 0:
            compound_scores = np.sum(
                (matrix.T[self.features].T - self.means) * self.scores, axis=1)
        return compound_scores


class Tree:

    def __init__(self, tree_json, forest):
        self.root = Node(tree_json, self, forest, cache=forest.cache)
        self.forest = forest

    def nodes(self, root=True):
        nodes = []
        nodes.extend(self.root.nodes())
        if root:
            nodes.append(self.root)
        return nodes

    def leaves(self, depth=None):
        leaves = self.root.leaves(depth=depth)
        if len(leaves) < 1:
            leaves.append(self.root)
        return leaves

    def stems(self):
        stems = self.root.stems()
        return stems

    def level(self, target):
        level_nodes = []
        for node in self.nodes():
            if node.level == target:
                level_nodes.append(node)
        return level_nodes

    def descend(self, level):
        return self.root.descend(level)

    def seek(self, directions):
        if len(directions) > 0:
            self.children[directions[0]].seek(directions[1:])
        else:
            return self

    def trim(self, depth):
        self.root.trim(depth)

    def feature_levels(self):
        return self.root.feature_levels()

    def plotting_representation(self, width=10, height=10):
        coordinates = []
        connectivities = []
        bars = []
        levels = self.root.nodes_by_level()
        jump = height / len(levels)
        for i, level in enumerate(levels):
            level_samples = sum([node.pop() for node in level])
            next_level_samples = 0
            if i < (len(levels) - 1):
                next_level_samples = sum([node.pop()
                                          for node in levels[i + 1]])
            consumed_width = 0
            next_consumed_width = 0
            for j, node in enumerate(level):
                sample_weight = float(node.pop()) / float(level_samples)
                half_width = (width * sample_weight) / 2
                center = consumed_width + half_width
                consumed_width = consumed_width + (half_width * 2)
                coordinates.append((i * jump, center))
                if i < (len(levels) - 1):
                    for child in node.children:
                        child_sample_weight = float(
                            child.pop()) / float(next_level_samples)
                        child_half_width = (width * child_sample_weight) / 2
                        child_center = next_consumed_width + child_half_width
                        next_consumed_width = next_consumed_width + \
                            (child_half_width * 2)
                        connectivities.append(
                            ([i * jump, (i + 1) * jump], [center, child_center]))
        coordinates = np.array(coordinates)
        plt.figure()
        plt.scatter(coordinates[:, 0], coordinates[:, 1], s=1)
        for connection in connectivities:
            plt.plot(connection[0], connection[1])
        plt.show()

        # return coordinates,connectivities

    def recursive_plotting_repesentation(self, axes, height=None, height_step=None, representation=None, limits=None):
        if limits is None:
            limits = axes.get_xlim()
        current_position = limits[0]
        width = float(limits[1] - limits[0])
        center = (limits[1] + limits[0]) / 2
        if representation is None:
            representation = self.root.plotting_representation()
            print(representation)
        if height_step is None or height is None:
            depth = self.root.depth()
            height_limits = axes.get_ylim()
            height = height_limits[1]
            height_step = -1 * (height_limits[1] - height_limits[0]) / depth
        # print(representation)
        for i, current_representation in enumerate(representation):
            width_proportion = current_representation[0]
            children = current_representation[1]
            node_start = current_position
            node_width = width_proportion * width
            padding = node_width * .05
            node_width = node_width - padding
            node_center = (node_width / 2) + current_position
            node_height = height + height_step
            node_end = (node_width) + current_position
            current_position = node_end + padding

            color = ['r', 'b'][(i % 2)]

            axes.plot([center, node_center], [height, node_height], c=color)
            # axes.plot([node_center],[node_height])
            axes.plot([node_start, node_end], [
                      node_height, node_height], c=color)

            self.recursive_plotting_repesentation(
                axes, height=node_height, height_step=height_step, representation=children, limits=(node_start, node_end))

    def plot(self):
        fig = plt.figure(figsize=(10, 20))
        ax = fig.add_subplot(111)
        self.recursive_plotting_repesentation(ax)
        fig.show()

    def tree_movie_frame(self, location, level=0, sorted=True, previous_frame=None, split_lines=True):
        descent_nodes = self.descend(level)
        total_samples = sum([node.pop() for node in descent_nodes])
        heatmap = np.zeros((total_samples, len(self.forest.output_features)))
        node_splits = []
        running_samples = 0
        for node in descent_nodes:
            if sorted:
                node_counts = node.sorted_node_counts()
            else:
                node_counts = node.node_counts()
            node_samples = node_counts.shape[0]
            heatmap[running_samples:running_samples +
                    node_samples] = node_counts
            running_samples += node_samples
            node_splits.append(running_samples)
        plt.figure(figsize=(10, 10))
        if previous_frame is None:
            plt.imshow(heatmap, aspect='auto')
        else:
            plt.imshow(previous_frame, aspect='auto')
        if split_lines:
            for split in node_splits[:-1]:
                plt.plot([0, len(self.forest.output_features) - 1],
                         [split, split], color='w')
        plt.savefig(location)
        return heatmap

    def tree_movie(self, location):
        max_depth = max([leaf.level for leaf in self.leaves()])
        previous_frame = None
        for i in range(max_depth):
            self.tree_movie_frame(location + "." + str(i) + ".a.png",
                                  level=i, sorted=False, previous_frame=previous_frame)
            previous_frame = self.tree_movie_frame(
                location + "." + str(i) + ".b.png", level=i, sorted=True)
        self.tree_movie_frame(location + "." + str(i + 1) +
                              ".b.png", level=i, sorted=True, split_lines=False)

    def summary(self, verbose=True):
        nodes = len(self.nodes)
        leaves = len(self.leaves)
        if verbose:
            print("Nodes: {}".format(nodes))
            print("Leaves: {}".format(leaves))

    def aborting_sample_descent(self, sample):
        return self.root.aborting_sample_descent(sample)

    def plot_leaf_counts(self):
        leaves = self.leaves()
        total_samples = sum([x.pop() for x in leaves])
        heatmap = np.zeros((total_samples, len(self.forest.output_features)))
        running_samples = 0
        for leaf in leaves:
            leaf_counts = leaf.node_counts()
            leaf_samples = leaf_counts.shape[0]
            heatmap[running_samples:running_samples +
                    leaf_samples] = leaf_counts
            running_samples += leaf_samples

        ordering = dendrogram(linkage(heatmap.T), no_plot=True)['leaves']
        heatmap = heatmap.T[ordering].T
        plt.figure()
        im = plt.imshow(heatmap, aspect='auto')
        plt.colorbar()
        plt.show()

        return heatmap
    # def cluster_distances(self):
    #     for leaf in self.leaves():


class Forest:

    def __init__(self, trees, input, output, test=None, input_features=None, output_features=None, samples=None, split_labels=None, cache=False):
        if input_features is None:
            input_features = [str(i) for i in range(input.shape[1])]
        if output_features is None:
            output_features = [str(i) for i in range(output.shape[1])]
        if samples is None:
            samples = [str(i) for i in range(input.shape[0])]
        if test is not None:
            self.test = test

        self.cache = cache

        self.truth_dictionary = TruthDictionary(
            output, output_features, samples)

        self.input = input
        self.output = output
        self.samples = samples

        self.input_features = input_features
        self.output_features = output_features

        self.input_dim = input.shape
        self.output_dim = output.shape

        self.trees = trees

        for i, node in enumerate(self.nodes()):
            node.index = i

    def test_forest(roots, inputs, outputs, samples=None):
        test_forest = Forest([], inputs, outputs, samples)
        test_trees = [Tree.test_tree(root, test_forest) for root in roots]
        test_forest.trees = test_trees

########################################################################
########################################################################

        # Node selection methods

      # Methods for picking specific nodes from the forest

########################################################################
########################################################################

    def nodes(self, root=True, depth=None):
        nodes = []
        for tree in self.trees:
            nodes.extend(tree.nodes(root=root))
        if depth is not None:
            nodes = [n for n in nodes if n.level <= depth]
        return nodes

    def leaves(self, depth=None):
        leaves = []
        for tree in self.trees:
            leaves.extend(tree.leaves(depth=depth))
        return leaves

    def level(self, target):
        level = []
        for tree in self.trees:
            level.extend(tree.level(target))
        return level

    def stems(self, depth=None):
        stems = []
        for tree in self.trees:
            stems.extend(tree.stems())
        if depth is not None:
            stems = [s for s in stems if s.level <= depth]
        return stems

    def roots(self):
        return [tree.root for tree in self.trees]

    def node_sample_encoding(self, nodes):

        # ROWS: SAMPLES
        # COLUMNS: NODES

        encoding = np.zeros((len(self.samples), len(nodes)), dtype=bool)
        for i, node in enumerate(nodes):
            encoding[:, i] = node.encoding()
        return encoding

    def node_representation(self, nodes=None, mode='gain', metric=None, pca=0):
        from sklearn.decomposition import IncrementalPCA

        if nodes is None:
            nodes = self.nodes()

        if mode == 'gain':
            print("Gain reduction")
            encoding = self.local_gain_matrix(nodes).T
        elif mode == 'error_ratio':
            encoding = self.error_ratio_matrix(nodes).T
        elif mode == 'additive':
            print("Additive reduction")
            encoding = self.additive_matrix(nodes).T
        elif mode == 'additive_mean':
            print("Additive mean reduction")
            encoding = self.mean_additive_matrix(nodes).T
        elif mode == 'sample':
            print("Sample reduction")
            encoding = self.node_sample_encoding(nodes).T
        elif mode == 'sister':
            print("Sister reduction")
            encoding = self.node_sister_encoding(nodes).T
        elif mode == 'median' or mode == 'medians':
            print("Median reduction")
            encoding = self.median_matrix(nodes)
        elif mode == 'mean' or mode == 'means':
            print("Mean reduction")
            encoding = self.mean_matrix(nodes)

        else:
            raise Exception(f"Mode not recognized:{mode}")

        if pca > 0:
            if pca > encoding.shape[1]:
                print("WARNING, PCA DIMENSION TOO SMALL, PICKING MINIMUM")
                pca = np.min([pca,encoding.shape[1]])
            # print(f"debug:{encoding.shape}")
            from sklearn.decomposition import IncrementalPCA
            model = IncrementalPCA(n_components=pca)
            chunks = int(np.floor(encoding.shape[0] / 10000)) + 1
            last_chunk = encoding.shape[0] - ((chunks - 1) * 10000)
            for i in range(1, chunks):
                print(f"Learning chunk {i}\r", end='')
                model.partial_fit(encoding[(i - 1) * 10000:i * 10000])
            model.partial_fit(encoding[-last_chunk:])
            # transformed = model.transform(encoding)
            # print(f"Chunks:{chunks}")
            transformed = np.zeros((encoding.shape[0], pca))
            for i in range(1, chunks):
                print(f"Transforming chunk {i}\r", end='')
                # print(f"coordinates:{((i-1)*10000,i*10000)}")
                transformed[(
                    i - 1) * 10000:i * 10000] = model.transform(encoding[(i - 1) * 10000:i * 10000])
            # print(f"coordinates:{-last_chunk}")
            transformed[-last_chunk:] = model.transform(encoding[-last_chunk:])
            print("")
            encoding = transformed
            # encoding = PCA(n_components=pca).fit_transform(encoding)

            # encoding = PCA(n_components=pca).fit_transform(encoding)

        if metric is not None:
            representation = squareform(pdist(encoding, metric=metric))
        else:
            representation = encoding

        return representation

    def agglomerate_representation(representation, feature_metric='correlation', sample_metric='cosine'):
        feature_sort = dendrogram(linkage(
            representation.T, metric=feature_metric, method='average'), no_plot=True)['leaves']
        sample_sort = dendrogram(linkage(
            representation, metric=sample_metric, method='average'), no_plot=True)['leaves']
        return representation[sample_sort].T[feature_sort].T

    def node_sister_encoding(self, nodes):
        encoding = np.zeros((len(self.samples), len(nodes)), dtype=int)
        for i, node in enumerate(nodes):
            encoding[:, i][node.sample_mask()] = 1
            # for sample in node.samples():
            #     encoding[sample, i] = 1
            if node.sister() is not None:
                encoding[:, i][node.sister().sample_mask()] = 1
                # for sample in node.sister().samples():
                #     encoding[sample, i] = -1
        return encoding

    def trim(self, depth):

        for tree in self.trees:
            tree.trim(depth)

    def leaf_mask(self):
        leaf_mask = np.zeros(len(self.nodes()), dtype=bool)
        leaf_mask[[l.index for l in self.leaves()]] = True
        return leaf_mask


########################################################################
########################################################################

        # Node/Matrix Methods

      # Methods for turning a set of nodes into an encoding matrix
      # For methods that turn nodes into float matrices see prediction

      # Encoding matrices are boolean matrices, usually node x property
      # Sample encoding matrix would be i,j, where i is ith node and j is whether
      # sample j appears in that node

########################################################################
########################################################################


    def absolute_gain_matrix(self, nodes):
        gains = np.zeros((len(self.output_features), len(nodes)))
        for i, node in enumerate(nodes):
            gains[i] = node.absolute_gains()
        return gains

    def local_gain_matrix(self, nodes):
        gains = np.zeros((len(self.output_features), len(nodes)))
        for i, node in enumerate(nodes):
            gains[:, i] = node.local_gains()
        return gains

    def error_ratio_matrix(self, nodes):
        ratios = np.zeros((len(self.output_features), len(nodes)))
        for i, node in enumerate(nodes):
            ratios[:, i] = node.mean_error_ratio()
        return ratios

    def additive_matrix(self, nodes):
        gains = np.zeros((len(self.output_features), len(nodes)))
        for i, node in enumerate(nodes):
            gains[:, i] = node.additive_gains()
        return gains

    def mean_additive_matrix(self, nodes):
        gains = np.zeros((len(self.output_features), len(nodes)))
        for i, node in enumerate(nodes):
            gains[:, i] = node.additive_mean_gains()
        return gains

    def mean_matrix(self, nodes):
        predictions = np.zeros((len(nodes), len(self.output_features)))
        for i, node in enumerate(nodes):
            predictions[i] = node.means()
        return predictions

    def median_matrix(self, nodes):
        predictions = np.zeros((len(nodes), len(self.output_features)))
        for i, node in enumerate(nodes):
            predictions[i] = node.medians()
        return predictions

    def weight_matrix(self, nodes):
        weights = np.zeros((len(nodes), len(self.output_features)))
        for i, node in enumerate(nodes):
            weights[i] = node.weights
        return weights

    def weighted_prediction_matrix(self, nodes):
        weighted_predictions = np.zeros(
            (len(nodes), len(self.output_features)))
        for i, node in enumerate(nodes):
            weighted_predictions[i] = node.weighted_prediction_cache
        return weighted_predictions

########################################################################
########################################################################

        # Loading/Creation Methods

      # This section deals with methods that load and unload the forest
      # from disk

########################################################################
########################################################################

    def backup(self, location):
        print("Saving forest")
        print(location)
        try:
            with open(location, mode='bw') as f:
                pickle.dump(self, f)
        except:
            print("Failed to save")

    def reconstitute(location):
        with open(location, mode='br') as f:
            return pickle.load(f)

    def set_cache(self, value):
        self.cache = value
        for node in self.nodes():
            node.cache = value

    def reset_cache(self):
        for node in self.nodes():
            node.reset_cache()

    def load(location, prefix="/run", ifh="/run.ifh", ofh='run.ofh', clusters='run.cluster', input="input.counts", output="output.counts", test="test.counts"):

        combined_tree_files = sorted(
            glob.glob(location + prefix + "*.compact"))

        input = np.loadtxt(location + input)
        output = np.loadtxt(location + output)
        ifh = np.loadtxt(location + ifh, dtype=str)
        ofh = np.loadtxt(location + ofh, dtype=str)

        try:
            test = np.loadtxt(location + test)
        except:
            print("Test data not detected")
            pass

        split_labels = None
        try:
            print(f"Looking for clusters:{location+clusters}")
            split_labels = np.loadtxt(location + clusters, dtype=int)
        except:
            pass

        first_forest = Forest([], input_features=ifh, output_features=ofh,
                              input=input, output=output, test=test, split_labels=split_labels)

        trees = []
        for tree_file in combined_tree_files:
            print(f"Loading {tree_file}")
            trees.append(
                Tree(json.load(open(tree_file.strip())), first_forest))

        # first_forest.prototype = Tree(json.load(open(location+prefix+".prototype")),first_forest)

        first_forest.trees = trees

        for i, node in enumerate(first_forest.nodes()):
            node.index = i

        sample_encoding = first_forest.node_sample_encoding(
            first_forest.leaves())

        if np.sum(np.sum(sample_encoding, axis=1) == 0) > 0:
            print("WARNING, UNREPRESENTED SAMPLES")

        return first_forest

    def add_output_feature(self, feature_values, feature_name=None):

        self.reset_cache()

        if not hasattr(self, 'core_output_features'):
            self.core_output_features = len(self.output_features)

        if feature_name is None:
            feature_name = str(len(self.output_features))

        if feature_name in self.truth_dictionary.feature_dictionary.keys():
            raise Exception("REPEAT FEATURE")

        feature_index = len(self.output_features)

        self.output_features = np.concatenate(
            [self.output_features, np.array([feature_name, ])])
        self.output = np.concatenate(
            [self.output, np.array([feature_values, ]).T], axis=1)
        self.truth_dictionary.feature_dictionary[feature_name] = feature_index

        for node in self.nodes():
            node.weights = np.append(node.weights, 1.)

    def remove_output_feature(self, feature):

        if feature not in self.truth_dictionary.feature_dictionary.keys():
            raise Exception("Feature not found")

        self.reset_cache()

        feature_index = self.truth_dictionary.feature_dictionary[feature]

        self.output_features = np.delete(self.output_features, feature_index)
        self.output = np.delete(self.output, feature_index, 1)
        for node in self.nodes():
            node.weights = np.delete(node.weights, feature_index)

        new_feature_dictionary = {f: i for i,
                                  f in enumerate(self.output_features)}
        self.truth_dictionary.feature_dictionary = new_feature_dictionary

    def reset_output_featuers(self):

        if hasattr(self, 'core_output_features'):
            if self.core_output_features < len(self.output_features):
                removed_features = self.output_features[self.core_output_features:]
                print(f"Removing:{removed_features}")
                for feature in removed_features:
                    self.remove_output_feature(feature)

########################################################################
########################################################################

            # PREDICTION METHODS

      # This section deals with methods that allow predictions on
      # samples

########################################################################
########################################################################

    def predict(self, matrix):
        prediction = Prediction(self, matrix)
        return prediction

    def predict_sample_leaves(self, sample):
        sample_leaves = []
        for tree in self.trees:
            sample_leaves.extend(tree.root.predict_sample_leaves(sample))
        return sample_leaves

    def predict_sample_nodes(self, sample):
        sample_nodes = []
        for tree in self.trees:
            sample_nodes.extend(tree.root.predict_sample_nodes(sample))
        return sample_nodes

    def predict_vector_leaves(self, vector, features=None):
        # if features is None:
        # features = self.input_features
        sample = {feature: value for feature,
                  value in zip(range(len(vector)), vector)}
        return self.predict_sample_leaves(sample)

    def predict_vector_nodes(self, vector, features=None):
        # if features is None:
        #     features = self.input_features
        sample = {feature: value for feature,
                  value in zip(range(len(vector)), vector)}
        return self.predict_sample_nodes(sample)

    def predict_node_sample_encoding(self, matrix, leaves=True, depth=None):
        encodings = []
        for i, root in enumerate(self.roots()):
            print(f"Predicting tree:{i}\r", end='')
            encodings.append(root.predict_matrix_encoding(matrix))
            encodings.append(np.ones(matrix.shape[0], dtype=bool))
        print('')
        encoding = np.vstack(encodings)
        if leaves:
            encoding = encoding[self.leaf_mask()]
        if depth is not None:
            depth_mask = np.zeros(encoding.shape[0], dtype=bool)
            for n in self.nodes():
                if n.level <= depth:
                    depth_mask[n.index] = True
            encoding = encoding[depth_mask]
        return encoding

    def feature_weight_matrix(self, nodes):
        fd = self.truth_dictionary.feature_dictionary
        weights = np.zeros((len(nodes), len(fd)))
        for i, node in enumerate(nodes):
            weights[i] = node.weights
        return weights

    def nodes_median_predict_feature(self, nodes, feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_median(feature))
        return predictions

    def nodes_mean_predict_feature(self, nodes, feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_mean(feature))
        return predictions

    def nodes_weighted_predict_feature(self, nodes, feature):
        predictions = np.zeros(len(nodes))
        feature_index = self.truth_dictionary.feature_dictionary[feature]
        for i, node in enumerate(nodes):
            predictions[i] = node.weighted_prediction_cache[feature_index]

        single = np.sum(predictions) * (len(self.nodes()) / len(nodes))

        return single

    def nodes_additive_predict_feature(self, nodes, feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_additive(feature))
        fi = self.truth_dictionary.feature_dictionary[feature]
        weights = []
        for node in nodes:
            weights.append(node.weights[fi])
        return predictions, weights

    def nodes_mean_additive_predict_feature(self, nodes, feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_additive_mean(feature))
        fi = self.truth_dictionary.feature_dictionary[feature]
        weights = []
        for node in nodes:
            weights.append(node.weights[fi])
        return predictions, weights

    def predict_sample(self, sample):

        leaves = self.predict_sample_leaves(sample)
        consolidated_predictions = self.mean_matrix(leaves)
        return np.mean(consolidated_predictions, axis=0)

    def predict_sample_leaf_cluster(self, sample):
        leaves = self.predict_sample_leaves(sample)
        leaf_clusters = [l.leaf_cluster for l in leaves]
        return np.mode(leaf_clusters)[0][0]

    def nodes_mean_predict_vector(self, nodes):

        predictions = self.node_representation(nodes, mode='mean')
        single_prediction = np.sum(predictions, axis=0) / len(nodes)

        return single_prediction

    def predict_additive(self, matrix, depth=8):
        encoding = self.predict_node_sample_encoding(
            matrix, depth=depth, leaves=False).T
        feature_predictions = self.mean_additive_matrix(self.nodes()).T
        prediction = np.dot(encoding.astype(dtype=int), feature_predictions)
        prediction /= len(self.trees)

        return prediction

    def predict_weighted(self, matrix, depth=8):
        encoding = self.predict_node_sample_encoding(
            matrix, depth=depth, leaves=False).T
        weighted_predictions = self.node_representation(
            self.nodes(), mode='weighted_predictions').T
        prediction = np.dot(encoding.astype(dtype=int), weighted_predictions)

        return prediction

    def predict_matrix(self, matrix, features=None, weighted=True):

        encoding_prediction = self.predict_node_sample_encoding(matrix).T
        feature_predictions = self.mean_matrix(self.leaves())
        scaling = np.dot(encoding_prediction,
                         np.ones(feature_predictions.shape))

        prediction = np.dot(encoding_prediction, feature_predictions) / scaling
        prediction[scaling == 0] = 0
        return prediction

    def predict_matrix_clusters(self, matrix, features=None):

        cluster_odds = np.array(
            [len(s.samples()) / self.output.shape[0] for s in self.sample_clusters])
        cluster_features = [self.truth_dictionary.feature_dictionary[f"sample_cluster_{i}"] for i in range(
            len(self.sample_clusters))]

        print("Odds ready")

        predicted_encoding = self.predict_node_sample_encoding(matrix).T

        print("Leaves predicted")

        feature_predictions = self.mean_matrix(
            self.leaves()).T[cluster_features].T

        print("Means predicted")

        scaling = np.dot(predicted_encoding, np.ones(
            feature_predictions.shape))

        print("Scaling computed")

        print(
            f"E:{predicted_encoding.shape},F:{feature_predictions.shape},S:{scaling.shape}")

        cluster_predictions = np.dot(
            predicted_encoding, feature_predictions) / scaling
        cluster_predictions /= cluster_odds

        return np.argmax(cluster_predictions, axis=1)

    def predict_factor_matrix(self, matrix):
        predicted_encoding = self.predict_node_sample_encoding(
            matrix, leaves=False)
        predicted_factors = np.zeros(
            (matrix.shape[0], len(self.split_clusters)))
        predicted_factors[:, 0] = 1.
        for i in range(1, len(self.split_clusters[0:])):
            predicted_factors[:, i] = self.split_clusters[i].predict_sister_scores(
                predicted_encoding)
        return predicted_factors


########################################################################
########################################################################

        # Clustering methods

########################################################################
########################################################################

    def split_labels(self, depth=3):
        nodes = self.nodes(depth=depth)
        return np.array([n.split_cluster for n in nodes])

    def set_sample_labels(self, sample_labels):

        self.sample_labels = np.array(sample_labels).astype(dtype=int)

        cluster_set = set(sample_labels)
        clusters = []
        for cluster in cluster_set:
            samples = np.arange(len(self.sample_labels))[
                self.sample_labels == cluster]
            clusters.append(SampleCluster(self, samples, int(cluster)))

        self.sample_clusters = clusters

        one_hot = np.array([sample_labels == x.id for x in clusters])

        self.sample_cluster_encoding = one_hot

    def cluster_samples_simple(self, override=False, pca=None, resolution=1, **kwargs):

        if hasattr(self, 'sample_labels') and override:
            self.reset_sample_clusters()

        counts = self.output

        if hasattr(self, 'sample_clusters') and not override:
            print("Clustering has already been done")
            return self.sample_labels
        else:
            if pca is not None:
                counts = PCA(n_components=pca).fit_transform(counts)
                # self.set_sample_labels(sdg.fit_predict(counts, *args, **kwargs))
                self.set_sample_labels(hacked_louvain(
                    fast_knn(counts, **kwargs), resolution=resolution))
            else:
                self.set_sample_labels(hacked_louvain(
                    fast_knn(counts, **kwargs), resolution=resolution))

        return self.sample_labels

    def cluster_samples_encoding(self, override=False, pca=None, depth=None, resolution=1, **kwargs):

        # Todo: remove this hack
        if depth is not None:
            depth_limit = depth

        if hasattr(self, 'sample_labels') and override:
            self.reset_sample_clusters()

        leaves = self.leaves(depth=depth)

        encoding = self.node_sample_encoding(leaves)

        if pca is not None:
            encoding = PCA(n_components=pca).fit_transform(encoding)

        if hasattr(self, 'sample_clusters') and not override:
            print("Clustering has already been done")
        else:
            # self.set_sample_labels(sdg.fit_predict(encoding, *args, **kwargs))
            self.set_sample_labels(
                1 + hacked_louvain(fast_knn(encoding, **kwargs), resolution=resolution))

        return self.sample_labels

    def cluster_samples_coocurrence(self, override=False, *args, **kwargs):

        if hasattr(self, 'sample_labels') and override:
            self.reset_sample_clusters()

        leaves = self.leaves()
        encoding = self.node_sample_encoding(leaves)
        coocurrence = coocurrence_matrix(encoding)

        if hasattr(self, 'sample_clusters') and not override:
            print("Clustering has already been done")
            return self.sample_labels
        else:
            self.set_sample_labels(sdg.fit_predict(
                coocurrence, precomputed=True, *args, **kwargs))

        return self.sample_labels

    def cluster_samples_leaf_cluster(self, override=False, *args, **kwargs):

        if hasattr(self, 'sample_labels') and override:
            self.reset_sample_clusters()

        leaves = [n for n in self.nodes() if hasattr(n, 'leaf_cluster')]
        encoding = self.node_sample_encoding(leaves)
        leaf_clusters = np.array([l.leaf_cluster for l in leaves])
        leaf_cluster_sizes = np.array(
            [np.sum(leaf_clusters == cluster) for cluster in range(len(set(leaf_clusters)))])

        print(f"encoding dimensions: {encoding.shape}")

        # from scipy.stats import mode

        sample_labels = []

        for leaf_mask in encoding:
            leaf_cluster_counts = np.array(
                [np.sum(leaf_clusters[leaf_mask] == lc) for lc in range(len(leaf_cluster_sizes))])
            odds = leaf_cluster_counts / leaf_cluster_sizes
            sample_labels.append(np.argmax(odds))

        # sample_labels = [mode(leaf_clusters[mask])[0][0] for mask in encoding]

        self.set_sample_labels(np.array(sample_labels).astype(dtype=int))

        return self.sample_labels

    def set_leaf_labels(self, labels):

        leaves = self.leaves()
        self.leaf_labels = np.array(labels)

        cluster_set = set(self.leaf_labels)

        clusters = []

        for cluster in cluster_set:
            leaf_index = np.arange(len(self.leaf_labels))[
                self.leaf_labels == cluster]
            clusters.append(NodeCluster(
                self, [leaves[i] for i in leaf_index], cluster))

        self.leaf_clusters = clusters
        for leaf, label in zip(leaves, self.leaf_labels):
            leaf.leaf_cluster = label

    def cluster_leaves_samples(self, override=False, pca=None, depth=None, *args, **kwargs):

        leaves = self.leaves(depth=depth)
        encoding = self.node_sample_encoding(leaves).T

        if pca is not None:
            encoding = PCA(n_components=pca).fit_transform(encoding)

        if hasattr(self, 'leaf_clusters') and not override:
            print("Clustering has already been done")
            return self.leaf_labels
        else:
            self.set_leaf_labels(sdg.fit_predict(encoding, *args, **kwargs))

        return self.leaf_labels

    def cluster_leaves_predictions(self, override=False, mode='mean', *args, **kwargs):

        leaves = self.leaves()
        predictions = self.node_representation(leaves, mode=mode)

        if hasattr(self, 'leaf_clusters') and not override:
            print("Clustering has already been done")
            return self.leaf_labels
        else:
            self.set_leaf_labels(sdg.fit_predict(predictions, *args, **kwargs))

        return self.leaf_labels

    def node_change_absolute(self, nodes1, nodes2):
        # First we obtain the medians for the nodes in question
        n1_predictions = self.nodes_mean_predict_vector(nodes1)
        n2_predictions = self.nodes_mean_predict_vector(nodes2)
        difference = n2_predictions - n1_predictions

        # Then sort by difference and return
        feature_order = np.argsort(difference)
        ordered_features = np.array(self.output_features)[feature_order]
        ordered_difference = difference[feature_order]

        return ordered_features, ordered_difference

    def node_change_log_fold(self, nodes1, nodes2):

        # First we obtain the medians for the nodes in question
        n1_means = np.mean(self.mean_matrix(nodes1), axis=0)
        n2_means = np.mean(self.mean_matrix(nodes2), axis=0)

        # We evaluate the ratio of median values
        # log_fold_change = np.log2(n2_medians/n1_medians)
        log_fold_change = np.log2(n2_means / n1_means)

        # Because we are working with a division and a log, we have to filter for
        # results that don't have division by zero issues

        degenerate_mask = np.isfinite(log_fold_change)
        non_degenerate_features = np.array(self.output_features)[
            degenerate_mask]
        non_degenerate_changes = log_fold_change[degenerate_mask]

        # Finally sort and return
        feature_order = np.argsort(non_degenerate_changes)
        ordered_features = non_degenerate_features[feature_order]
        ordered_difference = non_degenerate_changes[feature_order]

        return ordered_features, ordered_difference

    def node_change_logistic(self, nodes1, nodes2):

        from sklearn.linear_model import LogisticRegression

        n1_counts = self.mean_matrix(nodes1)
        n2_counts = self.mean_matrix(nodes2)

        combined = np.concatenate([n1_counts, n2_counts], axis=0)

        scaled = sklearn.preprocessing.scale(combined)

        labels = np.zeros(n1_counts.shape[0] + n1_counts.shape[0])

        labels[n2_counts.shape[0]:] = 1
        #
        # print(f"Logistic debug:{n1_counts.shape},{n2_counts.shape},{combined.shape},{labels.shape}")

        model = LogisticRegression().fit(combined, labels)

        feature_sort = np.argsort(model.coef_[0, :])

        ordered_features = np.array(self.output_features)[feature_sort]
        ordered_coefficients = model.coef_[0, :][feature_sort]

        return ordered_features, ordered_coefficients

    def node_encoding_error(self, node_sample_encoding, truth):

        # Given a prediction by some nodes for a set of samples

        return ordered_features, ordered_coefficients

    def cluster_leaf_total_predictions(self, override=False, *args, **kwargs):

        if hasattr(self, 'leaf_clusters') and not override:
            print("Clustering has already been done")
            return self.leaf_labels
        else:
            leaves = self.leaves()
            meta_predictions = self.node_total_predictions(leaves)
            self.leaf_labels = np.array(sdg.fit_predict(
                meta_predictions, *args, **kwargs))

        cluster_set = set(self.leaf_labels)

        clusters = []

        for cluster in cluster_set:
            leaf_index = np.arange(len(self.leaf_labels))[
                self.leaf_labels == cluster]
            clusters.append(NodeCluster(
                self, [leaves[i] for i in leaf_index], cluster))

        self.leaf_clusters = clusters
        for leaf, label in zip(leaves, self.leaf_labels):
            leaf.leaf_cluster = label

        return self.leaf_labels

    def cluster_features(self, depth=None, **kwargs):
        gain_matrix = self.node_representation(
            self.nodes(depth=depth), mode='additive_mean')
        return sdg.fit_predict(gain_matrix.T, **kwargs)

    def sdg_cluster_representation(representation, **kwargs):
        return np.array(sdg.fit_predict(representation, **kwargs))

    def interpret_splits(self, override=False, mode='additive_mean', metric='cosine', pca=100, relatives=True, resolution=1, k=5, depth=6, **kwargs):

        if pca > len(self.output_features):
            print("WARNING, PCA DIMENSION GREATER THAN FEATURE DIMENSION, PICKING MINIMUM")
            pca = len(self.output_features)

        nodes = np.array(self.nodes(root=True, depth=depth))

        stem_mask = np.array([n.level != 0 for n in nodes])
        root_mask = np.logical_not(stem_mask)

        labels = np.zeros(len(nodes)).astype(dtype=int)

        if relatives:

            print("Relativistic distance (heh)")

            own_representation = self.node_representation(
                nodes[stem_mask], mode=mode, pca=pca)
            sister_representation = self.node_representation(
                [n.sister() for n in nodes[stem_mask]], mode=mode, pca=pca)

            print("Running double knn")
            knn = double_fast_knn(own_representation,
                                  sister_representation, k=k, metric=metric)

        else:
            representation = self.node_representation(
                nodes, mode=mode, metric=None, pca=pca)

            print("Running knn")
            knn = fast_knn(representation, k=k, metric=metric)

        print("Calling clustering procedure")
        labels[stem_mask] = 1 + hacked_louvain(knn, resolution=resolution)

        for node, label in zip(nodes, labels):
            node.set_split_cluster(label)
            # node.split_cluster = label

        cluster_set = set(labels)
        clusters = []
        for cluster in cluster_set:
            split_index = np.arange(len(labels))[labels == cluster]
            clusters.append(NodeCluster(
                self, [nodes[i] for i in split_index], cluster))

        # split_order = np.argsort(labels)
        # split_order = dendrogram(linkage(reduction,metric='cos',method='average'),no_plot=True)['leaves']

        self.split_clusters = clusters

        return labels

    def external_split_labels(self, nodes, labels, roots=False):

        cluster_set = set(labels)
        clusters = []

        for node, label in zip(nodes, labels):
            node.set_split_cluster(label)
            # node.split_cluster = label

        if not roots:
            for node in self.roots():
                node.set_split_cluster(0)
            clusters.append(NodeCluster(self, self.roots(), 0))

        for cluster in cluster_set:
            split_index = np.arange(len(labels))[labels == cluster]
            clusters.append(NodeCluster(
                self, [nodes[i] for i in split_index], cluster))

        # split_order = np.argsort(labels)
        # split_order = dendrogram(linkage(reduction,metric='cos',method='average'),no_plot=True)['leaves']

        self.split_clusters = clusters

    def recursive_interpretation(self, cluster, mode='additive', **kwargs):

        print(f"Subclustering {cluster.name()}")
        cluster_nodes = cluster.nodes

        children = [child for node in cluster_nodes for child in node.children]
        print(f"{len(children)} children")

        child_representation = self.node_representation(children, mode=mode)
        child_labels = sdg.fit_predict(
            child_representation, **kwargs).astype(dtype=int) + len(self.split_clusters)
        new_labels = set(child_labels)

        for child, label in zip(children, child_labels):
            child.set_split_cluster(label)

        for new_label in new_labels:
            print(new_label)
            cluster_nodes = [c for c in children if hasattr(
                c, 'split_cluster') if c.split_cluster == new_label]
            self.split_clusters.append(
                NodeCluster(self, cluster_nodes, new_label))

    def create_root_cluster(self):

        roots = [t.root for t in self.trees]

        for node in roots:
            node.set_split_cluster(0)

        self.split_clusters = [NodeCluster(self, roots, 0), ]

    def plot_representation(self, representation, labels=None, metric='cos', pca=False):

        if metric is not None:
            # image = reduction[split_order].T[split_order].T
            agg_f = dendrogram(linkage(
                representation, metric=metric, method='average'), no_plot=True)['leaves']
            agg_s = dendrogram(linkage(
                representation.T, metric=metric, method='average'), no_plot=True)['leaves']
            image = representation[agg_f].T[agg_s].T
        else:
            # feature_order = dendrogram(linkage(reduction.T+1,metric='cosine',method='average'),no_plot=True)['leaves']
            # image = reduction[split_order].T[feature_order].T
            try:
                agg_f = dendrogram(linkage(
                    representation.T, metric='cosine', method='average'), no_plot=True)['leaves']
            except:
                agg_f = dendrogram(linkage(
                    representation.T, metric='cityblock', method='average'), no_plot=True)['leaves']
            try:
                agg_s = dendrogram(linkage(
                    representation, metric='cosine', method='average'), no_plot=True)['leaves']
            except:
                agg_s = dendrogram(linkage(
                    representation, metric='cityblock', method='average'), no_plot=True)['leaves']

            image = representation[agg_s].T[agg_f].T
            image = representation[agg_s].T[agg_f].T

        plt.figure(figsize=(10, 10))
        plt.imshow(image, aspect='auto', cmap='bwr')
        plt.show()

        if labels is not None:

            split_order = np.argsort(labels)

            image = representation[split_order].T[agg_f].T
            plt.figure(figsize=(10, 10))
            plt.imshow(image, aspect='auto', cmap='bwr')
            plt.show()

    def reset_sample_clusters(self):
        try:
            for i in range(len(self.sample_clusters)):
                self.remove_output_feature(f'sample_cluster_{i}')
            del self.sample_clusters
            del self.sample_cluster_encoding
            del self.sample_labels
        except:
            print("No sample clusters")

    def reset_split_clusters(self):
        try:
            del self.split_clusters
            for node in self.nodes():
                node.child_clusters = ([], [])
                if hasattr(node, 'split_cluster'):
                    del node.split_cluster
        except:
            print("No split clusters")

    def reset_leaf_clusters(self):
        try:
            del self.leaf_clusters
            del self.leaf_labels
            for node in self.nodes():
                if hasattr(node, 'leaf_cluster'):
                    del node.leaf_cluster
        except:
            print("No leaf clusters")

    def reset_clusters(self):

        self.reset_sample_clusters()
        self.reset_split_clusters()
        self.reset_leaf_clusters()


########################################################################
########################################################################

        # Plotting methods

########################################################################
########################################################################

    # def plot_counts(self,no_plot=False):
    #
    #     if not no_plot:
    #         cell_sort = dendrogram(linkage(encoding,metric='cos',method='average'),no_plot=True)['leaves']
    #         leaf_sort = dendrogram(linkage(encoding.T,metric='cos',method='average'),no_plot=True)['leaves']
    #
    #         plt.figure(figsize=(10,10))
    #         plt.imshow(encoding[cell_sort].T[leaf_sort].T,cmap='binary')
    #         plt.show()
    #
    #     return cell_sort,leaf_sort,self.ouput_counts


    def plot_sample_clusters(self, colorize=True, label=True):
        # if not hasattr(self,'leaf_clusters'):
        #     print("Warning, leaf clusters not detected")
        #     return None
        if not hasattr(self, 'sample_clusters'):
            print("Warning, sample clusters not detected")
            return None

        coordinates = self.coordinates(no_plot=True)

        cluster_coordiantes = np.zeros((len(self.sample_clusters), 2))

        for i, cluster in enumerate(self.sample_clusters):
            cluster_sample_mask = self.sample_labels == cluster.id
            mean_coordinates = np.mean(
                coordinates[cluster_sample_mask], axis=0)
            cluster_coordiantes[i] = mean_coordinates

        combined_coordinates = np.zeros(
            (self.output.shape[0] + len(self.sample_clusters), 2))

        combined_coordinates[0:self.output.shape[0]] = coordinates

        combined_coordinates[self.output.shape[0]:] = cluster_coordiantes

        highlight = np.ones(combined_coordinates.shape[0]) * 10
        highlight[len(self.sample_labels):] = [len(cluster.samples)
                                               for cluster in self.sample_clusters]
        # for i,cluster in enumerate(self.sample_clusters):
        #
        #     highlight[self.counts.shape[0] + i:] = len(cluster.samples/10)

        combined_labels = np.zeros(
            self.output.shape[0] + len(self.sample_clusters))
        if colorize:
            combined_labels[0:len(self.sample_labels)] = self.sample_labels
            combined_labels[len(self.sample_labels):] = [
                cluster.id for cluster in self.sample_clusters]

        cluster_names = [cluster.name() for cluster in self.sample_clusters]
        cluster_coordiantes = combined_coordinates[len(self.sample_labels):]

        if label:
            f = plt.figure(figsize=(10, 10))
            plt.title("Sample Coordinates")
            plt.scatter(combined_coordinates[:, 0], combined_coordinates[:,
                                                                         1], s=highlight, c=combined_labels, cmap='rainbow')
            for cluster, coordinates in zip(cluster_names, cluster_coordiantes):
                plt.text(*coordinates, cluster, verticalalignment='center',
                         horizontalalignment='center')
            # plt.savefig("./tmp.delete.png",dpi=300)
        else:
            f = plt.figure(figsize=(20, 20))
            plt.title("Sample Coordinates")
            plt.scatter(combined_coordinates[:len(self.samples), 0], combined_coordinates[:len(
                self.samples), 1], s=10, c=combined_labels[:len(self.samples)], cmap='rainbow')
            plt.savefig("./tmp.delete.png", dpi=300)
        return f

    def plot_split_clusters(self, colorize=True):
        if not hasattr(self, 'split_clusters'):
            print("Warning, split clusters not detected")
            return None

        coordinates = self.coordinates(no_plot=True)

        cluster_coordinates = np.zeros((len(self.split_clusters), 2))

        for i, cluster in enumerate(self.split_clusters):
            cluster_coordinates[i] = cluster.coordinates(
                coordinates=coordinates)

        combined_coordinates = np.zeros(
            (self.output.shape[0] + len(self.split_clusters), 2))

        combined_coordinates[0:self.output.shape[0]] = coordinates

        combined_coordinates[self.output.shape[0]:] = cluster_coordinates

        highlight = np.ones(combined_coordinates.shape[0])
        highlight[self.output.shape[0]:] = [
            len(cluster.nodes) for cluster in self.split_clusters]

        combined_labels = np.zeros(
            self.output.shape[0] + len(self.split_clusters))
        if colorize:
            combined_labels[self.output.shape[0]:] = [
                cluster.id for cluster in self.split_clusters]

        cluster_names = [cluster.name() for cluster in self.split_clusters]
        cluster_coordiantes = combined_coordinates[-1 *
                                                   len(self.split_clusters):]

        f = plt.figure(figsize=(5, 5))
        plt.title("TSNE-Transformed Sample Coordinates")
        plt.scatter(combined_coordinates[:, 0], combined_coordinates[:,
                                                                     1], s=highlight, c=combined_labels, cmap='rainbow')
        for cluster, coordinates in zip(cluster_names, cluster_coordiantes):
            plt.text(*coordinates, cluster, verticalalignment='center',
                     horizontalalignment='center')
        plt.show()

        return f

    def sample_cluster_feature_matrix(self, features=None):
        if features is None:
            features = self.output_features
        coordinates = np.zeros((len(self.sample_clusters), len(features)))
        for i, sample_cluster in enumerate(self.sample_clusters):
            for j, feature in enumerate(features):
                # coordinates[i,j] = sample_cluster.feature_median(feature)
                coordinates[i, j] = sample_cluster.feature_mean(feature)
        return coordinates

    def split_cluster_feature_matrix(self, features=None):
        if features is None:
            coordinates = np.zeros(
                (len(self.split_clusters), len(self.output_features)))
            for i, split_cluster in enumerate(self.split_clusters):
                coordinates[i] = np.mean(
                    self.mean_additive_matrix(split_cluster.nodes), axis=1)
        else:
            coordinates = np.zeros((len(self.split_clusters), len(features)))
            for i, split_cluster in enumerate(self.split_clusters):
                for j, feature in enumerate(features):
                    coordinates[i, j] = split_cluster.feature_mean_additive(
                        feature)
        return coordinates

    def split_cluster_mean_matrix(self, features=None):
        if features is None:
            coordinates = np.zeros(
                (len(self.split_clusters), len(self.output_features)))
            for i, split_cluster in enumerate(self.split_clusters):
                coordinates[i] = np.mean(
                    self.mean_matrix(split_cluster.nodes), axis=0)
        else:
            coordinates = np.zeros((len(self.split_clusters), len(features)))
            for i, split_cluster in enumerate(self.split_clusters):
                for j, feature in enumerate(features):
                    coordinates[i, j] = split_cluster.feature_mean(feature)
        return coordinates

    def split_cluster_domain_mean_matrix(self, features=None):
        if features is None:
            coordinates = np.zeros(
                (len(self.split_clusters), len(self.output_features)))
            for i, split_cluster in enumerate(self.split_clusters):
                coordinates[i] = np.mean(self.mean_matrix(
                    split_cluster.nodes + split_cluster.sisters()), axis=0)
        else:
            coordinates = np.zeros((len(self.split_clusters), len(features)))
            for i, split_cluster in enumerate(self.split_clusters):
                for j, feature in enumerate(features):
                    coordinates[i, j] = self.forest.nodes_mean_predict_feature(
                        split_cluster.nodes + split_cluster.sisters())
        return coordinates

    def tsne(self, no_plot=False, pca=100, override=False, **kwargs):
        if not hasattr(self, 'tsne_coordinates') or override:
            if pca:
                pca = np.min([pca, self.output.shape[0], self.output.shape[1]])
                self.tsne_coordinates = TSNE().fit_transform(
                    PCA(n_components=pca).fit_transform(self.output))
            else:
                self.tsne_coordinates = TSNE().fit_transform(self.output)

        if not no_plot:
            plt.figure()
            plt.title("TSNE-Transformed Sample Coordinates")
            plt.scatter(
                self.tsne_coordinates[:, 0], self.tsne_coordinates[:, 1], s=.1, **kwargs)
            plt.show()

        return self.tsne_coordinates

    def tsne_encoding(self, no_plot=False, override=False, pca=False, **kwargs):
        if not hasattr(self, 'tsne_coordinates') or override:
            if pca:
                self.tsne_coordinates = TSNE().fit_transform(
                    PCA(n_components=pca).fit_transform(self.node_sample_encoding(self.leaves())))
            else:
                self.tsne_coordinates = TSNE().fit_transform(
                    self.node_sample_encoding(self.leaves()))

        if not no_plot:
            plt.figure()
            plt.title("TSNE-Transformed Sample Coordinates")
            plt.scatter(
                self.tsne_coordinates[:, 0], self.tsne_coordinates[:, 1], s=.1, **kwargs)
            plt.show()

        return self.tsne_coordinates

    def pca(self, no_plot=False, override=False, **kwargs):
        if not hasattr(self, 'pca_coordinates') or override:
            self.pca_coordinates = PCA(
                n_components=2).fit_transform(self.output)

        if not no_plot:
            plt.figure()
            plt.title("PCA-Transformed Sample Coordinates")
            plt.scatter(
                self.pca_coordinates[:, 0], self.pca_coordinates[:, 1], s=.1, **kwargs)
            plt.show()

        return self.pca_coordinates

    def umap(self, no_plot=False, override=False, **kwargs):
        if not hasattr(self, 'umap_coordinates') or override:
            self.umap_coordinates = UMAP().fit_transform(self.output)

        if not no_plot:
            plt.figure()
            plt.title("UMAP-Transformed Sample Coordinates")
            plt.scatter(
                self.umap_coordinates[:, 0], self.umap_coordinates[:, 1], s=.1, **kwargs)
            plt.show()

        return self.umap_coordinates

    def umap_encoding(self, no_plot=False, override=False, **kwargs):
        if not hasattr(self, 'umap_coordinates') or override:
            self.umap_coordinates = UMAP().fit_transform(
                self.node_sample_encoding(self.leaves()))

        if not no_plot:
            plt.figure()
            plt.title("UMAP-Transformed Sample Coordinates")
            plt.scatter(
                self.umap_coordinates[:, 0], self.umap_coordinates[:, 1], s=.1, **kwargs)
            plt.show()

        return self.umap_coordinates

    def coordinates(self, type=None, scaled=True, **kwargs):

        if type is None:
            if hasattr(self, 'coordinate_type'):
                type = self.coordinate_type
            else:
                type = 'tsne'

        self.coordinate_type = type

        type_functions = {
            'tsne': self.tsne,
            'tsne_encoding': self.tsne_encoding,
            'pca': self.pca,
            'umap': self.umap,
            'umap_encoding': self.umap_encoding,
        }

        coordinates = type_functions[type](**kwargs)

        if scaled:
            coordinates = sklearn.preprocessing.scale(coordinates)

        return coordinates

    def plot_manifold(self, depth=3):

        f = self.plot_sample_clusters()

        def recursive_tree_plot(parent, children, figure, level=0):
            # print("Recursion debug")
            # print(f"p:{parent}")
            # print(f"c:{children}")
            pc = self.split_clusters[parent].coordinates()
            vectors = []
            for child, sub_children in children:
                if child == len(self.split_clusters):
                    continue
                cc = self.split_clusters[child].coordinates()
                # print("coordinates")
                # print(f"p{parent}:{pc}")
                # print(f"c{child}:{cc}")
                # v = pc - cc
                v = cc - pc
                # print(f"v:{v}")
                plt.figure(figure.number)
                # plt.arrow(pc[0],pc[1],v[0],v[1],width=(depth+1-level)*.3,length_includes_head=True)
                plt.arrow(pc[0], pc[1], v[0], v[1], length_includes_head=True)
                vectors.append((pc, v))
                figure, cv = recursive_tree_plot(
                    child, sub_children, figure, level=level + 1)
                vectors.extend(cv)
            # print("===============")
            return figure, vectors

        f, v = recursive_tree_plot(self.likely_tree[0], self.likely_tree[1], f)
        #
        # plt.savefig("./tmp.delete.png",dpi=300)
        #
        # return f,v

    def plot_braid_vectors(self):

        f = self.plot_sample_clusters(label=False)
        ax = f.add_axes([0, 0, 1, 1])

        for cluster in self.split_clusters:
            ax = cluster.plot_braid_vectors(ax=ax, scatter=False, show=False)

        plt.savefig("./tmp.delete.png", dpi=300)

        return f


########################################################################
########################################################################

        # Consensus tree methods

########################################################################
########################################################################

    def split_cluster_transition_matrix(self, depth=3):

        nodes = np.array(self.nodes(depth=depth))
        labels = self.split_labels(depth=depth)
        clusters = [cluster.id for cluster in self.split_clusters]
        transitions = np.zeros((len(clusters) + 1, len(clusters) + 1))

        for cluster in clusters:
            mask = labels == cluster
            cluster_nodes = nodes[mask]
            for node in cluster_nodes:
                node_state = node.split_cluster
                for child in node.children:
                    if hasattr(child, 'split_cluster'):
                        child_state = child.split_cluster
                    else:
                        child_state = len(clusters)
                    transitions[node_state, child_state] += 1
                if len(node.children) == 0:
                    transitions[node_state, -1] += 1
                if node.parent is None:
                    transitions[-1, node_state] += 1

        self.split_cluster_transitions = transitions

        return transitions

    def directional_matrix(self):

        downstream_frequency = np.zeros(
            (len(self.split_clusters), len(self.split_clusters)), dtype=int)
        upstream_frequency = np.zeros(
            (len(self.split_clusters), len(self.split_clusters)), dtype=int)

        for cluster in self.split_clusters:

            children = cluster.children()

            for child in children:
                if hasattr(child, 'split_cluster'):
                    downstream_frequency[cluster.id, child.split_cluster] += 1

            ancestors = cluster.ancestors()

            for ancestor in ancestors:
                if hasattr(ancestor, 'split_cluster'):
                    upstream_frequency[cluster.id, ancestor.split_cluster] += 1

        return upstream_frequency, downstream_frequency

    def conditional_split_probability(self):

        _, down_matrix = self.directional_matrix()
        total_descendents = np.sum(down_matrix, axis=1)
        conditional_probability = (down_matrix.T / (total_descendents + 1)).T

        return conditional_probability

    def probability_enrichment(self):

        _, down_matrix = self.directional_matrix()
        total_descendents = np.sum(down_matrix, axis=1)
        conditional_probability = (down_matrix.T / (total_descendents + 1)).T

        raw_probability = conditional_probability[0]
        enrichment = conditional_probability / raw_probability

        return enrichment

    def partial_dependence(self):
        total_nodes = self.nodes()
        path_matrix = np.zeros((len(self.split_clusters), len(total_nodes)))
        for node in total_nodes:
            path_matrix[node.split_cluster, node.index] = True
            if hasattr(node, 'split_cluster'):
                for descendant in node.nodes():
                    path_matrix[node.split_cluster, descendant.index] = True
        path_covariance = np.cov(path_matrix)
        precision = np.linalg.pinv(path_covariance)
    #     return precision

        precision_normalization = np.sqrt(
            np.outer(np.diag(precision), np.diag(precision)))
        path_partials = precision / precision_normalization

        path_partials[np.isnan(path_partials)] = 0

        return path_partials

    def split_cluster_odds_ratios(self):

        cluster_populations = [len(c.nodes) for c in self.split_clusters]
        total_nodes = np.sum(cluster_populations)

        downstream_frequency = np.ones(
            (len(self.split_clusters), len(self.split_clusters)))
        nephew_frequency = np.ones(
            (len(self.split_clusters), len(self.split_clusters)))

        for cluster in self.split_clusters[1:]:

            children = cluster.children()
            nephews = cluster.sisters() + [s.nodes()
                                           for s in cluster.sisters()]

            for child in children:
                if hasattr(child, 'split_cluster'):
                    downstream_frequency[cluster.id, child.split_cluster] += 1

            for nephew in nephews:
                if hasattr(nephew, 'split_cluster'):
                    nephew_frequency[cluster.id, nephew.split_cluster] += 1

            downstream_frequency[cluster.id] /= len(cluster.nodes) + 1
            nephew_frequency[cluster.id] /= len(cluster.nodes) + 1

        odds_ratio = downstream_frequency / nephew_frequency

        return odds_ratio

    ###############
    # Here we have several alternative methods for constructing the consensus tree.
    ###############

    # Most of them depend on these two helper methods that belong only in this scope

    # The finite tree method takes a prototype, which is a list of lists.
    # Each element in the list corresponds to which elements consider this element their parent

    def finite_tree(cluster, prototype, available):
        print(cluster)
        print(prototype)
        children = []
        try:
            available.remove(cluster)
        except:
            pass
        for child in prototype[cluster]:
            if child in available:
                available.remove(child)
                children.append(child)
        return [cluster, [Forest.finite_tree(child, prototype, available) for child in children]]

    def reverse_tree(tree):
        root = tree[0]
        sub_trees = tree[1]
        child_entries = {}
        for sub_tree in sub_trees:
            for child, path in Forest.reverse_tree(sub_tree).items():
                path.append(root)
                child_entries[child] = path
        child_entries[root] = []
        return child_entries

    # End helpers

    def most_likely_tree(self, depth=3, transitions=None):

        if transitions is None:
            transitions = self.split_cluster_transition_matrix(depth=depth)

        transitions[np.identity(transitions.shape[0]).astype(dtype=bool)] = 0

        clusters = [cluster.id for cluster in self.split_clusters]

        proto_tree = [[] for cluster in clusters]

        for cluster in clusters:
            parent = np.argmax(transitions[:-1, cluster])
            proto_tree[parent].append(cluster)

        print(f"Clusters:{clusters}")
        print(f"Prototype:{proto_tree}")
        print(f"Prototype length:{len(proto_tree)}")
        print(f"Transitions:{transitions}")
        print(f"Transition mtx shape: {transitions.shape}")

        tree = []
        # entry = np.argmax(transitions[-1])
        entry = 0

        tree = Forest.finite_tree(
            cluster=entry, prototype=proto_tree, available=clusters)
        rtree = Forest.reverse_tree(tree)

        self.likely_tree = tree
        self.reverse_likely_tree = rtree

        return tree

    def maximum_spanning_tree(self, depth=3, mode="transition_matrix", transitions=None):

        if mode == "transition_matrix":
            distances = self.split_cluster_transition_matrix(depth=depth)
            # np.diag(distances) = 0
            distances[:, -1] = 0
        elif mode == "odds_ratio":
            distance = 1. / self.split_cluster_odds_ratios()
        elif mode == "dependence":
            distance = self.partial_dependence()
        elif mode == "means":
            mean_matrix = self.split_cluster_mean_matrix()
            domain_matrix = self.split_cluster_domain_mean_matrix()
            print(mean_matrix)
            print(domain_matrix)
            distances = 1. - cdist(mean_matrix, domain_matrix, metric="cosine")
            print(distances)
            print(distances.shape)
            # distances = squareform(1. - pdist(mean_matrix,domain_matrix,metric="cosine"))
        elif mode == "samples":
            cluster_values = np.array([c.sample_scores()
                                       for c in self.split_clusters])
            distances = squareform(1. - pdist(cluster_values, metric="cosine"))
        else:
            raise Exception(f"Not a valid mode: {mode}")

        mst = np.array(scipy.sparse.csgraph.minimum_spanning_tree(
            distances * -1).todense()) * -1

        mst = np.maximum(mst, mst.T)

        clusters = set(range(len(self.split_clusters)))

        print("Max tree debug")
        print(distances)
        print(mst)

        def finite_tree(cluster, available):
            # print(cluster)
            # print(mst[cluster])
            children = []
            try:
                available.remove(cluster)
            except:
                pass
            for child in np.arange(mst.shape[0])[mst[cluster] > 0]:
                if child in available:
                    available.remove(child)
                    children.append(child)
            return [cluster, [finite_tree(child, available) for child in children]]

        tree = finite_tree(0, clusters)
        rtree = Forest.reverse_tree(tree)

        self.likely_tree = tree
        self.reverse_likely_tree = rtree

        return tree


#########################################################
# HTML Visualization methods
#########################################################

    def html_directory(self):

        location = Path(__file__).parent.parent.absolute()
        location = str(location) + "/html/"

        return location

    def location(self):
        return str(Path(__file__).parent.parent.absolute())

    def html_tree_summary(self, n=3, mode="ud", custom=None, labels=None, features=None, primary=True, cmap='viridis', secondary=True, figsize=(30, 30), output=None):

        from shutil import copyfile, rmtree
        from os import makedirs
        from json import dumps as jsn_dumps

        # First we'd like to make sure we are operating from scratch in the html directory:

        if n > (self.output.shape[1] / 2):
            print ("WARNING, PICKED N THAT IS TOO LARGE, SETTING LOWER")
            n = max(int(self.output.shape[1]/2),1)

        if output is None:
            location = self.location()
            html_location = location + "/html/"
            rmtree(html_location)
            makedirs(html_location)
        else:
            location = self.location()
            html_location = output

        for split_cluster in self.split_clusters:
            print(f"Summarizing:{split_cluster.name()}")
            split_cluster.html_cluster_summary(n=n, plot=False, output=output)

        copyfile(location + "/tree_template.html",
                 html_location + "tree_template.html")

        with open(html_location + "tree_template.html", 'a') as html_report:

            # Helper methods:

            # Find the leaves of a tree
            def leaves(tree):
                l = []
                for child in tree[1]:
                    l.extend(leaves(child))
                if len(l) < 1:
                    l.append(tree[0])
                return l

            # Find out how many levels are in this tree (eg its maximum depth)
            def levels(tree, level=0):
                l = []
                for child in tree[1]:
                    l.extend(levels(child, level=level + 1))
                l.append(level)
                return l

            # First we compute the width/height of the individual cells
            width = 1 / len(leaves(self.likely_tree))
            height = 1 / (max(levels(self.likely_tree)) + 1)

            # This function determines where to place everything, all coordinates are from 0 to 1, so are fractions of a canvas.
            # The coordinates are placed in a list we pass to the function, because I didn't want to think about how to avoid doing this
            def recursive_axis_coordinates(tree, child_coordinates, limits=[0, 0]):
                [x, y] = limits
                child_width = 0
                # First we go lower in recursive layer and find how many children we need to account for from this leaf
                for child in tree[1]:
                    cw = recursive_axis_coordinates(child, child_coordinates, [
                                                    x + child_width, y + (height)])
                    child_width += cw
                if child_width == 0:
                    child_width = width
                # print("Recursive tree debug")
                # print(f"x:{x},y:{y}")
                # print(f"{tree[0]}")
                # print(f"cw:{child_width}")

                # We have to place the current leaf at the average position of all leaves below
                padding = (child_width - width) / 2
                coordinates = [x + padding + (width * .5), y + (height * .5)]
                # print(f"coordinates:{coordinates}")

                child_coordinates.append([int(tree[0]), coordinates])

                return child_width

            # Here we actually call the recursive function

            coordinates = [[width, height], ]
            recursive_axis_coordinates(self.likely_tree, coordinates)
            coordinates[1:] = list(sorted(coordinates[1:], key=lambda x: x[0]))

            # Now we have to create an HTML-ish string in order to pass this information on to
            # the javascript without reading local files (siiigh)

            coordinate_json_string = jsn_dumps(coordinates)
            name_json_string = jsn_dumps(
                [c.name() for c in self.split_clusters])
            coordinate_html = f'<script> let treeCoordinates = {coordinate_json_string};</script>'
            name_html = f'<script> let clusterNames = {name_json_string};</script>'

            # Finally, we append to the template to pass on the information

            html_report.write(coordinate_html)
            html_report.write(name_html)

            # Next we want to calculate the connections between each node:

            # First we flatten the tree:

            def flatten_tree(tree):
                flat = []
                for child in tree[1]:
                    flat.extend(flatten_tree(child))
                flat.append([tree[0], [c[0] for c in tree[1]]])
                return flat

            flat_tree = flatten_tree(self.likely_tree)

            # Then we insert connections:

            if primary:

                # print(f"Coordinates:{coordinates}")
                # print(f"Flat tree:{flat_tree}")

                primary_connections = []

                for i, children in flat_tree:
                    x, y = coordinates[1:][i][1]
                    center_x = x + (width * .5)
                    center_y = y + (height * .5)
                    for ci in children:
                        cx, cy = coordinates[1:][ci][1]
                        child_center_x = cx + (width / 2)
                        child_center_y = cy + (height / 2)

                        # We would like to set the arrow thickness to be proportional to the mean population of the child
                        if ci < len(self.split_clusters):
                            cp = self.split_clusters[ci].mean_population()
                        else:
                            cp = 1
                        primary_connections.append([x, y, cx, cy, cp])
                        # primary_connections.append([center_x,center_y,child_center_x,child_center_y,cp])

                primary_connection_json = jsn_dumps(primary_connections)
                primary_connection_html = f'<script>let connections = {primary_connection_json};</script>'

                # Again, we append to the template to pass on the information

                html_report.write(primary_connection_html)

            elif secondary:

                # If we want to indicate secondary connections:
                secondary_connections = []

                for i in range(len(self.split_clusters)):
                    for j in range(len(self.split_clusters)):

                        # We scroll through every element in the split cluster transition
                        # matrix

                        # if self.split_cluster_transitions[i,j] > 0:
                        if self.dependence_scores[i, j] < 0:

                            # If the transitions are non-zero we obtain the coordinates

                            x, y = coordinates[1:][i][1]
                            center_x = x + (width * .5)
                            center_y = y - (height * .5)
                            cx, cy = coordinates[1:][j][1]
                            child_center_x = cx + (width / 2)
                            child_center_y = cy + (height / 2)

                            # And plot a line with a weight equivalent to the number of transitions

                            # cp = self.split_cluster_transitions[i,j]
                            # total = np.sum(self.split_cluster_transitions[i])
                            # arrow_canvas.plot([center_x,child_center_x],[center_y,child_center_y],alpha=min(1,cp/total*2),linewidth=(cp**2)*.01,transform=arrow_canvas.transAxes)

                            # Alternatively, plot a line with a weight equivalent to the partial correlation of split clusters:

                            cp = self.dependence_scores[i, j]
                            secondary_connections.append(
                                [center_x, center_y, child_center_x, child_center_y, cp])

                secondary_connection_json = jsn_dumps(secondary_connections)
                secondary_connection_html = f'<script> let connections = {secondary_connection_json}</script>'

                # Finally, we append to the template to pass on the information

                html_report.write(secondary_connection_html)

            else:
                raise Exception("Pick a connectivity!")

            # Now we need to loop over available clusters to place the cluster decorations into the template

            for cluster in self.split_clusters:
                cluster_summary_json = cluster.json_cluster_summary(n=n)
                cluster_summary_html = f'<script> summaries["cluster_{cluster.id}"] = {cluster_summary_json};</script>'
                html_report.write(cluster_summary_html)

        from subprocess import run
        run(["open", html_location + "tree_template.html"])

    def split_cluster_leaves(self):
        def tree_leaves(tree):
            leaves = []
            for child in tree[1]:
                leaves.extend(tree_leaves(child))
            if len(tree[1]) < 1:
                leaves.append(tree[0])
            return leaves

        tree = self.likely_tree
        leaf_clusters = [self.split_clusters[i] for i in tree_leaves(tree)]

        return leaf_clusters

    def cluster_samples_by_split_clusters(self, override=False, *args, **kwargs):

        if hasattr(self, 'sample_clusters') and not override:
            print("Clustering has already been done")
            return self.sample_labels

        leaf_split_clusters = self.split_cluster_leaves()
        leaf_split_cluster_sample_scores = np.array(
            [c.sample_counts() for c in leaf_split_clusters])
        sample_labels = np.array([np.argmax(
            leaf_split_cluster_sample_scores[:, i]) for i in range(len(self.samples))])

        self.set_sample_labels(sample_labels)

        print([c.id for c in self.split_clusters])

        return self.sample_labels

    def most_likely_sample_leaf_cluster(self, node_sample_encoding):
        cluster_scores = np.zeros(
            (node_sample_encoding.shape[0], len(self.leaf_clusters)))
        for cluster in self.leaf_clusters:
            cluster_mask = np.array(
                [n.leaf_cluster == cluster.id for n in self.leaves()])
            print(cluster.id)
            print(cluster_mask)
            print(np.sum(cluster_mask))
            for sample, sample_encoding in enumerate(node_sample_encoding):
                cluster_scores[sample, int(cluster.id)] = np.sum(
                    np.logical_and(sample_encoding, cluster_mask).astype(dtype=int))
        plt.figure()
        plt.imshow(cluster_scores, aspect='auto', cmap='gray')
        plt.colorbar()
        plt.show()
        sample_clusters = np.argmax(cluster_scores, axis=1)
        return sample_clusters

    def factor_matrix(self):
        matrix = np.zeros((len(self.samples), len(self.split_clusters)))
        for i, cluster in enumerate(self.split_clusters):
            matrix[:, i] = cluster.sister_scores()
        return matrix

    def global_correlations(self):

        global_correlations = np.corrcoef(self.output.T)

        return global_correlations


class Prediction:

    def __init__(self, forest, matrix):
        self.forest = forest
        self.matrix = matrix
        self.mode = None
        self.nse = None
        self.nme = None
        self.nae = None
        self.nwp = None
        self.smc = None
        self.factors = None

    def node_sample_encoding(self):
        if self.nse is None:
            self.nse = self.forest.predict_node_sample_encoding(
                self.matrix, leaves=False)
        return self.nse

    def node_mean_encoding(self):
        if self.nme is None:
            self.nme = self.forest.mean_matrix(self.forest.nodes())
        return self.nme

    def node_additive_encoding(self):
        if self.nae is None:
            self.nae = self.forest.mean_additive_matrix(self.forest.nodes())
        return self.nae

    def additive_prediction(self, depth=8):
        encoding = self.node_sample_encoding().T
        feature_predictions = self.node_additive_encoding().T
        prediction = np.dot(encoding.astype(dtype=int), feature_predictions)
        prediction /= len(self.forest.trees)

        return prediction

    def mean_prediction(self):
        leaf_mask = self.forest.leaf_mask()
        encoding_prediction = self.node_sample_encoding()[leaf_mask].T
        feature_predictions = self.node_mean_encoding()[leaf_mask]
        scaling = np.dot(encoding_prediction,
                         np.ones(feature_predictions.shape))

        prediction = np.dot(encoding_prediction, feature_predictions) / scaling
        prediction[scaling == 0] = 0
        return prediction

    def prediction(self, mode=None):

        if mode is None:
            mode = self.mode
        if mode is None:
            mode = "additive_mean"
        self.mode = mode

        if mode == "additive_mean":
            prediction = self.additive_prediction()
        elif mode == "mean":
            prediction = self.mean_prediction()
        else:
            raise Exception(f"Not a valid mode {mode}")

        return prediction

    def residuals(self, truth=None, mode=None):

        if truth is None:
            truth = self.matrix

        prediction = self.prediction(mode=mode)
        residuals = truth - prediction

        return residuals

    def null_residuals(self, truth=None):

        if truth is None:
            truth = self.matrix

        centered_truth = truth - np.mean(truth, axis=0)

        return centered_truth

    def node_residuals(self, node, truth=None):

        if truth is None:
            truth = self.matrix

        sample_predictions = self.node_sample_encoding()[node.index]
        feature_predictions = self.node_mean_encoding()[node.index]
        residuals = truth[sample_predictions] - feature_predictions

        return residuals

    def node_feature_remaining_error(self, nodes):

        per_node_fraction = []

        for node in nodes:

            if node.parent is not None:

                node_residuals = self.node_residuals(node)
                remaining_error = np.sum(np.power(node_residuals, 2), axis=0)

                sister_residuals = self.node_residuals(node.sister())
                remaining_error += np.sum(np.power(sister_residuals, 2), axis=0)

                parent_residuals = self.node_residuals(node.parent)
                original_error += np.sum(np.power(parent_residuals, 2), axis=0)

                # Avoid nans:
                # (there's gotta be a better way) *billy mays theme starts*

                remaining_error += 1
                original_error += 1

                per_node_fraction.append(remaining_error / original_error)

            else:
                per_node_fraction.append(1)

        return np.mean(np.array(per_node_fraction), axis=0)

    def sample_clusters(self):

        if self.smc is None:

            leaf_mask = self.forest.leaf_mask()
            encoding_prediction = self.node_sample_encoding()[leaf_mask].T
            leaf_means = np.array([l.sample_cluster_means() for l in self.forest.leaves()])
            scaling = np.dot(encoding_prediction,
                             np.ones(leaf_means.shape))

            prediction = np.dot(encoding_prediction, leaf_means) / scaling
            prediction[scaling == 0] = 0

            self.smc = np.argmax(prediction, axis=1)

        return self.smc

    def factor_matrix(self):
        predicted_encoding = self.node_sample_encoding()
        predicted_factors = np.zeros(
            (self.matrix.shape[0], len(self.forest.split_clusters)))
        predicted_factors[:, 0] = 1.
        for i in range(1, len(self.forest.split_clusters[0:])):
            predicted_factors[:, i] = self.forest.split_clusters[i].predict_sister_scores(
                predicted_encoding)
        return predicted_factors

    def compare_sample_clusters(self, other):

        self_samples = self.sample_clusters()
        other_samples = other.sample_clusters()

        plt.figure()
        plt.title("Sample Cluster Frequency, Self vs Other")
        plt.xlabel("Cluster")
        plt.ylabel("Frequency")
        plt.xticks(np.arange(len(self.forest.sample_clusters)))
        plt.hist(self_samples, alpha=.5, density=True, label="Self",
                 bins=np.arange(len(self.forest.sample_clusters) + 1))
        plt.hist(other_samples, alpha=.5, density=True, label="Other",
                 bins=np.arange(len(self.forest.sample_clusters) + 1))
        plt.legend()
        plt.show()
        pass

    def compare_factors(self, other, no_plot=False, log=True, bins=20):

        from scipy.stats import entropy
        from scipy.stats import ks_2samp
        from scipy.stats import ranksums

        own_factors = self.factor_matrix()
        other_factors = other.factor_matrix()

        symmetric_entropy = []
        bin_interval = 2. / bins

        for i, (own_f, other_f) in enumerate(zip(own_factors.T, other_factors.T)):
            own_hist = np.histogram(
                own_f, bins=np.arange(-1, 1, bin_interval))[0] + 1
            other_hist = np.histogram(
                other_f, bins=np.arange(-1, 1, bin_interval))[0] + 1
            own_prob = own_hist / np.sum(own_hist)
            other_prob = other_hist / np.sum(other_hist)
        #     print("##############################")
            forward_entropy = entropy(own_prob, qk=other_prob)
            reverse_entropy = entropy(other_prob, qk=own_prob)
            average_entropy = (forward_entropy + reverse_entropy) / 2
            symmetric_entropy.append(average_entropy)
            print(f"{i} Entropy: {average_entropy}")
            ks = ks_2samp(own_f, other_f)
            print(f"Kolmogorov-Smirnov: {ks}")
            mwu = ranksums(own_f, other_f)
            print(f"Rank Sum: {mwu}")
            if not no_plot:
                own_log_prob = np.log(own_hist / np.sum(own_hist))
                other_log_prob = np.log(other_hist / np.sum(other_hist))

                lin_min = np.min(
                    [np.min(own_log_prob), np.min(other_log_prob)])

                plt.figure(figsize=(5, 5))
                plt.title(f"{self.forest.split_clusters[i].name()} Comparison")
                plt.scatter(own_log_prob, other_log_prob,
                            c=np.arange(-1, 1, bin_interval)[:-1], cmap='seismic')
                plt.plot([0, lin_min], [0, lin_min], color='red', alpha=.5)
                plt.xlabel("Factor Frequency, Self (Log Probability)")
                plt.ylabel("Factor Frequency, Other (Log Probability)")
                plt.colorbar(label="Factor Value")
                plt.show()

        return symmetric_entropy

    def prediction_report(self, truth=None, n=10, mode="additive_mean", no_plot=False):

        null_square_residuals = np.power(self.null_residuals(truth=truth), 2)
        null_residual_sum = np.sum(null_square_residuals)

        forest_square_residuals = np.power(self.residuals(truth=truth), 2)
        predicted_residual_sum = np.sum(forest_square_residuals)

        explained = predicted_residual_sum / null_residual_sum

        print(explained)

        # Add one here to avoid divisions by zero, but this is bad
        # Need better solution

        null_feature_residuals = np.sum(null_square_residuals, axis=0) + 1
        forest_feature_residuals = np.sum(forest_square_residuals, axis=0) + 1

        features_explained = forest_feature_residuals / null_feature_residuals

        if not no_plot:
            plt.figure()
            plt.title("Distribution of Target Coefficients of Determination")
            plt.hist(features_explained, bins=np.arange(0, 1, .05), log=True)
            plt.xlabel("CoD")
            plt.ylabel("Frequency")
            plt.show()

        feature_sort = np.argsort(features_explained)

        print(
            (self.forest.output_features[feature_sort[:n]], features_explained[feature_sort[:n]]))
        print((self.forest.output_features[feature_sort[-n:]],
               features_explained[feature_sort[-n:]]))

        null_sample_residuals = np.sum(null_square_residuals, axis=1) + 1
        forest_sample_residuals = np.sum(forest_square_residuals, axis=1) + 1

        samples_explained = forest_sample_residuals / null_sample_residuals

        sample_sort = np.argsort(samples_explained)

        print(sample_sort[:n], samples_explained[sample_sort[:n]])
        print(sample_sort[-n:], samples_explained[sample_sort[-n:]])

        if not no_plot:
            plt.figure()
            plt.title("Distribution of Sample Coefficients of Determination")
            plt.hist(samples_explained, bins=np.arange(0, 1, .05), log=True)
            plt.xlabel("CoD")
            plt.ylabel("Frequency")
            plt.show()

        return features_explained, samples_explained

    def feature_mse(self, truth=None, mode='additive_mean'):

        residuals = self.residuals(truth=truth, mode=mode)
        mse = np.mean(np.power(residuals, 2), axis=0)

        return mse

    def jackknife_feature_mse_variance(self, mode='additive_mean'):

        squared_residuals = np.power(self.residuals(mode=mode), 2)
        residual_sum = np.sum(squared_residuals, axis=0)
        excluded_sum = residual_sum - squared_residuals
        excluded_mse = excluded_sum / (squared_residuals.shape[0] - 1)
        jackknife_variance = np.var(
            excluded_mse, axis=0) * (squared_residuals.shape[0] - 1)

        return jackknife_variance

    def feature_remaining_error(self, truth=None, mode='additive_mean'):

        null_square_residuals = np.power(self.null_residuals(truth=truth), 2)
        null_residual_sum = np.sum(null_square_residuals, axis=0)

        forest_square_residuals = np.power(self.residuals(truth=truth), 2)
        predicted_residual_sum = np.sum(forest_square_residuals, axis=0)

        remaining = predicted_residual_sum / null_residual_sum

        return remaining

    def feature_coefficient_of_determination(self, truth=None, mode='additive_mean'):
        remaining_error = self.feature_remaining_error(truth=truth, mode=mode)
        return 1 - remaining_error

    def compare_feature_residuals(self, other, mode='rank_sum', no_plot=True):
        from scipy.stats import ranksums
        from scipy.stats import mannwhitneyu
        from scipy.stats import ks_2samp
        from scipy.stats import t

        self_residuals = self.residuals()
        other_residuals = other.residuals()

        if mode == 'rank_sum':
            results = [ranksums(self_residuals[:, i], other_residuals[:, i])
                       for i in range(self_residuals.shape[1])]
        elif mode == 'mann_whitney_u':
            results = [mannwhitneyu(self_residuals[:, i], other_residuals[:, i])
                       for i in range(self_residuals.shape[1])]
        elif mode == 'kolmogorov_smirnov':
            results = [ks_2samp(self_residuals[:, i], other_residuals[:, i])
                       for i in range(self_residuals.shape[1])]

        elif mode == 'mse_delta':

            self_mse = self.feature_mse()
            other_mse = other.feature_mse()

            delta_mse = self_mse - other_mse

            jackknife_std = np.sqrt(self.jackknife_feature_mse_variance())
            jackknife_z = delta_mse / jackknife_std

            prob = t.pdf(jackknife_z, len(self.forest.samples))

            results = list(zip(jackknife_z, prob))

        elif mode == 'cod_delta':

            print("WARNING")
            print("NO SIGNFIFICANCE SCORE IS PROVIDED FOR DIFFERENCE IN COD")

            self_cod = self.feature_coefficient_of_determination()
            other_cod = other.feature_coefficient_of_determination()

            delta_cod = self_cod - other_cod

            results = list(zip(delta_cod, np.ones(len(delta_cod))))

        else:
            raise Exception(f"Did not recognize mode:{mode}")

        if not no_plot:
            plt.figure()
            plt.title("Distribution of Test Statistics")
            plt.hist([test for test, p in results], log=True, bins=50)
            plt.xlabel("Test Statistic")
            plt.ylabel("Frequency")
            plt.show()

            plt.figure()
            plt.title("Distribution of P Values")
            plt.hist([p for test, p in results], log=True, bins=50)
            plt.xlabel("P Value")
            plt.ylabel("Frequency")
            plt.show()

        return results


class TruthDictionary:

    def __init__(self, counts, header, samples=None):

        self.counts = counts
        self.header = header
        self.feature_dictionary = {}

        self.sample_dictionary = {}
        for i, feature in enumerate(header):
            self.feature_dictionary[feature.strip('""').strip("''")] = i
        if samples is None:
            samples = map(lambda x: str(x), range(counts.shape[0]))
        for i, sample in enumerate(samples):
            self.sample_dictionary[sample.strip("''").strip('""')] = i

    def look(self, sample, feature):
        #         print(feature)
        return(self.counts[self.sample_dictionary[sample], self.feature_dictionary[feature]])


class SampleCluster:

    def __init__(self, forest, samples, id):
        self.id = id
        self.samples = samples
        self.forest = forest

    def name(self):
        if hasattr(self, 'stored_name'):
            return self.stored_name
        else:
            return str(self.id)

    def set_name(self, name):
        self.stored_name = name

    def mask(self):
        mask = np.zeros(len(self.forest.samples), dtype=bool)
        mask[self.samples] = True
        return mask

    def median_feature_values(self):
        return np.median(self.forest.output[self.samples], axis=0)

    def mean_feature_values(self):
        return np.mean(self.forest.output[self.samples], axis=0)

    def increased_features(self, n=50, plot=True):
        initial_means = np.mean(self.forest.output)
        current_means = self.mean_feature_values()

        difference = current_means - initial_means
        feature_order = np.argsort(difference)
        ordered_features = np.array(self.forest.output_features)[feature_order]
        ordered_difference = difference[feature_order]

        if plot:
            plt.figure(figsize=(10, 8))
            plt.title("Upregulated Genes")
            plt.scatter(np.arange(n), ordered_difference[-n:])
            plt.xlim(0, n)
            plt.xlabel("Gene Symbol")
            plt.ylabel("Increase (LogTPM)")
            plt.xticks(np.arange(
                n), ordered_features[-n:], rotation=45, verticalalignment='top', horizontalalignment='right')
            plt.show()

        return ordered_features, ordered_difference

    def decreased_features(self, n=50, plot=True):
        initial_means = np.mean(self.forest.output)
        current_means = self.mean_feature_values()

        difference = current_means - initial_means
        feature_order = np.argsort(difference)
        ordered_features = np.array(self.forest.output_features)[feature_order]
        ordered_difference = difference[feature_order]

        if plot:
            plt.figure(figsize=(10, 8))
            plt.title("Upregulated Genes")
            plt.scatter(np.arange(n), ordered_difference[:n])
            plt.xlim(0, n)
            plt.xlabel("Gene Symbol")
            plt.ylabel("Increase (LogTPM)")
            plt.xticks(np.arange(
                n), ordered_features[:n], rotation=45, verticalalignment='top', horizontalalignment='right')
            plt.show()

        return ordered_features, ordered_difference

    def logistic_features(self, n=50):

        from sklearn.linear_model import LogisticRegression

        mask = self.mask()

        scaled = sklearn.preprocessing.scale(self.forest.input)

        model = LogisticRegression().fit(scaled, mask)

        coefficient_sort = np.argsort(model.coef_[0])

        sorted_features = self.forest.input_features[coefficient_sort][-n:]
        sorted_coefficients = model.coef_[0][coefficient_sort][-n:]

        return sorted_features, sorted_coefficients

    def leaf_encoding(self):
        leaves = self.forest.leaves()
        encoding = self.forest.node_sample_encoding(leaves)
        encoding = encoding[self.samples]
        return encoding

    def leaf_counts(self):
        encoding = self.leaf_encoding()
        return np.sum(encoding, axis=0)

    def leaf_cluster_frequency(self, plot=True):
        leaf_counts = self.leaf_counts()
        leaf_cluster_labels = self.forest.leaf_labels
        leaf_clusters = sorted(list(set(leaf_cluster_labels)))
        leaf_cluster_counts = []
        for leaf_cluster in leaf_clusters:
            cluster_mask = leaf_cluster_labels == leaf_cluster
            leaf_cluster_counts.append(np.sum(leaf_counts[cluster_mask]))
        if plot:
            plt.figure()
            plt.title(
                f"Distribution of Leaf Clusters in Sample Cluster {self.name()}")
            plt.bar(np.arange(len(leaf_clusters)), leaf_cluster_counts,)
            plt.ylabel("Frequency")
            plt.xlabel("Leaf Cluster")
            plt.xticks(np.arange(len(leaf_clusters)), leaf_clusters)
            plt.show()

        return leaf_clusters, leaf_cluster_counts

    def feature_median(self, feature):
        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        vector = self.forest.output[self.samples][:, fi]
        return np.median(vector)

    def feature_mean(self, feature):
        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        vector = self.forest.output[self.samples][:, fi]
        return np.mean(vector)


class NodeCluster:

    def __init__(self, forest, nodes, id):
        self.id = id
        self.nodes = nodes
        self.forest = forest

    def name(self):
        if hasattr(self, 'stored_name'):
            return self.stored_name
        else:
            return str(self.id)

    def set_name(self, name):
        self.stored_name = name

################################################################################
# Basic manipulation methods (Nodes/Representations etc)
################################################################################

    def encoding(self):
        return self.forest.node_sample_encoding(self.nodes)

    def node_mask(self):
        mask = np.zeros(len(self.forest.nodes()), dtype=bool)
        for node in self.nodes:
            mask[node.index] = True
        return mask

    def sisters(self):
        return [n.sister() for n in self.nodes if n.sister() is not None]

    def children(self):

        return [c for n in self.nodes for c in n.nodes()]

    def parents(self):

        return [n.parent for n in self.nodes if n.parent is not None]

    def ancestors(self):

        return [a for n in self.nodes for a in n.ancestors()]
    #
    # def weighted_feature_predictions(self):
    #     return self.forest.weighted_node_vector_prediction(self.nodes)


################################################################################
# Consensus tree methods. Kinda weird/hacky. Need to rethink
################################################################################


    def parent_cluster(self):
        try:
            return self.forest.split_clusters[self.forest.reverse_likely_tree[self.id][0]]
        except:
            return self

    def child_clusters(self):

        def traverse(tree):
            children = []
            if tree[0] == self.id:
                children.extend([c[0] for c in tree[1]])
            else:
                for child in tree[1]:
                    children.extend(traverse(child))
            return children

        indices = traverse(self.forest.likely_tree)
        print(indices)
        return [self.forest.split_clusters[i] for i in indices]

    def sibling_clusters(self):

        def traverse(tree):
            siblings = []
            if self.id in [c[0] for c in tree[1]]:
                siblings.extend([c[0] for c in tree[1]])
                own_index = siblings.index(self.id)
                siblings.pop(own_index)
            else:
                for child in tree[1]:
                    siblings.extend(traverse(child))
            return siblings

        indices = traverse(self.forest.likely_tree)
        print(indices)
        return [self.forest.split_clusters[i] for i in indices]


##############################################################################
# Feature change methods (eg changes relative to related nodes)
##############################################################################


    def changed_absolute_root(self):
        roots = self.forest.nodes(root=True, depth=0)
        ordered_features, ordered_difference = self.forest.node_change_absolute(
            roots, self.nodes)
        return ordered_features, ordered_difference

    def changed_log_root(self):
        roots = self.forest.nodes(root=True, depth=0)
        ordered_features, ordered_difference = self.forest.node_change_log_fold(
            roots, self.nodes)
        return ordered_features, ordered_difference

    def changed_absolute(self):
        parents = [n.parent for n in self.nodes if n.parent is not None]
        ordered_features, ordered_difference = self.forest.node_change_absolute(
            parents, self.nodes)
        return ordered_features, ordered_difference

    def changed_log_fold(self):
        parents = [n.parent for n in self.nodes if n.parent is not None]
        ordered_features, ordered_difference = self.forest.node_change_log_fold(
            parents, self.nodes)
        return ordered_features, ordered_difference

    def changed_absolute_sister(self):
        sisters = [n.sister() for n in self.nodes if n.sister() is not None]
        ordered_features, ordered_difference = self.forest.node_change_absolute(
            sisters, self.nodes)
        return ordered_features, ordered_difference

    def changed_log_sister(self):
        sisters = [n.sister() for n in self.nodes if n.sister() is not None]
        ordered_features, ordered_difference = self.forest.node_change_log_fold(
            sisters, self.nodes)
        return ordered_features, ordered_difference

    def ranked_additive(self):
        additive = self.forest.node_representation(self.nodes, mode='additive')
        mean_additive = np.mean(additive, axis=0)
        sort = np.argsort(mean_additive)
        return self.forest.output_features[sort], mean_additive[sort]

    def log_sister(self, plot=True):
        sisters = [n.sister() for n in self.nodes]
        ordered_features, ordered_difference = self.forest.node_change_log_fold(
            sisters, self.nodes)
        return ordered_features, ordered_difference

    def logistic_sister(self, n=50, plot=True):
        sisters = [n.sister() for n in self.nodes]
        ordered_features, ordered_difference = self.forest.node_change_logistic(
            sisters, self.nodes)
        return ordered_features, ordered_difference

    def coordinates(self, coordinates=None):

        if coordinates is None:
            coordinates = self.forest.coordinates(no_plot=True)

        sample_scores = self.sample_scores()
        sample_scores = np.power(sample_scores, 2)
        mean_coordinates = np.dot(
            sample_scores, coordinates) / np.sum(sample_scores)

        return mean_coordinates

    def regression(self):
        from sklearn.linear_model import LinearRegression

        scores = self.sister_scores()
        weights = np.abs(scores)
        top_features = self.top_split_features()
        selection = self.forest.output.T[top_features].T

        factor_model = LinearRegression().fit(
            selection, scores.reshape(-1, 1), sample_weight=weights)
        output_model = LinearRegression().fit(
            selection, self.forest.output, sample_weight=weights)

        return top_features, factor_model, output_model

    def error_ratio(self, sample_matrix=None, scores=None):

        # We would like to weigh the observed error by by the total of the cluster and its sisters, then by samples in the cluster only
        # The ratio should give us an idea of how much of the variance is explained by the cluster split.

        if sample_matrix is None:
            sample_matrix = self.forest.output
            scores = self.sister_scores()
        if scores is None:
            scores = self.sister_scores()

        positive = scores.copy()
        positive[scores < 0] = 0

        absolute = np.abs(scores)

        positive_mean = np.mean(self.forest.mean_matrix(self.nodes), axis=0)
        absolute_mean = np.mean(
            self.forest.mean_matrix(self.parents()), axis=0)
        # positive_mean = np.average(sample_matrix, axis=0, weights=positive)
        # absolute_mean = np.average(sample_matrix, axis=0, weights=absolute)

        positive_error = np.dot(
            np.power(sample_matrix - positive_mean, 2).T, positive)
        absolute_error = np.dot(
            np.power(sample_matrix - absolute_mean, 2).T, positive)

        print(
            f"Error: P:{positive_error},A:{absolute_error}")

        print(
            f"Error Ratio:{positive_error / absolute_error}")

        error_ratio = (positive_error + 1) / (absolute_error + 1)

        ratio_sort = np.argsort(error_ratio)

        sorted_features = self.forest.output_features[ratio_sort]
        sorted_ratios = error_ratio[ratio_sort]

        return sorted_features, sorted_ratios
    #
    # def error_ratio(self):
    #
    #     # We want to figure out how much of the error in a given feature is explained by the nodes in this cluster
    #     # We want the overall ratio of error in the parents vs the error in the nodes of this cluster
    #
    #     parent_total_error = np.ones(len(self.forest.output_features))
    #     sister_total_error = np.ones(len(self.forest.output_features))
    #     self_total_error = np.ones(len(self.forest.output_features))
    #
    #     for node in self.nodes:
    #         if node.parent is not None:
    #             self_total_error += node.squared_residual_sum()
    #             sister_total_error += node.sister().squared_residual_sum()
    #             parent_total_error += node.parent.squared_residual_sum()
    #
    #     print(self_total_error)
    #     print(sister_total_error)
    #     print(parent_total_error)
    #
    #     error_ratio = (self_total_error + sister_total_error) / \
    #         parent_total_error
    #
    #     ratio_sort = np.argsort(error_ratio)
    #
    #     sorted_features = self.forest.output_features[ratio_sort]
    #     sorted_ratios = error_ratio[ratio_sort]
    #
    #     return sorted_features, sorted_ratios

    def weighted_covariance(self):
        scores = self.sister_scores()
        weights = np.abs(scores)

        weighted_means = np.average(
            self.forest.output, axis=0, weights=weights)
        centered_data = self.forest.output - weighted_means
        covariance = np.dot(centered_data.T, centered_data)

    def top_split_features(self, n=10):
        from sklearn.linear_model import LinearRegression

        split_features = list(
            set([n.feature() for n in self.nodes if n.feature() is not None]))
        selection = self.forest.output.T[split_features].T
        factor_scores = self.sister_scores()
        model = LinearRegression().fit(selection, factor_scores,
                                       sample_weight=np.abs(factor_scores))
        top_features = [split_features[i]
                        for i in np.argsort(np.abs(model.coef_))]
        return top_features[-n:]

################################################################################
# Mean/summary methods (describe cluster contents)
################################################################################

    def feature_mean(self, feature):
        return np.mean(self.forest.nodes_mean_predict_feature(self.nodes, feature))

    def feature_additive(self, feature):
        return np.mean([n.feature_additive(feature) for n in self.nodes])

    def feature_mean_additive(self, feature):
        return np.mean(self.forest.nodes_mean_additive_predict_feature(self.nodes, feature))

    def feature_means(self, features):
        return np.array([self.feature_mean(feature) for feature in features])

    def feature_additives(self, features):
        return np.array([self.feature_additive(feature) for feature in features])

    def feature_mean_additives(self, features):
        return np.array([self.feature_mean_additive(feature) for feature in features])

    def mean_level(self):
        return np.mean([n.level for n in self.nodes])

    def mean_population(self):
        return np.mean([n.pop() for n in self.nodes])

    def local_correlations(self):

        weights = self.sample_counts()

        weighted_covariance = np.cov(self.forest.output.T, fweights=weights)
        diagonal = np.diag(weighted_covariance)
        normalization = np.sqrt(np.abs(np.outer(diagonal, diagonal)))
        correlations = weighted_covariance / normalization
        correlations[normalization == 0] = 0
        correlations[np.identity(correlations.shape[0], dtype=bool)] = 1.
        return correlations

    def most_local_correlations(self, n=10):

        global_correlations = self.forest.global_correlations()
        local_correlations = self.local_correlations()

        delta = local_correlations - global_correlations

        ranks = np.argsort(np.abs(delta.flatten()))

        tiled_indices = np.tile(
            np.arange(delta.shape[0]), ((delta.shape[0]), 1))

        ranked = list(zip(tiled_indices.flatten()[
            ranks], tiled_indices.T.flatten()[ranks]))

        return ranked[-n:]

##############################################################################
# Sample membership methods
##############################################################################

    def sample_scores(self):
        cluster_encoding = self.encoding()
        return np.sum(cluster_encoding, axis=1) / (cluster_encoding.shape[1] + 1)

    def sample_counts(self):
        encoding = self.encoding()
        return np.sum(encoding, axis=1)

    def sister_scores(self):
        own = self.nodes
        sisters = self.sisters()
        own_encoding = self.forest.node_sample_encoding(own).astype(dtype=int)
        sister_encoding = self.forest.node_sample_encoding(
            sisters).astype(dtype=int)
        scores = (np.sum(own_encoding, axis=1) + (-1 *
                                                  np.sum(sister_encoding, axis=1))) / own_encoding.shape[1]

        return scores

    def predict_sister_scores(self, node_sample_encoding):
        own_nodes = self.nodes
        own_mask = np.zeros(node_sample_encoding.shape[0], dtype=bool)
        own_mask[[n.index for n in own_nodes]] = True

        sisters = self.sisters()
        sister_mask = np.zeros(node_sample_encoding.shape[0], dtype=bool)
        sister_mask[[s.index for s in sisters]] = True

        own_encoding = node_sample_encoding[own_mask]
        sister_encoding = node_sample_encoding[sister_mask]

        # print(f"own:{own_encoding.shape}")
        # print(f"sister:{sister_encoding.shape}")

        scores = (np.sum(own_encoding, axis=0) + (-1 *
                                                  np.sum(sister_encoding, axis=0))) / own_encoding.shape[0]

        return scores

    def absolute_sister_scores(self):
        own = self.nodes
        sisters = [sister for n in own for sister in [
            n.sister(), ] if sister is not None]
        own_encoding = self.forest.node_sample_encoding(own).astype(dtype=int)
        sister_encoding = self.forest.node_sample_encoding(
            sisters).astype(dtype=int)
        scores = (np.sum(own_encoding, axis=1) +
                  np.sum(sister_encoding, axis=1)) / own_encoding.shape[1]

        return scores

##############################################################################
# Html methods
##############################################################################

# Methods here are used to generate HTML summaries of the cluster

    def html_directory(self):
        location = self.forest.html_directory() + str(self.id) + "/"
        if not os.path.exists(location):
            os.makedirs(location)
        return location

    def json_cluster_summary(self, n=20, features=None):

        # Summarizes the cluster in a json format, primarily for use in html summary documents
        # then returns the object wrapped in a

        from json import dumps as jsn_dumps

        attributes = {}

        error_features, error_ratio = self.error_ratio()
        coefficient_of_determination = 1 - error_ratio

        changed_vs_parent, fold_vs_parent = self.changed_log_fold()
        changed_vs_all, fold_vs_all = self.changed_log_root()
        changed_vs_sister, fold_vs_sister = self.changed_log_sister()

        changed_vs_parent, fold_vs_parent = self.changed_absolute()
        changed_vs_all, fold_vs_all = self.changed_absolute_root()
        changed_vs_sister, fold_vs_sister = self.changed_absolute_sister()

        # probability_enrichment = np.around(self.probability_enrichment(),3)
        probability_enrichment = np.around(self.odds_ratio(), 3)
        probability_enrichment = [(self.forest.split_clusters[i].name(), enrichment) for (
            i, enrichment) in enumerate(probability_enrichment)]

        attributes['clusterName'] = str(self.name())
        attributes['clusterId'] = int(self.id)
        attributes['errorUp'] = generate_feature_value_html(
            error_features[-n:], coefficient_of_determination[-n:], cmap='bwr')
        attributes['errorDown'] = generate_feature_value_html(
            error_features[:n], coefficient_of_determination[:n], cmap='bwr')
        attributes['parentUpregulatedHtml'] = generate_feature_value_html(
            reversed(changed_vs_parent[-n:]), reversed(fold_vs_parent[-n:]), cmap='bwr')
        attributes['parentDownregulatedHtml'] = generate_feature_value_html(
            reversed(changed_vs_parent[:n]), reversed(fold_vs_parent[:n]), cmap='bwr')
        attributes['sisterUpregulatedHtml'] = generate_feature_value_html(
            reversed(changed_vs_sister[-n:]), reversed(fold_vs_sister[-n:]), cmap='bwr')
        attributes['sisterDownregulatedHtml'] = generate_feature_value_html(
            reversed(changed_vs_sister[:n]), reversed(fold_vs_sister[:n]), cmap='bwr')
        attributes['absoluteUpregulatedHtml'] = generate_feature_value_html(
            reversed(changed_vs_all[-n:]), reversed(fold_vs_all[-n:]), cmap='bwr')
        attributes['absoluteDownregulatedHtml'] = generate_feature_value_html(
            reversed(changed_vs_all[:n]), reversed(fold_vs_all[:n]), cmap='bwr')
        attributes['probability_enrichment'] = probability_enrichment
        # attributes['children'] = ", ".join(
        #     [c.name() for c in self.child_clusters()])
        # attributes['parent'] = self.parent_cluster().name()
        # attributes['siblings'] = ", ".join(
        #     [s.name() for s in self.sibling_clusters()])

        if features is not None:

            specified_feature_mask = [f in features for f in changed_features]
            specified_features = changed_features[specified_feature_mask]
            specified_feature_changes = change_fold[specified_feature_mask]

            attributes['specifiedHtml'] = generate_feature_value_html(
                specified_features, specified_feature_changes, cmap='bwr')

        return jsn_dumps(attributes)

    def top_local(self, n, no_plot=False):

        import matplotlib.patheffects as PathEffects
        #
        # changed_vs_sister, fold_vs_sister = self.changed_absolute_sister()
        #
        # important_features = list(
        #     changed_vs_sister[:n]) + list(changed_vs_sister[-n:])
        # important_folds = list(fold_vs_sister[:n]) + list(fold_vs_sister[-n:])
        # important_indices = [
        #     self.forest.truth_dictionary.feature_dictionary[f] for f in important_features]

        error_features, error_ratio = self.error_ratio()

        cod = 1 - error_ratio

        important_features = list(error_features[:n * 2])
        important_ratios = list(cod[:n * 2])
        important_indices = [
            self.forest.truth_dictionary.feature_dictionary[f] for f in important_features]

        local_correlations = self.local_correlations()

        selected_local = local_correlations[important_indices].T[important_indices].T

        m = len(important_indices)

        fig = plt.figure(figsize=(n, n))
        ax = fig.add_axes([0, 0, 1, 1])
        plt.title(f"Local Correlations in {self.name()}")
        im = ax.imshow(selected_local, vmin=-1, vmax=1, cmap='bwr')
        for i in range(m):
            for j in range(m):
                text = ax.text(j, i, np.around(selected_local[i, j], 3),
                               ha="center", va="center", c='w', fontsize=7)
                text.set_path_effects(
                    [PathEffects.withStroke(linewidth=.5, foreground='black')])

        plt.xticks(np.arange(m), labels=important_features, rotation=45)
        plt.yticks(np.arange(m), labels=important_features, rotation=45)
        plt.colorbar(im)
        plt.tight_layout()
        if no_plot:
            return fig
        else:
            plt.show()
            return fig

    def top_global(self, n, no_plot=False):

        import matplotlib.patheffects as PathEffects
        #
        # changed_vs_sister, fold_vs_sister = self.changed_absolute_sister()
        #
        # important_features = list(
        #     changed_vs_sister[:n]) + list(changed_vs_sister[-n:])
        # important_folds = list(fold_vs_sister[:n]) + list(fold_vs_sister[-n:])
        # important_indices = [
        #     self.forest.truth_dictionary.feature_dictionary[f] for f in important_features]

        error_features, error_ratio = self.error_ratio()

        cod = 1 - error_ratio

        important_features = list(error_features[:n * 2])
        important_ratios = list(cod[:n * 2])
        important_indices = [
            self.forest.truth_dictionary.feature_dictionary[f] for f in important_features]

        global_correlations = self.forest.global_correlations()

        selected_global = global_correlations[important_indices].T[important_indices].T

        m = len(important_indices)

        fig = plt.figure(figsize=(n, n))
        ax = fig.add_axes([0, 0, 1, 1])
        plt.title("Global Correlations")
        im = ax.imshow(selected_global, vmin=-1, vmax=1, cmap='bwr')
        for i in range(m):
            for j in range(m):
                text = ax.text(j, i, np.around(selected_global[i, j], 3),
                               ha="center", va="center", c='w', fontsize=7)
                text.set_path_effects(
                    [PathEffects.withStroke(linewidth=.5, foreground='black')])

        plt.xticks(np.arange(m), labels=important_features, rotation=45)
        plt.yticks(np.arange(m), labels=important_features, rotation=45)
        plt.colorbar(im)
        plt.tight_layout()
        if no_plot:
            return fig
        else:
            plt.show()
            return fig

    def html_cross_reference(self, n=10, plot=False, output=None):

        if output is None:
            location = self.html_directory()
        else:
            location = output

        local_cross = self.top_local(n, no_plot=True)
        global_cross = self.top_global(n, no_plot=True)

        print(f"Saving cross ref to {location}")

        local_cross.savefig(location + "local_cross.png", bbox_inches='tight')
        global_cross.savefig(
            location + "global_cross.png", bbox_inches='tight')

        local_html = f'<img class="local_cross" src="{location + "local_cross.png"}" />'
        global_html = f'<img class="global_cross" src="{location + "global_cross.png"}" />'

        return (local_html, global_html)

    def html_cluster_summary(self, n=20, plot=True, output=None):

        location = self.forest.location()
        if output is None:
            html_location = self.html_directory()
        else:
            html_location = output

        # First we read in the template (TO DO improve safety)
        html_string = open(
            location + "/cluster_summary_template_js.html", 'r').read()

        # Reading the file in allows us to both write a summary file somewhere appropriate and return an html string in case we want to do something else

        # This function puts the sister score image in the appropriate location (we discard its return string, not relevant here)

        self.html_sister_scores(output=output)
        self.html_sample_scores(output=output)
        self.html_cross_reference(n=n, output=output)

        with open(html_location + "cluster_summary_template_js.html", 'w') as html_file:
            json_string = js_wrap("attributes", self.json_cluster_summary(n=n))
            html_string = html_string + json_string
            html_file.write(html_string)

        if plot:
            # We ask the OS to open the html file.
            from subprocess import run
            run(["open", html_location + "cluster_summary_template_js.html"])

        # Finally we return the HTML string
        # CAUTION, this contains the whole template file, so it has a bunch of javascript in it.
        return html_string

    def html_feature_means(self, features):
        feature_values = self.feature_means(features)
        html = generate_feature_value_html(features, feature_values)
        return html

    def html_feature_additives(self, features):
        feature_values = self.feature_additives(features)
        html = generate_feature_value_html(features, feature_values)
        return html

    def html_sister_scores(self, output=None):

        from matplotlib.colors import DivergingNorm

        if output is None:
            location = self.html_directory()
        else:
            location = output

        forest_coordinates = self.forest.coordinates()
        sister_scores = self.sister_scores()
        plt.figure()
        plt.title(
            f"Distribution of Samples \nIn {self.name()} (Red) vs Its Sisters (Blue)")
        plt.scatter(forest_coordinates[:, 0], forest_coordinates[:, 1], s=1,
                    alpha=.6, c=sister_scores, norm=DivergingNorm(0), cmap='bwr')
        plt.colorbar()
        plt.ylabel("tSNE Coordinates (AU)")
        plt.xlabel("tSNE Coordinates (AU)")
        plt.savefig(location + "sister_map.png")

        html = f'<img class="sister_score" src="{location + "sister_map.png"}" />'

        return html

    def html_sample_scores(self, output=None):

        if output is None:
            location = self.html_directory()
        else:
            location = output

        forest_coordinates = self.forest.coordinates()
        sample_scores = self.sample_scores()
        plt.figure()
        plt.title(f"Frequency of Samples In {self.name()}")
        plt.scatter(
            forest_coordinates[:, 0], forest_coordinates[:, 1], s=1, alpha=.6, c=sample_scores)
        plt.colorbar()
        plt.ylabel("tSNE Coordinates (AU)")
        plt.xlabel("tSNE Coordinates (AU)")
        plt.savefig(location + "score_map.png")

        html = f'<img class="score_map" src="{location + "score_map.png"}" />'

        return html

    def sample_cluster_frequency(self, plot=True):
        sample_cluster_labels = self.forest.sample_labels
        sample_counts = self.sample_counts()
        sample_clusters = sorted(list(set(sample_cluster_labels)))
        cluster_counts = []
        for cluster in sample_clusters:
            cluster_mask = sample_cluster_labels == cluster
            cluster_counts.append(np.sum(sample_counts[cluster_mask]))

        if plot:
            plt.figure()
            plt.title("Frequency of sample clusters in leaf cluster")
            plt.bar(np.arange(len(sample_clusters)),
                    cluster_counts, tick_labels=sample_clusters)
            plt.ylabel("Frequency")
            plt.show()

        return sample_clusters, cluster_counts

    def plot_sample_counts(self, **kwargs):
        counts = self.sample_counts()
        plt.figure(figsize=(15, 10))
        plt.scatter(self.forest.coordinates(no_plot=True)[
                    :, 0], self.forest.coordinates(no_plot=True)[:, 1], c=counts, **kwargs)
        plt.colorbar()
        plt.show()

    def probability_enrichment(self):
        enrichment = self.forest.probability_enrichment()
        return enrichment.T[self.id]

    def odds_ratio(self):
        odds_ratios = self.forest.split_cluster_odds_ratios()
        return odds_ratios.T[self.id]


##############################################################################
# EXPERIMENTAL
##############################################################################

    #
    # def cluster_children(self, nodes=None, mode='gain', metric='cosine', pca=False, distance='cosine', **kwargs):
    #
    #     print("Interpreting cluster children")
    #     print(f"Cluster:{self.id}")
    #
    #     children = []
    #     for node in self.nodes:
    #         children.extend(node.children)
    #
    #     print(f"Child nodes:{len(children)}")
    #
    #     child_representation = self.forest.node_representation(
    #         children, mode=mode, metric=metric, pca=pca)
    #
    #     child_labels = len(self.forest.split_clusters) + Forest.sdg_cluster_representation(
    #         child_representation, distance=distance, **kwargs)
    #
    #     self.forest.plot_representation(
    #         child_representation, labels=child_labels, mode=mode, metric=metric, pca=pca)
    #
    #     for child, label in zip(children, child_labels):
    #         child.set_split_cluster(label)
    #
    #     cluster_set = set(child_labels)
    #
    #     print(f"New clusters: {cluster_set}")
    #
    #     clusters = []
    #
    #     for cluster in cluster_set:
    #         split_indices = np.arange(len(children))[child_labels == cluster]
    #         clusters.append(NodeCluster(
    #             self.forest, [children[i] for i in split_indices], cluster))
    #
    #     self.forest.split_clusters.extend(clusters)
    #
    #     return clusters

################################
################################
################################
################################
#
#       Rest of module
#       Split out later
#
################################
################################
################################
################################

# import community
# import networkx as nx


def numpy_mad(mtx):
    medians = []
    for column in mtx.T:
        medians.append(np.median(column[column != 0]))
    median_distances = np.abs(
        mtx - np.tile(np.array(medians), (mtx.shape[0], 1)))
    mads = []
    for (i, column) in enumerate(median_distances.T):
        mads.append(np.median(column[mtx[:, i] != 0]))
    return np.array(mads)


def ssme(mtx, axis=None):
    medians = np.median(mtx, axis=0)
    median_distances = mtx - np.tile(np.array(medians), (mtx.shape[0], 1))
    ssme = np.sum(np.power(median_distances, 2), axis=axis)
    return ssme


def nonzero_var_column(mtx):
    nzv = np.zeros(mtx.shape[1])
    for i in range(mtx.shape[1]):
        nzv[i] = np.var(mtx[:, i][mtx[:, i] != 0])
    return nzv


def sample_node_encoding(nodes, samples):
    encoding = np.zeros((len(nodes), samples), dtype=bool)
    for i, node in enumerate(nodes):
        encoding[i] = node.sample_mask()
    sample_encoding = encoding.T
    unrepresented = np.sum(sample_encoding, axis=1) == 0
    if np.sum(unrepresented) > 0:
        sample_encoding[unrepresented] = 1
    return sample_encoding


def feature_node_index(nodes, feature):
    encoding = np.zeros(len(nodes), dtype=bool)
    for i, node in enumerate(nodes):
        if feature in node.features:
            encoding[i] = True
    return encoding


def coocurrence_matrix(sample_encoding):

    #     co_mtx = np.zeros((sample_encoding.shape[1],sample_encoding.shape[1]))
    #     for node in sample_encoding:
    #         node_tile = np.tile(node,(node.shape[0],1))
    #         intersect = np.logical_and(node_tile,node_tile.T)
    #         co_mtx += intersect.astype(dtype=int)
    #     return co_mtx

    co_mtx = np.matmul(sample_encoding.astype(dtype=int),
                       sample_encoding.T.astype(dtype=int))

    print(co_mtx.shape)

    return np.array(co_mtx)


def coocurrence_distance(sample_encoding):

    co_mtx = coocurrence_matrix(sample_encoding)
    distance_mtx = 1.0 / (co_mtx + 1)
    return distance_mtx


def node_tsne_sample(nodes, samples):

    encoding = node_sample_encoding(nodes, samples)
    embedding_model = TSNE(n_components=2, metric='correlation')
    coordinates = embedding_model.fit_transform(encoding)

    #     distances = coocurrence_matrix(encoding)

    return coordinates


def sample_tsne(nodes, samples):

    encoding = node_sample_encoding(nodes, samples)
    print("TSNE Encoding: {}".format(encoding.shape))
    pre_computed_distance = coocurrence_distance(encoding.T)
#     pre_computed_distance = scipy.spatial.distance.squareform(pdist(encoding.T,metric='jaccard'))
    print("TSNE Distance Matrix: {}".format(pre_computed_distance.shape))
#     pre_computed_distance[pre_computed_distance == 0] += .000001
    pre_computed_distance[np.isnan(pre_computed_distance)] = 10000000
    embedding_model = TSNE(n_components=2, metric='precomputed')
    coordinates = embedding_model.fit_transform(pre_computed_distance)

    #     distances = coocurrence_matrix(encoding)

    return coordinates
#
# def node_hdbscan_samples(nodes,samples):
#
#     node_encoding = node_sample_encoding(nodes,samples)
#
#     pre_computed_distance = pdist(node_encoding,metric='cityblock')
#
#     clustering_model = HDBSCAN(min_cluster_size=50, metric='precomputed')
#
# #     plt.figure()
# #     plt.title("Dbscan observed distances")
# #     plt.hist(pre_computed_distance,bins=50)
# #     plt.show()
#
#     clusters = clustering_model.fit_predict(scipy.spatial.distance.squareform(pre_computed_distance))
#
# #     clusters = clustering_model.fit_predict(node_encoding)

    return clusters


def node_gain_table(nodes, forest):
    node_gain_table = np.zeros((len(nodes), len(forest.features)))
    for i, node in enumerate(nodes):
        for j, feature in enumerate(node.features):
            feature_index = forest.truth_dictionary.feature_dictionary[feature]
            try:
                feature_gain = node.absolute_gains[j]
                node_gain_table[i, feature_index] = feature_gain
            except:
                print(node.absolute_gains)
    return node_gain_table
#


def hacked_louvain(knn, resolution=1):
    import louvain
    import igraph as ig
    from sklearn.neighbors import NearestNeighbors

    g = ig.Graph()
    g.add_vertices(knn.shape[0])  # this adds adjacency.shape[0] vertices
    edges = [(s, t) for s in range(knn.shape[0]) for t in knn[s]]

    g.add_edges(edges)

    if g.vcount() != knn.shape[0]:
        logg.warning(
            f'The constructed graph has only {g.vcount()} nodes. '
            'Your adjacency matrix contained redundant nodes.'
        )

    print("Searching for partition")
    part = louvain.find_partition(
        g, partition_type=louvain.RBConfigurationVertexPartition, resolution_parameter=resolution)
    clustering = np.zeros(knn.shape[0], dtype=int)
    for i in range(len(part)):
        clustering[part[i]] = i
    print("Louvain: {}".format(clustering.shape))
    return clustering


# def embedded_hdbscan(coordinates):
#
#     clustering_model = HDBSCAN(min_cluster_size=50)
#     clusters = clustering_model.fit_predict(coordinates)
#     return clusters
#
# def sample_hdbscan(nodes,samples):
#
#     node_encoding = node_sample_encoding(nodes,samples)
#     embedding_model = PCA(n_components=100)
#     pre_computed_embedded = embedding_model.fit_transform(node_encoding.T)
#     print("Sample HDBscan Encoding: {}".format(pre_computed_embedded.shape))
# #     pre_computed_distance = coocurrence_distance(node_encoding)
#     pre_computed_distance = scipy.spatial.distance.squareform(pdist(pre_computed_embedded,metric='correlation'))
#     print("Sample HDBscan Distance Matrix: {}".format(pre_computed_distance.shape))
# #     pre_computed_distance[pre_computed_distance == 0] += .000001
#     pre_computed_distance[np.isnan(pre_computed_distance)] = 10000000
#     clustering_model = HDBSCAN(min_samples=3,metric='precomputed')
#     clusters = clustering_model.fit_predict(pre_computed_distance)
#
#     return clusters


def cluster_labels_to_connectivity(labels):
    samples = labels.shape[0]
    clusters = list(set(labels))
    cluster_masks = []
    connectivity = np.zeros((samples, samples), dtype=bool)
    for cluster in clusters:
        cluster_masks.append(labels == cluster)
    for cluster_mask in cluster_masks:
        vertical_mask = np.zeros((samples, samples), dtype=bool)
        horizontal_mask = np.zeros((samples, samples), dtype=bool)
        vertical_mask[:, cluster_mask] = True
        horizontal_mask[cluster_mask] = True
        square_mask = np.logical_and(vertical_mask, horizontal_mask)
        connectivity[square_mask] = True
    return connectivity


def sample_agglomerative(nodes, samples, n_clusters):

    node_encoding = node_sample_encoding(nodes, samples)

    pre_computed_distance = pdist(node_encoding.T, metric='cosine')

    clustering_model = AgglomerativeClustering(
        n_clusters=n_clusters, affinity='precomputed')

    clusters = clustering_model.fit_predict(
        scipy.spatial.distance.squareform(pre_computed_distance))

#     clusters = clustering_model.fit_predict(node_encoding)

    return clusters


def stack_dictionaries(dictionaries):
    stacked = {}
    for dictionary in dictionaries:
        for key, value in dictionary.items():
            if key not in stacked:
                stacked[key] = []
            stacked[key].append(value)
    return stacked


def partition_mutual_information(p1, p2):
    p1 = p1.astype(dtype=float)
    p2 = p2.astype(dtype=float)
    population = p1.shape[1]
    intersections = np.dot(p1, p2.T)
    partition_size_products = np.outer(np.sum(p1, axis=1), np.sum(p2, axis=1))
    log_term = np.log(intersections) - \
        np.log(partition_size_products) + np.log(population)
    log_term[np.logical_not(np.isfinite(log_term))] = 0
    mutual_information_matrix = (intersections / population) * log_term
    return mutual_information_matrix


def consolidate_entries(keys, dictionaries):
    consolidated = empty_list_dictionary(keys)
    for dictionary in dictionaries:
        for key, value in iter(dictionary):
            if key not in consolidated:
                consolidated[key] = []
            consolidated[entry].append(value)
    return consolidated


def empty_list_dictionary(keys):
    return {key: [] for key in keys}


def count_list_elements(elements):
    dict = {}
    for element in elements:
        if element not in dict:
            dict[element] = 0
        dict[element] += 1
    return dict


def triangulate_knn(elements, k):

    distances = {}

    anchor = 0


def generate_feature_value_html(features, values, normalization=None, cmap=None):

    if not isinstance(cmap, mpl.colors.Colormap):
        from matplotlib.cm import get_cmap
        try:
            cmap = get_cmap(cmap)
        except:
            cmap = get_cmap('viridis')
    # if normalization is None:
    #     from matplotlib.colors import SymLogNorm, DivergingNorm
    #     normalization = DivergingNorm(0)
        # normalization = SymLogNorm(linthresh=.05)

    html_elements = [
        # '<table width="100%">',
        '<table>',
        "<style>", "th,td {padding:5px;border-bottom:1px solid #ddd;}", "</style>",
        "<tr>",
        "<th>", "Features", "</th>",
        "<th>", "Values", "</th>",
        "</tr>",
    ]
    for feature, value in zip(features, values):
        value_color_tag = ""
        # if normalization is not None:
        #     normed_value = normalization(value)
        #     r,g,b,a = cmap(normed_value)
        #     r,g,b,a = r*100,g*100,b*100,a*100
        # value_color_tag = f'style="background-color:rgba({r}%,{g}%,{b}%,50%);"'
        # value_color_tag = f'style="background-image:linear-gradient(to right,rgba({r}%,{g}%,{b}%,0%),rgba({r}%,{g}%,{b}%,50%));"'
        feature_elements = f"""
            <tr>
                <td>{feature}</td>
                <td {value_color_tag}>{value}</td>
            </td>
        """
        html_elements.append(feature_elements)

    html_elements.append("</table>")
    return "".join(html_elements)

# def generate_local_correlation_table(f1,f2,local,global):
#
#     html_elements = [
#         # '<table width="100%">',
#         '<table>',
#         "<style>", "th,td {padding:5px;border-bottom:1px solid #ddd;}", "</style>",
#         "<tr>",
#         "<th>", "Features", "</th>",
#         "<th>", "Values", "</th>",
#         "</tr>",
#     ]


def js_wrap(name, content):
    return f"<script> let {name} = {content};</script>"


def fast_knn(elements, k, neighborhood_fraction=.01, metric='euclidean'):

    # Finds the indices of k nearest neighbors for each sample in a matrix,
    # using any of the standard scipy distance metrics.

    nearest_neighbors = np.zeros((elements.shape[0], k), dtype=int)
    complete = np.zeros(elements.shape[0], dtype=bool)

    neighborhood_size = max(
        k * 3, int(elements.shape[0] * neighborhood_fraction))
    anchor_loops = 0

    while np.sum(complete) < complete.shape[0]:

        anchor_loops += 1

        available = np.arange(complete.shape[0])[~complete]
        np.random.shuffle(available)
        anchors = available[:int(complete.shape[0] / neighborhood_size) * 3]

        for anchor in anchors:
            print(f"Complete:{np.sum(complete)}\r", end='')

            anchor_distances = cdist(elements[anchor].reshape(
                1, -1), elements, metric=metric)[0]

            neighborhood = np.argpartition(anchor_distances, neighborhood_size)[
                :neighborhood_size]
            anchor_local = np.where(neighborhood == anchor)[0]

            local_distances = squareform(
                pdist(elements[neighborhood], metric=metric))

            anchor_to_worst = np.max(local_distances[anchor_local])

            for i, sample in enumerate(neighborhood):
                if not complete[sample]:

                    # First select the indices in the neighborhood that are knn
                    best_neighbors_local = np.argpartition(
                        local_distances[i], k + 1)

                    # Next find the worst neighbor among the knn observed
                    best_worst_local = best_neighbors_local[np.argmax(
                        local_distances[i][best_neighbors_local[:k + 1]])]
                    # And store the worst distance among the local knn
                    best_worst_distance = local_distances[i, best_worst_local]
                    # Find the distance of the anchor to the central element
                    anchor_distance = local_distances[anchor_local, i]

                    # By the triangle inequality the closest any element outside the neighborhood
                    # can be to element we are examining is the criterion distance:
                    criterion_distance = anchor_to_worst - anchor_distance

#                     if sample == 0:
#                         print(f"ld:{local_distances[i][best_neighbors_local[:k]]}")
#                         print(f"bwd:{best_worst_distance}")
#                         print(f"cd:{criterion_distance}")

                    # Therefore if the criterion distance is greater than the best worst distance, the local knn
                    # is also the best global knn

                    if best_worst_distance >= criterion_distance:
                        continue
                    else:
                        # Before we conclude we must exclude the sample itself from its
                        # k nearest neighbors
                        best_neighbors_local = [
                            bn for bn in best_neighbors_local[:k + 1] if bn != i]
                        # Finally translate the local best knn to the global indices
                        best_neighbors = neighborhood[best_neighbors_local]

                        nearest_neighbors[sample] = best_neighbors
                        complete[sample] = True
    print("\n")

    return nearest_neighbors


def double_fast_knn(elements1, elements2, k, neighborhood_fraction=.01, metric='cosine'):

    if elements1.shape != elements2.shape:
        raise Exception("Average metric knn inputs must be same size")

    nearest_neighbors = np.zeros((elements1.shape[0], k), dtype=int)
    complete = np.zeros(elements1.shape[0], dtype=bool)

    neighborhood_size = max(
        k * 3, int(elements1.shape[0] * neighborhood_fraction))
    anchor_loops = 0
    # failed_counter = 0

    while np.sum(complete) < complete.shape[0]:

        anchor_loops += 1

        available = np.arange(complete.shape[0])[~complete]
        np.random.shuffle(available)
        anchors = available[:int(complete.shape[0] / neighborhood_size) * 3]

        for anchor in anchors:
            print(f"Complete:{np.sum(complete)}\r", end='')

            ad_1 = cdist(elements1[anchor].reshape(
                1, -1), elements1, metric=metric)[0]
            ad_2 = cdist(elements2[anchor].reshape(
                1, -1), elements2, metric=metric)[0]
            anchor_distances = (ad_1 + ad_2) / 2

    #         print(f"anchor:{anchor}")

            neighborhood = np.argpartition(anchor_distances, neighborhood_size)[
                :neighborhood_size]
            anchor_local = np.where(neighborhood == anchor)[0]

    #         print(neighborhood)

            ld_1 = squareform(pdist(elements1[neighborhood], metric=metric))
            ld_2 = squareform(pdist(elements2[neighborhood], metric=metric))
            local_distances = (ld_1 + ld_2) / 2

            anchor_to_worst = np.max(local_distances[anchor_local])

            for i, sample in enumerate(neighborhood):
                if not complete[sample]:

                    # First select the indices in the neighborhood that are knn
                    best_neighbors_local = np.argpartition(
                        local_distances[i], k + 1)

                    # Next find the worst neighbor among the knn observed
                    best_worst_local = best_neighbors_local[np.argmax(
                        local_distances[i][best_neighbors_local[:k + 1]])]
                    # And store the worst distance among the local knn
                    best_worst_distance = local_distances[i, best_worst_local]
                    # Find the distance of the anchor to the central element
                    anchor_distance = local_distances[anchor_local, i]

                    # By the triangle inequality the closest any element outside the neighborhood
                    # can be to element we are examining is the criterion distance:
                    criterion_distance = anchor_to_worst - anchor_distance

#                     if sample == 0:
#                         print(f"ld:{local_distances[i][best_neighbors_local[:k]]}")
#                         print(f"bwd:{best_worst_distance}")
#                         print(f"cd:{criterion_distance}")

                    # Therefore if the criterion distance is greater than the best worst distance, the local knn
                    # is also the best global knn

                    if best_worst_distance >= criterion_distance:
                        continue
                    else:
                        # Before we conclude we must exclude the sample itself from its
                        # k nearest neighbors
                        best_neighbors_local = [
                            bn for bn in best_neighbors_local[:k + 1] if bn != i]
                        # Finally translate the local best knn to the global indices
                        best_neighbors = neighborhood[best_neighbors_local]

                        nearest_neighbors[sample] = best_neighbors
                        complete[sample] = True
    print("\n")

    return nearest_neighbors


if __name__ != "__main__":
    import matplotlib as mpl
    mpl.rcParams['figure.dpi'] = 300
