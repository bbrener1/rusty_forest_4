{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook will go through a conventional scanpy analysis of citeseq data, so that we can compare it to an \n",
    "# analysis performed by a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the data, fortunately the facilities for this are pretty nice\n",
    "\n",
    "datasets = [\n",
    "    '/localscratch/bbrener1/johnston_sc/Retina2/outs/filtered_gene_bc_matrices/dmel_r6.20/matrix.mtx',\n",
    "    '/home/bbrener1/transfer/all_raw/raw_data/nelmari/ctrl/matrix.mtx',\n",
    "    '/home/bbrener1/transfer/all_raw/raw_data/fan_tendon/TH1/matrix.mtx',\n",
    "    '/home/bbrener1/transfer/all_raw/raw_data/citeseq/GSE100866_CBMC_human_umis.tsv', # This dataset had spiked in mouse cells\n",
    "                                                                                      # but this is a filtered matrix\n",
    "        \n",
    "    '/home/bbrener1/transfer/all_raw/raw_data/vision_sc/raw_counts.txt',   # These are the unfiltered UMIs of this dataset\n",
    "                                                                           # Actual paper analysis was done on like 1600 cells, so needs to be checked out\n",
    "]\n",
    "# Whether this dataset needs to be transposed\n",
    "transpose = [\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "    False,\n",
    "]\n",
    "\n",
    "scanpy_objects = []\n",
    "umi_objects = []\n",
    "\n",
    "for dataset,d_transpose in zip(datasets,transpose): \n",
    "    \n",
    "    print(dataset)\n",
    "    \n",
    "    large_scanpy_object = sc.read(dataset)\n",
    "    \n",
    "    if d_transpose:\n",
    "        large_scanpy_object = large_scanpy_object.T\n",
    "\n",
    "    sc.pp.downsample_counts(large_scanpy_object,counts_per_cell=1200)\n",
    "    large_scanpy_object.X = large_scanpy_object.X.astype(dtype=float)\n",
    "        \n",
    "    sc.pp.filter_cells(large_scanpy_object,min_genes=100)\n",
    "    sc.pp.filter_cells(large_scanpy_object,min_counts=100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Read in\")\n",
    "#     This filtration is roughly analogous to the standard zheng, but it retains the UMI object. \n",
    "\n",
    "    sc.pp.filter_genes(large_scanpy_object, min_counts=10)         # only consider genes with more than 10 counts\n",
    "\n",
    "    \n",
    "    scpy_copy = large_scanpy_object.copy()\n",
    "    sc.pp.normalize_per_cell(scpy_copy)\n",
    "    filter_result = sc.pp.filter_genes_dispersion(  # select highly-variable genes\n",
    "        scpy_copy.X, flavor='cell_ranger', n_top_genes=2000, log=False\n",
    "    )\n",
    "    scpy_filtered = large_scanpy_object[:, filter_result.gene_subset].copy()     # subset the genes\n",
    "    \n",
    "    del(large_scanpy_object)\n",
    "        \n",
    "    umis = scpy_filtered.copy().X\n",
    "\n",
    "    sc.pp.normalize_per_cell(scpy_filtered)                 # renormalize after filtering\n",
    "    sc.pp.log1p(scpy_filtered)                      # log transform: adata.X = log(adata.X + 1)\n",
    "    sc.pp.scale(scpy_filtered)\n",
    "\n",
    "#     print(f\"zero mean:{np.sum(np.mean(umis,axis=0) == 0)}/{np.mean(umis,axis=0).shape}\")\n",
    "#     print(f\"zero var:{np.sum(np.var(umis,axis=0) == 0)}\")\n",
    "    \n",
    "    scanpy_objects.append(scpy_filtered)\n",
    "    umi_objects.append(umis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,umi in enumerate(umi_objects):\n",
    "    print(type(umi))\n",
    "    if type(umi) is not type(np.zeros(0)):\n",
    "        umi_objects[i] = np.array(umi.todense())\n",
    "    print(umi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,umi in enumerate(umi_objects):\n",
    "    print(type(umi))\n",
    "    print(umi.shape)\n",
    "    print(f\"zero mean:{np.sum(np.mean(umi,axis=0) == 0)}/{np.mean(umis,axis=0).shape}\")\n",
    "    print(f\"zero var:{np.sum(np.var(umi,axis=0) == 0)}\")\n",
    "    print(np.log(np.mean(umi,axis=0)))\n",
    "    print(np.log(np.var(umi,axis=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/localscratch/bbrener1/rusty_forest_v3/src')\n",
    "# sys.path.append('../src')\n",
    "import tree_reader as tr \n",
    "import lumberjack\n",
    "\n",
    "forest_objects = []\n",
    "\n",
    "for umi in umi_objects: \n",
    "    forest = lumberjack.fit(\n",
    "        umi,\n",
    "        trees=100,\n",
    "        braids=3,\n",
    "        ifs=500,\n",
    "        ofs=500,\n",
    "        ss=200,\n",
    "        depth=8,\n",
    "        leaves=50,\n",
    "        sfr=0\n",
    "    )\n",
    "\n",
    "    forest.set_cache(True)\n",
    "    \n",
    "    forest_objects.append(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session(\"scanpy_poisson_session.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset,forest in zip(datasets,forest_objects):\n",
    "\n",
    "#     forest.reset_sample_clusters()\n",
    "    if len(forest.samples) > 3000:\n",
    "        k = 20\n",
    "    else:\n",
    "        k = 10\n",
    "    forest.cluster_samples_encoding(\n",
    "        sub=.8,\n",
    "        k=k,\n",
    "        depth=8,\n",
    "        metric='cosine',\n",
    "        pca=100\n",
    "    )\n",
    "    forest.tsne(pca=100)\n",
    "    forest.plot_sample_clusters()\n",
    "    \n",
    "    umi_means = np.array(np.mean(forest.output,axis=0)).ravel()\n",
    "    umi_vars = np.array(np.var(forest.output,axis=0)).ravel()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"UMI Mean vs Variance per feature, Log/Log, Global\")\n",
    "    plt.scatter(np.log(umi_means),np.log(umi_vars),s=1)\n",
    "    plt.plot([-8,4],[-8,4],c='red')\n",
    "    plt.show()\n",
    "\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     plt.title(\"UMI Mean vs Variance per feature, Linear, Global\")\n",
    "#     plt.scatter(umi_means,umi_vars,s=1)\n",
    "#     plt.plot([0,10],[0,10],c='red')\n",
    "#     plt.xlim(0,10)\n",
    "#     plt.ylim(0,10)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     plt.title(\"UMI Mean/Variance Ratio per feature, Linear\")\n",
    "#     plt.scatter(np.arange(umi_means.shape[0]),(umi_means/umi_vars)[np.argsort(umi_means)],s=1)\n",
    "#     plt.show()\n",
    "    \n",
    "    forest_clusters = []\n",
    "    \n",
    "    for cluster in set(forest.sample_labels):\n",
    "        mask = forest.sample_labels == cluster\n",
    "        filtered_cells = forest.output[mask]\n",
    "        cluster_means = np.mean(filtered_cells,axis=0)\n",
    "        cluster_var = np.var(filtered_cells,axis=0)\n",
    "        forest_clusters.append((cluster_means,cluster_var))\n",
    "    \n",
    "    forest_cluster_means = [m for c in forest_clusters for m in c[0]]\n",
    "    forest_cluster_var = [v for c in forest_clusters for v in c[1]]\n",
    "        \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"Clustered vs Global\")\n",
    "    plt.scatter(np.log(forest_cluster_means),np.log(forest_cluster_var),c='blue',s=1,alpha=.5,label=\"Cluster\")\n",
    "    plt.scatter(np.log(umi_means),np.log(umi_vars),c='red',s=1,alpha=.5,label=\"Global\")\n",
    "    plt.plot([0,10],[0,10],c='red')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Mean\")\n",
    "    plt.ylabel(\"Variance\")\n",
    "    plt.show()        \n",
    "    \n",
    "# #     plt.figure(figsize=(10,10))\n",
    "# #     plt.title(\"Clustered vs Global\")\n",
    "# #     plt.scatter(forest_cluster_means,forest_cluster_var,c='blue',s=1,alpha=.5,label=\"Cluster\")\n",
    "# #     plt.scatter(umi_means,umi_vars,c='red',s=1,alpha=.5,label=\"Global\")\n",
    "# #     plt.plot([0,10],[0,10],c='red')\n",
    "# #     plt.legend()\n",
    "# #     plt.xlabel(\"Mean\")\n",
    "# #     plt.ylabel(\"Variance\")\n",
    "# #     plt.show()        \n",
    "\n",
    "    mean_sort = np.argsort(umi_means)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f\"UMI Mean/Variance Ratio per feature, Dataset:{dataset}\")\n",
    "    for cluster in forest_clusters:    \n",
    "        cluster_means,cluster_var = cluster\n",
    "        plt.scatter(np.arange(len(cluster_means)),(np.array(cluster_means)/np.array(cluster_var))[mean_sort],s=1,c='blue')\n",
    "    plt.ylim(0,2)\n",
    "    plt.scatter(np.arange(len(umi_means)),(umi_means/umi_vars)[mean_sort],s=3,c='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forest in forest_objects:\n",
    "    print(len(forest.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We now establish the neighbor graph because several methods rely on it\n",
    "import warnings\n",
    "\n",
    "for dataset,scanpy_object,umis in zip(datasets,scanpy_objects,umi_objects):\n",
    "\n",
    "#     print(scanpy_object.shape)\n",
    "#     print(umis.shape)\n",
    "    \n",
    "    sc.pp.neighbors(scanpy_object)\n",
    "    sc.tl.umap(scanpy_object)\n",
    "    sc.tl.tsne(scanpy_object)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        sc.tl.louvain(scanpy_object,resolution=1)\n",
    "        \n",
    "    sc.pl.umap(scanpy_object,color='louvain')\n",
    "    sc.pl.tsne(scanpy_object,color='louvain')\n",
    "    \n",
    "    umi_means = np.mean(umis,axis=0)\n",
    "    umi_vars = np.var(umis,axis=0)\n",
    "    \n",
    "    louvain_clusters = []\n",
    "    \n",
    "    for cluster in set(scanpy_object.obs['louvain']):\n",
    "        mask = scanpy_object.obs['louvain'] == cluster\n",
    "        filtered_cells = np.array(umis[mask])\n",
    "        cluster_means = np.mean(filtered_cells,axis=0)\n",
    "        cluster_vars = np.var(filtered_cells,axis=0)\n",
    "        louvain_clusters.append((cluster_means,cluster_vars))\n",
    "        \n",
    "#         print(len(cluster_means))\n",
    "#         print(len(cluster_vars))\n",
    "\n",
    "    louvain_cluster_means = [m for c in louvain_clusters for m in c[0]]\n",
    "    louvain_cluster_vars = [v for c in louvain_clusters for v in c[1]]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"Clustered vs Global\")\n",
    "    plt.scatter(np.log(louvain_cluster_means),np.log(louvain_cluster_vars),c='blue',s=1,alpha=.5,label=\"Cluster\")\n",
    "    plt.scatter(np.log(umi_means),np.log(umi_vars),c='red',s=1,alpha=.5,label=\"Global\")\n",
    "    plt.plot([0,10],[0,10],c='red')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Mean\")\n",
    "    plt.ylabel(\"Variance\")\n",
    "    plt.show()        \n",
    "    \n",
    "    \n",
    "    mean_sort = np.argsort(umi_means)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f\"UMI Mean/Variance Ratio per feature, Dataset:{dataset}\")\n",
    "    for cluster in louvain_clusters:    \n",
    "        cluster_means,cluster_vars = cluster        \n",
    "        cluster_ratios = np.array(cluster_means)/np.array(cluster_vars)\n",
    "        print(len(cluster_ratios))\n",
    "        print(len(umi_means))\n",
    "        print(len(mean_sort))\n",
    "        plt.scatter(np.arange(len(cluster_means)),cluster_ratios[mean_sort],s=1,c='blue')\n",
    "    plt.ylim(0,2)\n",
    "    plt.scatter(np.arange(len(umi_means)),(umi_means/umi_vars)[mean_sort],s=3,c='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# plt.scatter(*citeseq_scaled.obsm['X_tsne'].T,c=forest.sample_labels,s=4,cmap='rainbow')\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(*johnston_working.obsm['X_umap'].T,c=forest.sample_labels,s=4,cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.tsne_coordinates = johnston_working.obsm['X_umap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forest.plot_sample_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.reset_split_clusters()\n",
    "forest.interpret_splits(mode='additive_mean',depth=5,sub=.5,k=20,metric='cosine',relatives=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_cluster in forest.split_clusters:\n",
    "    split_cluster.html_sister_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest.maximum_spanning_tree(depth=5)\n",
    "# forest.most_likely_tree(depth=5)\n",
    "forest.html_tree_summary(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forest.nodes(root=True,depth=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we wish to examine the behavior of distributions within clusters as defined by louvain and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umi_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we wish to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.argsort(np.mean(umis.X,axis=0))[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest,poisson,nbinom\n",
    "\n",
    "def poisson_ks_check(x):\n",
    "    mean = np.mean(x)\n",
    "    cdf = lambda x: poisson.cdf(x,mean)\n",
    "    return kstest(x,cdf)\n",
    "\n",
    "# Correctly fitting a negative binomial is a pain, here is a hacked out version for now:\n",
    "def nb_pr_estimation(x):\n",
    "    mean = np.mean(x)\n",
    "    var = np.var(x)\n",
    "    counter = mean/var\n",
    "        \n",
    "    p = -1 * (counter - 1)\n",
    "    r = mean * ((1-p)/p)\n",
    "        \n",
    "    return p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_draws = nbinom.rvs(4,.5,size=1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(nb_draws)\n",
    "plt.show()\n",
    "\n",
    "nb_pr_estimation(nb_draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ks_values = [poisson_ks_check(x)[0] for x in forest.output.T]\n",
    "print(ks_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Kolomogorov-Smirnov Test Statistic Vs ML Estimated Poisson Distribution vs Mean, Global\")\n",
    "plt.scatter(umi_means,ks_values[:2000],s=1)\n",
    "plt.xlabel(\"Mean\")\n",
    "plt.ylabel(\"K-S Statistic\")\n",
    "plt.xlim(0,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in set(forest.sample_labels):\n",
    "    mask = forest.sample_labels == cluster\n",
    "    filtered_cells = np.array(umis[mask])\n",
    "\n",
    "    ks_values = [poisson_ks_check(x)[0] for x in filtered_cells.T]\n",
    "    means = np.mean(filtered_cells,axis=0)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(f\"Kolomogorov-Smirnov Test Statistic Vs ML Estimated Poisson Distribution vs Mean, Cluster {cluster}\")\n",
    "    plt.scatter(means,ks_values[:2000],s=1)\n",
    "    plt.xlabel(\"Mean\")\n",
    "    plt.ylabel(\"K-S Statistic\")\n",
    "    plt.xlim(0,30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in set(forest.sample_labels):\n",
    "    mask = forest.sample_labels == cluster\n",
    "    filtered_cells = np.array(umis[mask])\n",
    "\n",
    "    size_factors = np.sum(filtered_cells,axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(f\"Size factor distributions, Cluster {cluster}\")\n",
    "    plt.hist(np.array(size_factors),log=True,bins=np.arange(0,50000,200))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_factors = np.array(np.sum(np.array(umis),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(size_factors,bins=np.arange(0,50000,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA.fit_transform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
