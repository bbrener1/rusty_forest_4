{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"../data/aging_brain/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Importing Data](#importing_data)\n",
    "* [Filtering](#filtering)\n",
    "* [Forest Analysis](#forest_analysis)\n",
    "* [Cross-Validation](#cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data <a class=\"anchor\" id=\"importing_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datafetch\n",
    "# !wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE129nnn/GSE129788/suppl/GSE129788_RAW.tar\n",
    "# !tar -xvf GSE129788_RAW.tar\n",
    "# !gunzip *.gz\n",
    "\n",
    "# Accessions for raw datafetch:\n",
    "# accessions = [\n",
    "#     \"SRR8895023\",\n",
    "#     \"SRR8895024\",\n",
    "#     \"SRR8895025\",\n",
    "#     \"SRR8895026\",\n",
    "#     \"SRR8895027\",\n",
    "#     \"SRR8895028\",\n",
    "#     \"SRR8895029\",\n",
    "#     \"SRR8895030\",\n",
    "#     \"SRR8895031\",\n",
    "#     \"SRR8895032\",\n",
    "#     \"SRR8895033\",\n",
    "#     \"SRR8895034\",\n",
    "#     \"SRR8895035\",\n",
    "#     \"SRR8895036\",\n",
    "#     \"SRR8895037\",\n",
    "#     \"SRR8895038\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# Assembly is mostly GCF_000001635.20, except for last which is MM10? \n",
    "# Don't use 16\n",
    "\n",
    "# age = [\n",
    "#     'old',\n",
    "#     'old',\n",
    "#     'old',\n",
    "#     'old',\n",
    "#     'old',\n",
    "#     'old',\n",
    "#     'old',\n",
    "#     'young',\n",
    "#     'young',\n",
    "#     'young',\n",
    "#     'young',\n",
    "#     'young',\n",
    "#     'young',\n",
    "#     'young',\n",
    "#     'young',\n",
    "#     'old', # weird assemblym mm10 for some reason?    \n",
    "# ]\n",
    "\n",
    "# Hooray for garbage that hasn't been maintained since the reagan administration\n",
    "\n",
    "# for acc in accessions:\n",
    "#     !./sratoolkit.2.10.8-ubuntu64/bin/prefetch {acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRAs were moved to the folder \n",
    "\n",
    "# For shell background paralleleizaiton this will have to be a shell script:\n",
    "\n",
    "# for acc in $(find ./aging_brain/SRR*/*.sra)\n",
    "# do\n",
    "#         echo $acc\n",
    "#         echo $acc > $acc.log &\n",
    "#         ./sratoolkit.2.10.8-ubuntu64/bin/fastq-dump --readids --dumpbase --split-files --clip $acc > $acc.log 2>&1 &\n",
    "# done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir aging_brain\n",
    "# !mv GSM37221* aging_brain\n",
    "# !mv GSE129788_RAW.tar aging_brain/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering <a class=\"anchor\" id=\"filtering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_locations = !ls ../data/aging_brain/GSM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for sample_location in sample_locations:\n",
    "    samples.append(sc.read(sample_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [s.T for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples are already normalized so we will have to stack them unless I want to spend forever and a day downloading bullshit raw data\n",
    "\n",
    "# samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.vstack([s.X for s in samples])\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = sc.AnnData(combined)\n",
    "stacked.var_names = samples[0].var_names\n",
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_result = sc.pp.filter_genes_dispersion(  # select highly-variable genes\n",
    "    stacked.X, flavor='cell_ranger', n_top_genes=2000, log=True\n",
    ")\n",
    "filtered = stacked[:, filter_result.gene_subset]     # subset the genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to set up young vs old annotation:\n",
    "\n",
    "young_mask = np.zeros(37069,dtype=bool)\n",
    "old_mask = np.zeros(37069,dtype=bool)\n",
    "\n",
    "young_samples = np.sum([s.shape[0] for s in samples[:8]])\n",
    "young_mask[:young_samples] = True\n",
    "\n",
    "old_samples = np.sum([s.shape[0] for s in samples[8:]])\n",
    "old_mask[young_samples:] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = stacked.shape[0]\n",
    "batch_encoding = np.zeros((total_samples,len(samples)),dtype=bool)\n",
    "batch_labels = np.zeros(total_samples)\n",
    "\n",
    "for i,sample in enumerate(samples):\n",
    "    current_total = np.sum(batch_encoding)\n",
    "    batch_encoding[current_total:current_total+sample.shape[0],i] = True\n",
    "    batch_labels[current_total:current_total+sample.shape[0]] = i\n",
    "    \n",
    "np.savetxt(\"aging_batch_encoding.tsv\",batch_encoding,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# young_samples+old_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "young = filtered[young_mask].copy()\n",
    "old = filtered[old_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(young,open(data_location + \"aging_brain_young.pickle\",mode='bw'))\n",
    "pickle.dump(old,open(data_location + \"aging_brain_old.pickle\",mode='bw'))\n",
    "pickle.dump(filtered,open(data_location + \"aging_brain_filtered.pickle\",mode='bw'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "\n",
    "# sample_agglomeration = dendrogram(linkage(young.X, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "# feature_agglomeration = dendrogram(linkage(young.X.T, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.title(\"Unsorted Cell X Gene Plot, Brain Cells\")\n",
    "plt.imshow(young.X,aspect='auto')\n",
    "plt.colorbar(label=\"Gene Expression, Log TPM\")\n",
    "plt.ylabel(\"Cells\")\n",
    "plt.xlabel(\"Genes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic scanpy processing:\n",
    "\n",
    "sc.pp.neighbors(filtered)\n",
    "sc.tl.umap(filtered)\n",
    "sc.pl.umap(filtered)\n",
    "\n",
    "sc.pp.neighbors(young)\n",
    "sc.tl.umap(young)\n",
    "sc.pl.umap(young)\n",
    "\n",
    "sc.pp.neighbors(old)\n",
    "sc.tl.umap(old)\n",
    "sc.pl.umap(old)\n",
    "\n",
    "\n",
    "# sc.tl.louvain(young,resolution=3)\n",
    "# sc.pl.umap(young,color='louvain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# r = np.arange(batch_encoding.shape[1])\n",
    "# batch_labels = np.array([r[m][0] for m in batch_encoding])\n",
    "\n",
    "# plt.figure(figsize=((10,10)))\n",
    "# plt.scatter(*filtered.obsm['X_umap'].T,s=1,c=batch_labels,cmap='rainbow')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=((10,10)))\n",
    "# plt.scatter(*filtered.obsm['X_umap'].T,s=1,c=young_mask,alpha=.3,cmap='bwr')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest Analysis <a class=\"anchor\" id=\"forest_analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../src')\n",
    "# import tree_reader as tr \n",
    "# import lumberjack\n",
    "\n",
    "# forest = lumberjack.fit(\n",
    "#     young.X,\n",
    "# #     old.X,\n",
    "#     header=young.var_names,\n",
    "#     trees=100,\n",
    "#     braids=3,\n",
    "#     ifs=700,\n",
    "#     ofs=700,\n",
    "#     ss=500,\n",
    "#     depth=9,\n",
    "#     leaves=50,\n",
    "#     sfr=0,\n",
    "#     norm='l1',\n",
    "#     reduce_input=\"true\",\n",
    "#     reduce_output=\"false\",\n",
    "# )\n",
    "\n",
    "# forest.set_cache(True)\n",
    "# forest.backup(data_location + \"scanpy_cmp_aging_brain_true_l1\")\n",
    "\n",
    "# with open(\"scanpy_cmp_aging_brain_trim_prediction\", mode='bw') as f:\n",
    "#     pickle.dump(forest.old_predictions, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "\n",
    "import pickle \n",
    "\n",
    "# data_location = \"../data/aging_brain/\"\n",
    "\n",
    "# young = pickle.load(open(data_location + \"aging_brain_young.pickle\",mode='rb'))\n",
    "# old = pickle.load(open(data_location + \"aging_brain_old.pickle\",mode='rb'))\n",
    "# filtered = pickle.load(open(data_location + \"aging_brain_filtered.pickle\",mode='rb'))\n",
    "\n",
    "# batch_encoding = np.loadtxt(data_location + 'aging_batch_encoding.tsv')\n",
    "# batch_encoding = batch_encoding.astype(dtype=bool)\n",
    "\n",
    "# young_mask = np.zeros(37069,dtype=bool)\n",
    "# old_mask = np.zeros(37069,dtype=bool)\n",
    "\n",
    "# young_mask[:young.shape[0]] = True\n",
    "# old_mask[young.shape[0]:] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/localscratch/bbrener1/rusty_forest_v3/src')\n",
    "sys.path.append('../src')\n",
    "import tree_reader as tr \n",
    "import lumberjack\n",
    "\n",
    "data_location = \"../data/aging_brain/\"\n",
    "\n",
    "forest = tr.Forest.load(data_location + 'scanpy_cmp_aging_brain_true_l1')\n",
    "forest.arguments\n",
    "\n",
    "# old_forest = tr.Forest.reconstitute('scanpy_cmp_aging_brain_old')\n",
    "# old_forest.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we must interpret split clusters, since this will play a role in establishing the existence thereof\n",
    "# (I realize the paradox of creating a thing I have not proven exists, but hey, this isn't the philosophy department)\n",
    "\n",
    "# forest.reset_split_clusters()\n",
    "# forest.interpret_splits(depth=6,k=100,relatives=True,pca=100,mode='additive_mean',metric='cosine')\n",
    "# forest.interpret_splits(depth=6,neighborhood_fraction=.01,k=100,relatives=True,pca=100,mode='additive_mean',metric='euclidean')\n",
    "print(len(forest.split_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forest.plot_split_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = forest.split_cluster_transition_matrix()\n",
    "\n",
    "transition_agglomeration = dendrogram(linkage(transition_matrix.T, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Fig S3: Frequency of Transition Between Cluster Nodes\")\n",
    "plt.imshow(transition_matrix,cmap='binary')\n",
    "plt.ylabel(\"Origin\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.colorbar(label=\"Number of transitions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we would like to demonstrate the existence of correlations that are unusual within the node clusters.\n",
    "\n",
    "global_correlations = forest.global_correlations()\n",
    "\n",
    "for cluster in forest.split_clusters:\n",
    "    print(\"#############################\")\n",
    "    print(cluster.id)\n",
    "    print(\"#############################\")\n",
    "\n",
    "    local_correlations = cluster.local_correlations()\n",
    "    most_local = cluster.most_local_correlations()\n",
    "    for (f1,f2) in most_local:\n",
    "        print((forest.output_features[f1],forest.output_features[f2]))\n",
    "        print(f\"Global:{global_correlations[f1,f2]}\")\n",
    "        print(f\"Local:{local_correlations[f1,f2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can demonstrate significant local behavior reversals. \n",
    "# Ex Apod vs Ptgds in cluster 17.\n",
    "# Global: .63 Pearson\n",
    "# Local: -.37 Pearson\n",
    "\n",
    "apod_index = list(forest.output_features).index(\"Apod\")\n",
    "ptgds_index = list(forest.output_features).index(\"Ptgds\")\n",
    "\n",
    "apod_values = forest.output[:,apod_index]\n",
    "ptgds_values = forest.output[:,ptgds_index]\n",
    "\n",
    "factor_17 = forest.factor_matrix()[:,17]\n",
    "mask_17 = np.abs(factor_17) > .1\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "global_slope,global_intercept,_,_,_ = linregress(apod_values,ptgds_values)\n",
    "local_slope,local_intercept,_,_,_ = linregress(apod_values[mask_17],ptgds_values[mask_17])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 4A: Apod vs. Ptgds Expression (Global)\")\n",
    "plt.scatter(apod_values,ptgds_values,s=1,alpha=.3,label=\"Raw Values\")\n",
    "plt.xlabel(\"Apod (Centered Log TPM)\")\n",
    "plt.ylabel(\"Ptgds (Centered Log TPM))\")\n",
    "plt.plot(np.arange(7), global_intercept + (np.arange(7) * global_slope),c='red',label=\"Linear Fit\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 4B: Apod vs. Ptgds Expression (Factor 17 +/- Only)\")\n",
    "plt.scatter(apod_values[mask_17],ptgds_values[mask_17],s=1,alpha=.3,label=\"Raw Values\")\n",
    "plt.xlabel(\"Apod (Centered Log TPM)\")\n",
    "plt.ylabel(\"Ptgds (Centered Log TPM))\")\n",
    "plt.plot(np.arange(7), local_intercept + (np.arange(7) * local_slope),c='red',label=\"Linear Fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest.most_likely_tree(depth=6)\n",
    "forest.maximum_spanning_tree(mode='samples',depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# forest.tsne_coordinates = young.obsm['X_umap']\n",
    "# forest.tsne(pca=100)\n",
    "forest.html_tree_summary(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forest.split_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We would like to check on the consistency of the clustering procedure\n",
    "\n",
    "# for i in range(10):\n",
    "\n",
    "#     forest.reset_sample_clusters()\n",
    "#     forest.cluster_samples_encoding(k=100,pca=100,depth=8,metric='cosine')\n",
    "#     # forest.cluster_samples_simple(pca=100,sub=.8,k=20,metric='cosine',verbose=True)\n",
    "#     f = forest.plot_sample_clusters()\n",
    "#     f.savefig(f\"cluster_{i}.png\")\n",
    "\n",
    "forest.reset_sample_clusters()\n",
    "forest.cluster_samples_encoding(k=50,pca=100,depth=8,metric='cosine',override=True)\n",
    "forest.plot_sample_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = forest.factor_matrix()\n",
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import dendrogram,linkage\n",
    "\n",
    "# factor_sort = np.array(dendrogram(linkage(np.abs(factors.T[1:]),metric='correlation',method='average'),no_plot=True)['leaves']) + 1 \n",
    "# sample_forest_sort = np.argsort(forest.sample_labels)\n",
    "# sample_agg_sort = dendrogram(linkage(np.abs(factors.T[1:].T),metric='cosine',method='average'),no_plot=True)['leaves']\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(factors[sample_forest_sort].T[factor_sort].T,aspect='auto',interpolation='none',cmap=\"seismic\",vmin=-1,vmax=1)\n",
    "# plt.colorbar()\n",
    "# plt.show()http://localhost:8888/notebooks/scanpy_aging_brain_2.ipynb#\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(factors[sample_agg_sort].T[factor_sort].T,aspect='auto',interpolation='none',cmap=\"seismic\",vmin=-1,vmax=1)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# young.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.sample_clusters[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# forest.old_predictions = forest.predict(old.X)\n",
    "# forest.young_predicitons = forest.predict(young.X)\n",
    "# forest.young_predicitons.node_sample_encoding()\n",
    "# forest.old_predictions.node_sample_encoding()\n",
    "\n",
    "# old_features,old_samples = forest.old_predictions.prediction_report(mode='additive_mean')\n",
    "# young_features,young_samples = forest.young_predicitons.prediction_report(mode='additive_mean')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(*young.obsm[\"X_umap\"].T,c=pca_recovered_fraction_per_sample,s=3,alpha=.4)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(*young.obsm[\"X_umap\"].T,c=young_samples,s=3,alpha=.4)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(*filtered.obsm[\"X_umap\"][young_mask].T,c=young_samples,s=3,alpha=.4)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(*filtered.obsm[\"X_umap\"][old_mask].T,c=old_samples,s=3,alpha=.4)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(young_features,bins=np.arange(0,1,.05),log=True)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(old_features,bins=np.arange(0,1,.05),log=True)\n",
    "# plt.show()\n",
    "\n",
    "# results = forest.young_predicitons.compare_feature_residuals(forest.old_predictions,mode='rank_sum')\n",
    "\n",
    "# forest.young_predicitons.compare_factors(forest.old_predictions,bins=100)\n",
    "\n",
    "# forest.young_predicitons.compare_sample_clusters(forest.old_predictions)\n",
    "\n",
    "forest.young_predicitons.compare_factors(forest.old_predictions,bins=100)\n",
    "\n",
    "# print(list(delta_sort).index(feature_index))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(deltas,bins=np.arange(0,1,.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.reset_sample_clusters()\n",
    "forest.young_predicitons.compare_feature_residuals(forest.old_predictions,mode='cod_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we would like to run a few brief comparisons to PCA\n",
    "\n",
    "# PCA recovers this fraction using the same number of \"Factors\"\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA(n_components=30).fit(young.X)\n",
    "transformed = model.transform(young.X)\n",
    "recovered = model.inverse_transform(transformed)\n",
    "\n",
    "centered = young.X - np.mean(young.X,axis=0)\n",
    "transformed_residual = np.power(centered,2)\n",
    "\n",
    "recovered_residual = np.power(young.X - recovered,2)\n",
    "\n",
    "pca_recovered_per_sample = np.sum(recovered_residual,axis=1)\n",
    "pca_recovered_fraction_per_sample = np.sum(recovered_residual,axis=1) / np.sum(transformed_residual,axis=1)\n",
    "print(np.sum(transformed_residual))\n",
    "print(np.sum(recovered_residual))\n",
    "\n",
    "print(f\"Remaining variance:{(np.sum(recovered_residual) / np.sum(transformed_residual))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we would like to compare the factors to PCs\n",
    "# We will use c-dist\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# young_factors = forest.factor_matrix()\n",
    "\n",
    "# comparison = cdist(young_factors.T,transformed.T,metric='correlation') - 1\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 7: Correlations between PCs and Forest Factors\")\n",
    "plt.imshow(comparison,cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.ylabel(\"Factors\")\n",
    "plt.xlabel(\"PCs\")\n",
    "plt.show()\n",
    "\n",
    "# comparison[12,0]\n",
    "# comparison[8,4]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 8A: PC1 vs Factor 12 (correlation: .90)\")\n",
    "plt.scatter(young_factors[:,12],transformed[:,0],s=1)\n",
    "plt.xlabel(\"Factor 12 (AU -1,1)\")\n",
    "plt.ylabel(\"PC1 (AU, -inf,+inf)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 8B: PC4 vs Factor 8 (Correlation: .37)\")\n",
    "plt.scatter(young_factors[:,8],transformed[:,4],s=1)\n",
    "plt.xlabel(\"Factor 8 (AU -1,1)\")\n",
    "plt.ylabel(\"PC5 (AU, -inf,+inf)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# young_mse = forest.young_predicitons.feature_mse()\n",
    "# old_mse = forest.old_predictions.feature_mse()\n",
    "# mse_jackknife = forest.young_predicitons.jackknife_feature_mse_variance()\n",
    "\n",
    "# jackknife_sort = np.argsort(mse_jackknife)\n",
    "# top_jackknife = jackknife_sort[-100:]\n",
    "delta_sort = np.argsort(deltas)\n",
    "top_deltas = delta_sort[-100:]\n",
    "\n",
    "# np.sum(mismatch)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Figure 5: MSE Of Predictions in Young vs Old Samples\")\n",
    "# plt.scatter(np.log(young_mse),np.log(old_mse),s=1)\n",
    "plt.scatter(young_mse,old_mse,s=1)\n",
    "plt.xlabel(\"Young (MSE Log TPM Expression)\")\n",
    "plt.ylabel(\"Old (MSE Log TPM Expression)\")\n",
    "plt.plot([0,1],[0,1],c='red')\n",
    "for i in top_deltas:\n",
    "    var = np.sqrt(mse_jackknife[i])\n",
    "    segment = var/np.sqrt(2)\n",
    "    x,y = (young_mse[i],old_mse[i])\n",
    "    plt.plot([x+segment,x-segment],[y,y],linewidth=1)\n",
    "#     whisker_p,whisker_m = x+segment,x-segment\n",
    "#     plt.plot(np.log([whisker_p,whisker_m]),np.log([y,y]),linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_top = delta_sort[-100:]\n",
    "reverse_top = np.array(list(reversed(reverse_top)))\n",
    "top_deltas = forest.output_features[reverse_top]\n",
    "delta_mse = deltas[reverse_top]\n",
    "\n",
    "for f,d in zip(top_deltas,delta_mse):\n",
    "    print(f\"{f}: \\t{np.around(d,decimals=3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We observe large MSE delta for SNCA \n",
    "# Let's plot its behavior VS KLK6 in the relevant cluster (17)\n",
    "\n",
    "# First we need to predict which samples in the old set will fall in cluster 17\n",
    "young_factors = forest.factor_matrix()\n",
    "old_factors = forest.old_predictions.factor_matrix()\n",
    "\n",
    "mask_17 = np.abs(young_factors[:,17]) > .1\n",
    "mask_17_old = np.abs(old_factors[:,17]) > .1\n",
    "\n",
    "print(young_factors.shape)\n",
    "print(old_factors.shape)\n",
    "\n",
    "snca_index = list(forest.output_features).index(\"Snca\")\n",
    "klk6_index = list(forest.output_features).index(\"Klk6\")\n",
    "\n",
    "snca_values_young = forest.output[:,snca_index][mask_17]\n",
    "klk6_values_young = forest.output[:,klk6_index][mask_17]\n",
    "\n",
    "snca_values_old = old.X[:,snca_index][mask_17_old]\n",
    "klk6_values_old = old.X[:,klk6_index][mask_17_old]\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "young_slope,young_intercept,_,_,_ = linregress(klk6_values_young,snca_values_young)\n",
    "old_slope,old_intercept,_,_,_ = linregress(klk6_values_old,snca_values_old)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 6A: Klk6 vs SNCA (Young)\")\n",
    "plt.scatter(klk6_values_young,snca_values_young,s=1,label=\"Raw Values\")\n",
    "plt.plot(np.arange(6),(np.arange(6)*young_slope)+young_intercept,c='red',label='Linear Fit')\n",
    "plt.xlabel(\"Klk6\")\n",
    "plt.ylabel(\"SNCA\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 6B: Klk6 vs SNCA (Old)\")\n",
    "plt.scatter(klk6_values_old,snca_values_old,s=1,label=\"Raw Values\")\n",
    "plt.plot(np.arange(6),(np.arange(6)*old_slope)+old_intercept,c='red',label='Linear Fit')\n",
    "plt.xlabel(\"Klk6\")\n",
    "plt.ylabel(\"SNCA\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../gsea/src')\n",
    "\n",
    "from gsea import gsea,readgenesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = [g.upper() for g in forest.input_features]\n",
    "background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_sets = readgenesets(\"../gsea/data/kegg_allpathways.txt\")\n",
    "extant = []\n",
    "not_found = []\n",
    "for g in background:\n",
    "    for k in kegg_sets:\n",
    "        if g in kegg_sets[k]:\n",
    "            extant.append(g)\n",
    "            break\n",
    "    if g not in extant:\n",
    "        not_found.append(g)\n",
    "print(len(extant))\n",
    "extant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from colorama import Fore, Back, Style \n",
    "\n",
    "for cluster in forest.split_clusters[1:]:\n",
    "    changed_vs_sister,fold_vs_sister = cluster.logistic_sister()\n",
    "    test_up = [g.upper() for g in changed_vs_sister[-50:]]\n",
    "#     test_down = [g.upper() for g in changed_vs_sister[:50]]\n",
    "    enrichment = gsea(test_up,background,\"../gsea/data/kegg_allpathways.txt\")\n",
    "    sorted_enrichment = [(e,enrichment[e]) for e in sorted(enrichment,key=lambda x: enrichment[x][5])]\n",
    "    sorted_enrichment = [(e,s) for (e,s) in sorted_enrichment if s[0] > 0]\n",
    "    sorted_enrichment = [(e,s) for (e,s) in sorted_enrichment if s[5] < .05]\n",
    "    print(\"#######################\")\n",
    "    print(f\"Cluster {cluster.id}\")\n",
    "    print(\"#######################\")\n",
    "    print(\"\\n\".join([str(Fore.RED + e) + str(Fore.BLACK + str(s)) for (e,s) in sorted_enrichment]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(*forest.tsne_coordinates.T,s=1,c=forest.output[:,1571])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(forest.output_features).index('Rps23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(*young.obsm['X_umap'].T,s=1,c=forest.output[:,1571])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation <a class=\"anchor\" id=\"cross_validation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: We would like to do biological replicate cross-validation in this dataset as it is relatively deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we train a leave-one-out forest\n",
    "\n",
    "cv_forest = lumberjack.fit(\n",
    "    young.X[:-2648],\n",
    "    header=filtered.var_names,\n",
    "    trees=80,\n",
    "    braids=3,\n",
    "    ifs=700,\n",
    "    ofs=700,\n",
    "    ss=500,\n",
    "    depth=9,\n",
    "    leaves=10,\n",
    "    sfr=.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_forest.set_cache(True)\n",
    "cv_forest.backup(\"scanpy_aging_brain_cv_forest_compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/localscratch/bbrener1/rusty_forest_v3/src')\n",
    "sys.path.append('../src')\n",
    "import tree_reader as tr \n",
    "import lumberjack\n",
    "\n",
    "cv_forest = tr.Forest.reconstitute('scanpy_aging_brain_cv_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_forest.reset_split_clusters()\n",
    "cv_forest.interpret_splits(k=100,pca=100,depth=7,mode='additive_mean',relatives=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_forest.maximum_spanning_tree(depth=7,mode='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_forest.html_tree_summary(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_forest.cv_self_prediction = cv_forest.predict(young.X[:-2648])\n",
    "cv_forest.cv_other_prediction = cv_forest.predict(young.X[-2648:])\n",
    "cv_forest.cv_self_prediction.node_sample_encoding()\n",
    "cv_forest.cv_other_prediction.node_sample_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_forest.cv_self_prediction.compare_factors(cv_forest.cv_other_prediction,no_plot=False,log=False,bins=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "young_factors = cv_forest.cv_self_prediction.factor_matrix()\n",
    "old_factors = cv_forest.cv_other_prediction.factor_matrix()\n",
    "\n",
    "for cluster in cv_forest.split_clusters:\n",
    "    young_scores = young_factors[:,cluster.id]\n",
    "    old_scores = old_factors[:,cluster.id]\n",
    "    young_hist = np.histogram(young_scores,bins=np.arange(-1,1,.1))[0] + 1\n",
    "    old_hist = np.histogram(old_scores,bins=np.arange(-1,1,.1))[0] + 1\n",
    "    young_prob = young_hist / np.sum(young_hist)\n",
    "    old_prob = old_hist / np.sum(old_hist)\n",
    "#     print(\"##############################\")\n",
    "    print(f\"{cluster.id} Entropy: {entropy(young_prob,qk=old_prob)}\")\n",
    "#     print(\"##############################\")\n",
    "#     print(young_prob)\n",
    "#     print(old_prob)\n",
    "#     print(\"##############################\")\n",
    "    plt.figure()\n",
    "    plt.axes([0,.5,1,.5])\n",
    "    plt.ylabel(\"7 Mouse Frequency\")\n",
    "    plt.title(str(cluster.id))\n",
    "    plt.hist(young_scores,bins=np.arange(-.5,.5,.05),alpha=.5,density=True,color='gold',label=\"7\",log=True)\n",
    "    plt.legend()\n",
    "    plt.axes([0,0,1,.5])\n",
    "    plt.ylabel(\"1 Mouse Frequency\")\n",
    "    plt.xlabel(\"Factor Value\")\n",
    "    plt.hist(old_scores,bins=np.arange(-.5,.5,.05),alpha=.5,color='blue',density=True,label=\"1\",log=True)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_forest.cv_self_prediction.prediction_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_forest.cv_other_prediction.prediction_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_forest.split_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = cv_forest.cv_self_prediction.compare_feature_residuals(cv_forest.cv_other_prediction,mode='rank_sum',no_plot=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_mask = [p < .00001 for (s,p) in comparisons]\n",
    "np.sum(significant_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also wish to check what the actual effect sizes are for these oh-so-significant residuals\n",
    "\n",
    "# self_remaining = cv_forest.cv_self_prediction.feature_remaining_error()\n",
    "# other_remaining = cv_forest.cv_other_prediction.feature_remaining_error()\n",
    "\n",
    "# self_remaining\n",
    "# ratio = self_remaining/other_remaining\n",
    "\n",
    "# ratio.shape\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Distribution of CoD Effect Sizes\")\n",
    "# plt.hist(ratio[significant_mask],bins=50,log=True)\n",
    "# plt.xlabel(\"Effect Size\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.show()\n",
    "\n",
    "# self_mse = cv_forest.cv_self_prediction.feature_mse()\n",
    "# other_mse = cv_forest.cv_other_prediction.feature_mse()\n",
    "\n",
    "ratio = other_mse / self_mse\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Distribution of MSE Effect Sizes\")\n",
    "plt.hist(ratio[significant_mask],bins=50,log=True)\n",
    "plt.xlabel(\"Effect Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the more intersting but obnoxious-to-check questions is how good our ability to predict pre-discovered sample clusters is\n",
    "# across biological replicates. For this we will need a CV forest and an approximately equivalent forest trained on all samples\n",
    "# Let's load both and perform clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also plot the distribution of the prediction error\n",
    "\n",
    "# predicted = cv_forest.cv_other_prediction.prediction()\n",
    "# residuals = cv_forest.cv_other_prediction.residuals()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Random Forest True vs Predicted\")\n",
    "# plt.scatter(cv_forest.cv_other_prediction.matrix.flatten(),predicted,s=1)\n",
    "# plt.xlabel(\"True Value\")\n",
    "# plt.ylabel(\"Predicted Value\")\n",
    "# plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.hist(residuals.flatten(),bins=50)\n",
    "plt.xlabel(\"Residual (log tpm)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to see what the distribution of the test statistics will look like if I actually randomly split the residuals\n",
    "\n",
    "# from scipy.stats import ranksums\n",
    "\n",
    "# residuals = cv_forest.cv_self_prediction.residuals()\n",
    "\n",
    "# random_mask = np.random.random(residuals.shape[0]) > .5\n",
    "\n",
    "# results = [ranksums(residuals[:,i][random_mask],residuals[:,i][~random_mask]) for i in range(residuals.shape[1])]\n",
    "# results\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist([r[1] for r in results],bins=50)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# OUTCOME OF TEST: When the residuals are split randomly, this test behaves as it should, so we really are over-fitting \n",
    "# to the biological samples and finding differences that shouldn't be there. \n",
    "\n",
    "# Next step is to check whether or not it's a biolocial replicate issue. We can do this by training a cv forest on a \n",
    "# random mask of the cells instead of splitting by biological replicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mask = np.random.random(young.X.shape[0]) > .5\n",
    "\n",
    "\n",
    "random_cv_forest = lumberjack.fit(\n",
    "    young.X[random_mask],\n",
    "    header=filtered.var_names,\n",
    "    trees=300,\n",
    "    braids=3,\n",
    "    ifs=700,\n",
    "    ofs=700,\n",
    "    ss=500,\n",
    "    depth=7,\n",
    "    leaves=100,\n",
    "    sfr=.5\n",
    ")\n",
    "\n",
    "random_cv_forest.random_mask = random_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv_forest.set_cache(True)\n",
    "random_cv_forest.backup(\"scanpy_aging_brain_random_cv_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv_forest = tr.Forest.reconstitute(\"scanpy_aging_brain_random_cv_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv_forest.self_pred = random_cv_forest.predict(young.X[random_cv_forest.random_mask])\n",
    "random_cv_forest.other_pred = random_cv_forest.predict(young.X[~random_cv_forest.random_mask])\n",
    "\n",
    "random_cv_forest.self_pred.node_sample_encoding()\n",
    "random_cv_forest.other_pred.node_sample_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = random_cv_forest.self_pred.compare_feature_residuals(random_cv_forest.other_pred,no_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([p < .00001 for t,p in comparisons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we wish to see if PCA recovers a similar amount of information to forest-based reconstruction. Obviously for \n",
    "# well-posed problems, enough PCs can recover ALL information, so let's restrict ourselves to a number of PCs that is\n",
    "# approximately in line with the number of forest factors discovered. (More than 30 PCs don't usually contain too \n",
    "# much additioanl real information in scRNAseq anyway)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "training = young.X[:-2648]\n",
    "test = young.X[-2648:]\n",
    "\n",
    "model = PCA(n_components=30).fit(training)\n",
    "training_recovery = model.inverse_transform(model.transform(training))\n",
    "test_recovery = model.inverse_transform(model.transform(test))\n",
    "\n",
    "training_deviation = np.power(training - np.mean(training,axis=0),2)\n",
    "test_deviation = np.power(test - np.mean(test,axis=0),2)\n",
    "\n",
    "recovered_training_deviation = np.power(training_recovery - np.mean(training_recovery,axis=0),2)\n",
    "print(f\"Recovered training deviation {np.sum(recovered_training_deviation)}\")\n",
    "training_recovery_error = np.power(training - training_recovery,2)\n",
    "print(f\"Recovery training error {np.sum(training_recovery_error)}\")\n",
    "print(f\"Ratio:{np.sum(training_recovery_error)/np.sum(training_deviation)}\")\n",
    "\n",
    "recovered_test_deviation = np.power(test_recovery - np.mean(test_recovery,axis=0),2)\n",
    "print(f\"Recovered test deviation {np.sum(recovered_test_deviation)}\")\n",
    "test_recovery_error = np.power(test - test_recovery,2)\n",
    "print(f\"Recovery test error {np.sum(test_recovery_error)}\")\n",
    "print(f\"Ratio:{np.sum(test_recovery_error)/np.sum(test_deviation)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information recovered by PCA is somewhat comparable on order of magnitude to RFR. Now let's see if we are recovering\n",
    "# the same targets well. \n",
    "\n",
    "pca_feature_error = np.sum(training_recovery_error,axis=0) + 1\n",
    "pca_feature_original = np.sum(training_deviation,axis=0) + 1\n",
    "\n",
    "pca_feature_remaining = pca_feature_error/pca_feature_original\n",
    "\n",
    "pca_remaining_sort = np.argsort(pca_feature_remaining)\n",
    "\n",
    "print(f\"Features best recovered by PCA: {cv_forest.output_features[pca_cod_sort[:50]]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_feature_remaining = cv_forest.cv_self_prediction.feature_remaining_error()\n",
    "# forest_remaining_sort = np.argsort(cv_forest.cv_self_prediction.feature_remaining_error())\n",
    "\n",
    "# print(f\"Features best recovered by RFR: {cv_forest.output_features[forest_remaining_sort[:50]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Fraction of Variance Unexplained, PCA vs RFR\")\n",
    "plt.scatter(forest_feature_remaining,pca_feature_remaining,s=2)\n",
    "plt.plot([0,1],[0,1],color='red')\n",
    "plt.xlim(0,1.01)\n",
    "plt.ylim(0,1.01)\n",
    "plt.ylabel(\"FVU PCA\")\n",
    "plt.xlabel(\"FVU RFR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets that have uniquely good prediction by the RFR compared to PCA:\n",
    "\n",
    "fvu_delta = forest_feature_remaining - pca_feature_remaining \n",
    "\n",
    "delta_sort = np.argsort(fvu_delta)\n",
    "\n",
    "print(f\"Forest-predicted features: {cv_forest.output_features[delta_sort[:20]]}\")\n",
    "print(f\"PCA FVU: {pca_feature_remaining[delta_sort[:20]]}\")\n",
    "print(f\"Forest FVU: {forest_feature_remaining[delta_sort[:20]]}\")\n",
    "\n",
    "print(f\"PCA-predicted features: {cv_forest.output_features[delta_sort[-20:]]}\")\n",
    "print(f\"PCA FVU: {pca_feature_remaining[delta_sort[-20:]]}\")\n",
    "print(f\"Forest FVU: {forest_feature_remaining[delta_sort[-20:]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(cv_prediction.,aspect='auto',interpolation='none')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.imshow(young.X[-2648:],aspect='auto',interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_encoding = cv_prediction.node_sample_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = young.X[-2648:]\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    for cluster in cv_forest.split_clusters[1:]:\n",
    "        parent_cluster = cluster.parent_cluster()\n",
    "        print(f\"Cluster {cluster.id}\")\n",
    "        print(f\"Parent: {parent_cluster.id}\")\n",
    "        reg,features = cluster.regression()\n",
    "        parent_weights = np.abs(old_factors[:,parent_cluster.id])\n",
    "        print(f\"Local Old:{Fore.RED}{reg.score(old.X.T[features].T,old.X,sample_weight=parent_weights)}\")\n",
    "        print(f\"Global Old:{Fore.BLUE}{reg.score(old.X.T[features].T,old.X)}\")    \n",
    "        parent_weights = np.abs(young_factors[:,parent_cluster.id])\n",
    "        print(f\"Local Young:{Fore.RED}{reg.score(young.X.T[features].T,young.X,sample_weight=parent_weights)}\")\n",
    "        print(f\"Global Young:{Fore.BLUE}{reg.score(young.X.T[features].T,young.X)}\")    \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.reset_sample_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Back, Style \n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    g_o = []\n",
    "    l_o = []\n",
    "    g_y = []\n",
    "    l_y = []\n",
    "    e_o = []\n",
    "    e_y = []\n",
    "    for cluster in forest.split_clusters[1:]:\n",
    "        parent_cluster = cluster.parent_cluster()\n",
    "        print(\"########################\")\n",
    "        print(f\"Cluster {cluster.id}\")\n",
    "        print(f\"Parent: {parent_cluster.id}\")\n",
    "        p_y,n_y,a_y = cluster.error_ratio()\n",
    "        e_y.append((p_y+n_y)/a_y)\n",
    "        p_o,n_o,a_o = cluster.error_ratio(sample_matrix=old.X,scores=old_factors[:,cluster.id])\n",
    "        e_o.append((p_o+n_o)/a_o)\n",
    "        ff,f_r,r_r = cluster.regression()\n",
    "#         weights = np.abs(old_factors[:,parent_cluster.id])\n",
    "        weights = np.abs(old_factors[:,cluster.id])\n",
    "#         old_intermediate = f_r.predict(old.X).reshape(-1, 1)\n",
    "        old_intermediate = old.X.T[ff].T\n",
    "        global_old = r_r.score(old_intermediate,old.X)\n",
    "        local_old = r_r.score(old_intermediate,old.X,sample_weight=weights)\n",
    "        g_o.append(global_old)\n",
    "        l_o.append(local_old)\n",
    "        print(f\"Global Old:{Fore.RED}{global_old}{Fore.BLACK}\")\n",
    "        print(f\"Local Old:{Fore.BLUE}{local_old}{Fore.BLACK}\")    \n",
    "#         weights = np.abs(young_factors[:,parent_cluster.id])\n",
    "        weights = np.abs(young_factors[:,cluster.id])\n",
    "#         young_intermediate = f_r.predict(young.X).reshape(-1, 1)\n",
    "        young_intermediate = young.X.T[ff].T\n",
    "        global_young = r_r.score(young_intermediate,young.X)\n",
    "        local_young = r_r.score(young_intermediate,young.X,sample_weight=weights)\n",
    "        g_y.append(global_young)\n",
    "        l_y.append(local_young)\n",
    "        print(f\"Global Young:{Fore.RED}{global_young}{Fore.BLACK}\")    \n",
    "        print(f\"Local Young:{Fore.BLUE}{local_young}{Fore.BLACK}\")\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "young_factors = forest.factor_matrix()\n",
    "old_factors = forest.old_predictions.factor_matrix()\n",
    "\n",
    "for cluster in forest.split_clusters:\n",
    "    young_scores = young_factors[:,cluster.id]\n",
    "    old_scores = old_factors[:,cluster.id]\n",
    "    young_hist = np.histogram(young_scores,bins=np.arange(-1,1,.1))[0] + 1\n",
    "    old_hist = np.histogram(old_scores,bins=np.arange(-1,1,.1))[0] + 1\n",
    "    young_prob = young_hist / np.sum(young_hist)\n",
    "    old_prob = old_hist / np.sum(old_hist)\n",
    "#     print(\"##############################\")\n",
    "    print(f\"{cluster.id} Entropy: {entropy(young_prob,qk=old_prob)}\")\n",
    "#     print(\"##############################\")\n",
    "#     print(young_prob)\n",
    "#     print(old_prob)\n",
    "#     print(\"##############################\")\n",
    "    plt.figure()\n",
    "    plt.axes([0,.5,1,.5])\n",
    "    plt.ylabel(\"Young Frequency\")\n",
    "    plt.title(str(cluster.id))\n",
    "    plt.hist(young_scores,bins=np.arange(-.5,.5,.05),alpha=.5,density=True,color='gold',label=\"Young\",log=True)\n",
    "    plt.legend()\n",
    "    plt.axes([0,0,1,.5])\n",
    "    plt.ylabel(\"Old Frequency\")\n",
    "    plt.xlabel(\"Factor Value\")\n",
    "    plt.hist(old_scores,bins=np.arange(-.5,.5,.05),alpha=.5,color='blue',density=True,label=\"Old\",log=True)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.maximum_spanning_tree(mode='samples',depth=10)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(forest.sample_labels,alpha=.5,density=True,label=\"Young\",bins=np.arange(len(forest.sample_clusters)+1))\n",
    "# plt.hist(old_predicted_clusters,alpha=.5,density=True,label=\"Old\",bins=np.arange(len(forest.sample_clusters)+1))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# old_factors = forest.old_predictions.factor_matrix()\n",
    "# young_factors = forest.young_predicitons.factor_matrix()\n",
    "\n",
    "# young_factors.shape\n",
    "\n",
    "bins = 100\n",
    "bin_interval = 2. / bins\n",
    "\n",
    "for own_f,other_f in zip(old_factors.T,young_factors.T):\n",
    "    own_hist = np.histogram(\n",
    "    own_f, bins=np.arange(-1, 1, bin_interval))[0] + 1\n",
    "    other_hist = np.histogram(\n",
    "        other_f, bins=np.arange(-1, 1, bin_interval))[0] + 1\n",
    "    own_prob = np.log(own_hist / np.sum(own_hist))\n",
    "    other_prob = np.log(other_hist / np.sum(other_hist))\n",
    "\n",
    "\n",
    "    lin_min = np.min([np.min(own_prob),np.min(other_prob)])\n",
    "\n",
    "    print(lin_min)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(own_prob,other_prob,s=3)\n",
    "    plt.plot([0,lin_min],[0,lin_min],color='red',alpha=.5)\n",
    "    plt.xlabel(\"Factor Frequency, Self (Log Probability)\")\n",
    "    plt.ylabel(\"Factor Frequency, Other (Log Probability)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from colorama import Fore, Back, Style \n",
    "\n",
    "# import warnings\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "\n",
    "#     for cluster in forest.split_clusters[1:]:\n",
    "#         parent_cluster = cluster.parent_cluster()\n",
    "#         print(\"########################\")\n",
    "#         print(f\"Cluster {cluster.id}\")\n",
    "#         print(f\"Parent: {parent_cluster.id}\")\n",
    "#         reg,features = cluster.regression()\n",
    "#         parent_weights = np.abs(old_factors[:,parent_cluster.id])\n",
    "#         print(f\"Local Old:{Fore.RED}{reg.score(old.X.T[features].T,old.X,sample_weight=parent_weights)}{Fore.BLACK}\")\n",
    "#         print(f\"Global Old:{Fore.BLUE}{reg.score(old.X.T[features].T,old.X)}{Fore.BLACK}\")    \n",
    "#         parent_weights = np.abs(young_factors[:,parent_cluster.id])\n",
    "#         print(f\"Local Young:{Fore.RED}{reg.score(young.X.T[features].T,young.X,sample_weight=parent_weights)}{Fore.BLACK}\")\n",
    "#         print(f\"Global Young:{Fore.BLUE}{reg.score(young.X.T[features].T,young.X)}{Fore.BLACK}\")    \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = \"Ttyh2\"\n",
    "# f_i = forest.truth_dictionary.feature_dictionary[feature]\n",
    "# plt.figure()\n",
    "# plt.title(f'{feature}')\n",
    "# plt.scatter(*forest.tsne_coordinates.T,c=forest.output[:,f_i],s=3,alpha=.4)\n",
    "# plt.show()\n",
    "\n",
    "# forest.tsne_coordinates=young.obsm['X_umap']\n",
    "\n",
    "feature = \"Actb\"\n",
    "f_i = forest.truth_dictionary.feature_dictionary[feature]\n",
    "plt.figure()\n",
    "plt.title(f'{feature}')\n",
    "plt.scatter(*forest.tsne_coordinates.T,c=forest.output[:,f_i],s=3,alpha=.4)\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(*forest.tsne_coordinates.T,c=forest.sample_labels == 24,s=3,alpha=.4)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature = \"Crlf2\"\n",
    "f_i = forest.truth_dictionary.feature_dictionary[feature]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'{feature}')\n",
    "plt.scatter(*filtered.obsm[\"X_umap\"][young_mask].T,c=forest.output[:,f_i],s=3,alpha=.4)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'{feature}')\n",
    "plt.scatter(*filtered.obsm['X_umap'][old_mask].T,c=old.X[:,f_i],s=3,alpha=.4)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(f'{feature}')\n",
    "# plt.scatter(*young.obsm[\"X_umap\"].T,c=forest.output[:,f_i],s=3,alpha=.4)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(f'{feature}')\n",
    "# plt.scatter(*old.obsm['X_umap'].T,c=old.X[:,f_i],s=3,alpha=.4)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"Spock3\"\n",
    "f_i = forest.truth_dictionary.feature_dictionary[feature]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'{feature}')\n",
    "plt.scatter(*forest.tsne_coordinates.T,c=forest.output[:,f_i],s=3,alpha=.4)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'{feature}')\n",
    "plt.scatter(*old.obsm['X_umap'].T,c=old.X[:,f_i],s=3,alpha=.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klk6_index = forest.truth_dictionary.feature_dictionary['Klk6']\n",
    "klk6_correlations = np.corrcoef(forest.output.T)[klk6_index]\n",
    "klk_sort = np.argsort(np.abs(klk6_correlations))\n",
    "top_features = forest.output_features[klk_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_features[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mask1 = oligo_mask\n",
    "mask2 = oligo_old_mask\n",
    "\n",
    "f1 = \"Opalin\"\n",
    "f2 = \"Klk6\"\n",
    "f1i = forest.truth_dictionary.feature_dictionary[f1]\n",
    "f2i = forest.truth_dictionary.feature_dictionary[f2]\n",
    "\n",
    "noise11 = np.random.random(young.X.shape[0])/3\n",
    "noise12 = np.random.random(young.X.shape[0])/3\n",
    "noise21 = np.random.random(old.X.shape[0])/3\n",
    "noise22 = np.random.random(old.X.shape[0])/3\n",
    "\n",
    "plt.figure()\n",
    "ax1 = plt.axes([0,0,.8,.8])\n",
    "plt.scatter((young.X[:,f1i]+noise11)[mask1],(young.X[:,f2i]+noise12)[mask1],alpha=.5,s=2)\n",
    "plt.plot()\n",
    "ax2 = plt.axes([.8,0,.2,.8])\n",
    "plt.hist((young.X[:,f2i]+noise11)[mask1],bins=20,orientation='horizontal')\n",
    "ax3 = plt.axes([0,.8,.8,.2])\n",
    "plt.hist((young.X[:,f1i]+noise12)[mask1],bins=20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ax1 = plt.axes([0,0,.8,.8])\n",
    "plt.scatter((young.X[:,f1i]+noise11),(young.X[:,f2i]+noise12),alpha=.5,s=2)\n",
    "plt.plot()\n",
    "ax2 = plt.axes([.8,0,.2,.8])\n",
    "plt.hist((young.X[:,f2i]+noise11),bins=20,orientation='horizontal')\n",
    "ax3 = plt.axes([0,.8,.8,.2])\n",
    "plt.hist((young.X[:,f1i]+noise12),bins=20)\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# ax1 = plt.axes([0,0,.8,.8])\n",
    "# plt.scatter((old.X[:,f1i]+noise21)[mask2],(old.X[:,f2i]+noise22)[mask2],alpha=.5,s=2)\n",
    "# ax2 = plt.axes([.8,0,.2,.8])\n",
    "# plt.hist((old.X[:,f2i]+noise21)[mask2],bins=20,orientation='horizontal')\n",
    "# ax3 = plt.axes([0,.8,.8,.2])\n",
    "# plt.hist((old.X[:,f1i]+noise22)[mask2],bins=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://link.springer.com/article/10.1007/s10048-016-0478-0\n",
    "# Snca expression associated with amyloid-like inclusion bodies, proteostatic stress? \n",
    "# Klk6 association reverses from .09 young to -.08 old. Now inverse association locally? \n",
    "# Klk6 & Spock3 both protease involved\n",
    "\n",
    "# Tubb3 Microtubule assembly, guidance of axons, maintenance\n",
    "# Spock3 modulates metalloproteases (High correlation/significance?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prkj_mask = young_factors[:,1] > 0\n",
    "prkj_old_mask = old_factors[:,1] > 0\n",
    "\n",
    "ppia_index = forest.truth_dictionary.feature_dictionary[\"Pcp4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prkj_local = young.X[prkj_mask]\n",
    "prkj_old_local = old.X[prkj_old_mask]\n",
    "\n",
    "prkj_local_correlations = np.corrcoef(prkj_local.T)[ppia_index]\n",
    "prkj_local_correlations[~np.isfinite(prkj_local_correlations)] = 0.00000000001\n",
    "prkj_local_sort = np.argsort(np.abs(prkj_local_correlations))\n",
    "prkj_local_features = forest.output_features[prkj_local_sort][-20:]\n",
    "print(prkj_local_features)\n",
    "print(prkj_local_correlations[prkj_local_sort][-20:])\n",
    "\n",
    "prkj_old_local_correlations = np.corrcoef(prkj_old_local.T)[ppia_index]\n",
    "prkj_old_local_correlations[~np.isfinite(prkj_old_local_correlations)] = 0.00000000001\n",
    "prkj_old_local_sort = np.argsort(np.abs(prkj_old_local_correlations))\n",
    "prkj_old_local_features = forest.output_features[prkj_old_local_sort][-20:]\n",
    "print(prkj_old_local_features)\n",
    "print(prkj_old_local_correlations[prkj_old_local_sort][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prkj_delta_correlations = prkj_local_correlations[:1999] - prkj_old_local_correlations\n",
    "prkj_delta_correlations[~np.isfinite(prkj_delta_correlations)] = .0000000001\n",
    "prkj_delta_sort = np.argsort(np.abs(prkj_delta_correlations))\n",
    "prkj_delta_features = forest.output_features[prkj_delta_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prkj_delta_features[-20:])\n",
    "print(prkj_delta_correlations[prkj_delta_sort][-20:])\n",
    "print(prkj_local_correlations[prkj_delta_sort][-20:])\n",
    "print(prkj_old_local_correlations[prkj_delta_sort][-20:])\n",
    "\n",
    "# Go enrichment of the deltas was not obviously helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prkj_local_correlations[snca_i])\n",
    "print(prkj_old_local_correlations[snca_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sgk1, Spock3, C4b, inflammation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoding = forest.node_representation(forest.nodes(depth=5,root=False),mode='sample',pca=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_knn = tr.fast_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    indices_fast = fast_knn(sample_encoding,k=10,metric='euclidean')\n",
    "    print(sorted(indices_fast[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=11, algorithm='ball_tree',metric='euclidean').fit(sample_encoding)\n",
    "distances,indices_good = nbrs.kneighbors(sample_encoding)\n",
    "\n",
    "sorted(indices_good[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist,pdist,squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(indices_good).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(indices_good.shape[0]):\n",
    "    if sorted(indices_fast[i]) != sorted(indices_good[i][1:]):\n",
    "        distances = cdist(sample_encoding[i].reshape(1,-1),sample_encoding)[0]\n",
    "        print(i)\n",
    "        print(sorted(indices_fast[i]))\n",
    "        print(distances[sorted(indices_fast[i])])\n",
    "        print(sorted(indices_good[i][1:]))\n",
    "        print(distances[sorted(indices_good[i][1:])])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
