{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existence Of Node Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate that in random forest that has been trained on some set of data, the nodes can be reasonably organized into clusters.\n",
    "\n",
    "First, we must train or load a forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/localscratch/bbrener1/rusty_forest_v3/src')\n",
    "sys.path.append('../src')\n",
    "import tree_reader as tr \n",
    "import lumberjack\n",
    "\n",
    "data_location = \"../data/aging_brain/\"\n",
    "\n",
    "forest = tr.Forest.load(data_location + 'full_clustering')\n",
    "forest.arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forest.output_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest is a collection of decision trees, and a decision tree is a collection of individual decision points, commonly known as \"Nodes\"\n",
    "\n",
    "To understand Random Forests and Decision Trees, it is important to understand how Nodes work. Each individual node is a (very crappy) regressor, eg. each Node makess a prediction based on a rule like \"If Gene 1 has expression > 10, Gene 2 will have expression < 5\", or \"If a house is < 5 miles from a school, it will cost > $100,000\". A very important property of each node, however, is that it can also have children, which are other nodes. When a node makes a prediction like \"If Gene 1 has expression > 10 then Gene 2 has expression < 5\", it can pass all the samples for which Gene 1 is > 10 to one of its children, and all the samples for which Gene 1 < 10 to the other child. After that, each one of its children can make a different prediction, which results in compound rules.\n",
    "\n",
    "This is how a decision tree is formed. A decision tree with a depth of 2 might contain a rule like \"If Gene 1 > 10 AND Gene 3 > 10, THEN Gene 2 and Gene 4 are both < 2, which would represent one of the \"Leaf\" nodes that it has. Leaf nodes are nodes with no children. \n",
    "\n",
    "Individual decision trees, then, are somewhat crappy predictors, but they're better than individual nodes. In order to improve the performance of decision trees, we can construct a Random Forest. To construct a random forest, we can train many decision trees on bootstraps of a dataset\n",
    "\n",
    "If many decision trees are combined and their predictions averaged together, you have a Random Forest, which is a pretty good kind of regressor. \n",
    "\n",
    "A practical demonstration might help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.reset_split_clusters()\n",
    "forest.interpret_splits(depth=5,mode='additive_mean',neighborhood_fraction=.1,metric='cosine',pca=100,relatives=True,k=100,resolution=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we know that random forests are collections of ordered nodes, we can examine a more interesting question: do certain nodes occur repeatedly in the forest, despite operating on bootstrapped samples? \n",
    "\n",
    "In order to examine this question first we must understand different ways of describing a node. I think generally there are three helpful ways of looking at a node:\n",
    "\n",
    "* **Node Sample Encoding**: A binary vector the length of the number of samples you are considering. 0 or false means the sample is absent from the node. A 1 or true means the sample is present in the node. \n",
    "\n",
    "* **Node Mean Encoding**: A float vector the length of the number of targets you are considering. Each value is the mean of the target values for all samples in this node. This is the node's prediction for samples that occur in it.\n",
    "\n",
    "* **Node Additive Encoding**: A float vector the length of the number of targets you are considering. Each value is THE DIFFERENCE between the mean value for that target in THIS NODE and the mean value for that target IN THE PARENT of this node. For root nodes, which have no parents, the additive encoding is simply th mean value across the entire dataset. (As if the mean of a hypothetical parent would have been 0). This encoding represents the marginal effect of each node.\n",
    "\n",
    "We should examine if there are any common patterns that appear if we encode many nodes from a forest using each of these representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the sample representations of nodes. \n",
    "# This generates a set of figures demonstrating the existence of node clusters\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sample_encoding = forest.node_representation(forest.nodes(depth=5,root=False),mode='sample')\n",
    "reduced_sample = PCA(n_components=100).fit_transform(sample_encoding.T)\n",
    "reduced_node = PCA(n_components=100).fit_transform(sample_encoding)\n",
    "\n",
    "print(sample_encoding.shape)\n",
    "print(reduced_sample.shape)\n",
    "print(reduced_node.shape)\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "\n",
    "sample_agglomeration = dendrogram(linkage(reduced_sample, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "node_agglomeration = dendrogram(linkage(reduced_node, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 1: Sample Presence in Node (Two-Way Agglomerated)\")\n",
    "plt.imshow(sample_encoding[node_agglomeration].T[sample_agglomeration].T,cmap='binary',aspect='auto',interpolation='none')\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# And here we sort the nodes after they have been clustered (more on the clustering procedure in a bit)\n",
    "\n",
    "node_cluster_sort = np.argsort([n.split_cluster for n in forest.nodes(depth=5,root=False)])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure S1: Sample Presence in Node (Clustered)\")\n",
    "plt.imshow(sample_encoding[node_cluster_sort].T[sample_agglomeration].T,cmap='binary',aspect='auto',interpolation='none')\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sample_encoding = forest.node_representation(forest.nodes(depth=5,root=False),mode='sister')\n",
    "reduced_sample = PCA(n_components=100).fit_transform(sample_encoding.T)\n",
    "reduced_node = PCA(n_components=100).fit_transform(sample_encoding)\n",
    "\n",
    "print(sample_encoding.shape)\n",
    "print(reduced_sample.shape)\n",
    "print(reduced_node.shape)\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "\n",
    "sample_agglomeration = dendrogram(linkage(reduced_sample, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "node_agglomeration = dendrogram(linkage(reduced_node, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "\n",
    "cluster_node_sort = np.argsort([n.split_cluster for n in forest.nodes(depth=5,root=False)])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 1SC: Sample Presence in Node vs Sister (Two-Way Agglomerated)\")\n",
    "plt.imshow(sample_encoding[node_agglomeration].T[sample_agglomeration].T,cmap='bwr',aspect='auto',interpolation='none')\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure 1SC: Sample Presence in Node vs Sister (Clustered By Gain)\")\n",
    "plt.imshow(sample_encoding[cluster_node_sort].T[sample_agglomeration].T,cmap='bwr',aspect='auto',interpolation='none')\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the construct and agglomerate the additive gain representation \n",
    "\n",
    "\n",
    "feature_encoding = forest.node_representation(forest.nodes(depth=5,root=False),mode='additive_mean')\n",
    "reduced_feature = PCA(n_components=100).fit_transform(feature_encoding.T)\n",
    "reduced_node = PCA(n_components=100).fit_transform(feature_encoding)\n",
    "\n",
    "feature_agglomeration = dendrogram(linkage(reduced_feature, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "node_agglomeration = dendrogram(linkage(reduced_node, metric='cosine', method='average'), no_plot=True)['leaves']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the additive gain representation \n",
    "\n",
    "print(feature_encoding.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure S2 a: Target Gain in Node (Double-Agglomerated)\")\n",
    "plt.imshow(feature_encoding[node_agglomeration].T[feature_agglomeration].T,cmap='bwr',interpolation='none',aspect='auto',vmin=-2,vmax=2)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.colorbar(label=\"Parent Target Mean - Node Target Mean\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Figure S2 b: Target Gain in Node (Clustered)\")\n",
    "plt.imshow(feature_encoding[node_cluster_sort].T[feature_agglomeration].T,cmap='bwr',interpolation='none',aspect='auto',vmin=-2,vmax=2)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.colorbar(label=\"Parent Target Mean - Node Target Mean\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can look at silhouette plots scores for various node encodings in order to get a feel for whether or not we are adequately clustering them and whether or not the clusters meaningfully exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Plots For Node Clusters \n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "node_labels = np.array([n.split_cluster for n in forest.nodes(depth=5,root=False)])\n",
    "\n",
    "# silhouette_scores = silhouette_samples(reduced_node,node_labels,metric='cosine')\n",
    "silhouette_scores = silhouette_samples(feature_encoding,node_labels,metric='cosine')\n",
    "# silhouette_scores = silhouette_samples(sample_encoding,node_labels,metric='cosine')\n",
    "\n",
    "sorted_silhouette = np.zeros(silhouette_scores.shape)\n",
    "sorted_colors = np.zeros(silhouette_scores.shape)\n",
    "\n",
    "current_index = 0\n",
    "next_index = 0\n",
    "for i in sorted(set(node_labels)):\n",
    "    mask = node_labels == i\n",
    "    selected_values = sorted(silhouette_scores[mask])    \n",
    "    next_index = current_index + np.sum(mask)\n",
    "    sorted_silhouette[current_index:next_index] = selected_values\n",
    "    sorted_colors[current_index:next_index] = i\n",
    "    current_index = next_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Silhouette Plots For Nodes Clustered By Gain\")\n",
    "for i,node in enumerate(sorted_silhouette):\n",
    "    plt.plot([0,node],[i,i],color=cm.nipy_spectral(sorted_colors[i] / len(forest.split_clusters)))\n",
    "# plt.scatter(sorted_silhouette,np.arange(len(sorted_silhouette)),s=1)\n",
    "plt.plot([0,0],[0,len(sorted_silhouette)],color='red')\n",
    "plt.xlabel(\"Silhouette Score\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "\n",
    "import pickle \n",
    "\n",
    "data_location = \"../data/aging_brain/\"\n",
    "\n",
    "young = pickle.load(open(data_location + \"aging_brain_young.pickle\",mode='rb'))\n",
    "old = pickle.load(open(data_location + \"aging_brain_old.pickle\",mode='rb'))\n",
    "\n",
    "filtered = pickle.load(open(data_location + \"aging_brain_filtered.pickle\",mode='rb'))\n",
    "\n",
    "batch_encoding = np.loadtxt(data_location + 'aging_batch_encoding.tsv')\n",
    "batch_encoding = batch_encoding.astype(dtype=bool)\n",
    "\n",
    "young_mask = np.zeros(37069,dtype=bool)\n",
    "old_mask = np.zeros(37069,dtype=bool)\n",
    "\n",
    "young_mask[:young.shape[0]] = True\n",
    "old_mask[young.shape[0]:] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.maximum_spanning_tree(mode='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.html_tree_summary(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans =  forest.split_cluster_transition_matrix(depth=10)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Node Cluster Transition Frequency\")\n",
    "plt.imshow(trans[:-1],cmap='binary',interpolation='none')\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Origin\")\n",
    "plt.colorbar(label=\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(trans[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(np.arange(50),trans[:,37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(trans[:,37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trans[45,37])\n",
    "print(trans[23,37])\n",
    "print(trans[37,37])\n",
    "# print(trans[34,9])\n",
    "# print(trans[34,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "990 - 677 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "99/313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
